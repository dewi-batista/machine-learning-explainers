\relax 
\providecommand{\transparent@use}[1]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Supervised Learning}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Linear Regression}{3}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Statistical Motivation}{6}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Goodness of fit: $R^2$}{7}{subsubsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Why `regression'?}{8}{subsubsection.2.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Logistic Regression}{9}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Statistical Motivation}{10}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Support Vector Machines (SVMs)}{11}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Hard-margin SVMs \textcolor {red}{TODO: switch notation from z to y}}{12}{subsubsection.2.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A hard-margin SVM in two dimensions.}}{13}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:SVM_hard_margin}{{1}{13}{A hard-margin SVM in two dimensions}{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Soft-margin SVMs}{14}{subsubsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}Non-linear SVMs}{14}{subsubsection.2.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A soft-margin SVM in two dimensions. Samples are labelled according to their true class.}}{15}{figure.caption.3}\protected@file@percent }
\newlabel{fig:SVM_soft_margin}{{2}{15}{A soft-margin SVM in two dimensions. Samples are labelled according to their true class}{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Non-linearly separable samples being transformed to a space in which they are linearly separable. Almost like punching the curve into a line. I'm unsure why the empty set symbol above the arrow is present.}}{16}{figure.caption.4}\protected@file@percent }
\newlabel{fig:SVM_non_linear}{{3}{16}{Non-linearly separable samples being transformed to a space in which they are linearly separable. Almost like punching the curve into a line. I'm unsure why the empty set symbol above the arrow is present}{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Decision Trees and Random Forests}{16}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Regularisation}{17}{subsection.2.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Two fits, both yielding 0 test error.}}{17}{figure.caption.5}\protected@file@percent }
\newlabel{fig:regularisation_two_plots}{{4}{17}{Two fits, both yielding 0 test error}{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.1}L1 regularisation (LASSO: least absolute shrinkage and selection operator)}{18}{subsubsection.2.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Laplace pdf with means 0 and decreasing variances according to $\lambda \in \{0.5,1,2,3\}$.}}{18}{figure.caption.6}\protected@file@percent }
\newlabel{fig:laplace_pdf}{{5}{18}{Laplace pdf with means 0 and decreasing variances according to $\lambda \in \{0.5,1,2,3\}$}{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.2}L2 regularisation (Ridge regression)}{18}{subsubsection.2.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.3}Visualisation via loss landscapes}{19}{subsubsection.2.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}The Bias-Variance Tradeoff}{19}{subsection.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.1}Derivation with MSE loss}{19}{subsubsection.2.6.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The graph of learning method error against model complexity in terms of bias and variance.}}{22}{figure.caption.7}\protected@file@percent }
\newlabel{fig:bias_variance}{{6}{22}{The graph of learning method error against model complexity in terms of bias and variance}{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.2}\textcolor {red}{TODO:} How it motivates regularisation}{23}{subsubsection.2.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.3}Beyond MSE}{23}{subsubsection.2.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.4}Double descent}{24}{subsubsection.2.6.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces How double descent typically looks.}}{24}{figure.caption.8}\protected@file@percent }
\newlabel{fig:double_descent}{{7}{24}{How double descent typically looks}{figure.caption.8}{}}
\@writefile{toc}{\setcounter {tocdepth}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Misc. questions}{24}{subsection.2.7}\protected@file@percent }
\@writefile{toc}{\setcounter {tocdepth}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Gradient Descent and its Optimisation}{26}{section.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The graph of a complex loss landscape $\{(\theta ,R_D(\theta ))|\theta \in \mathbb  {R}^2\}$ whose minimiser we seek.}}{26}{figure.caption.9}\protected@file@percent }
\newlabel{fig:loss_landscape_complex}{{8}{26}{The graph of a complex loss landscape $\{(\theta ,R_D(\theta ))|\theta \in \R ^2\}$ whose minimiser we seek}{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Gradient Descent and Batch Learning}{27}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}In which direction does $f$ most ascend locally from $\mathbf  {x}^{(t)}$?}{27}{subsubsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Sensitivity to $\mathbf  {x}^{(0)}$}{28}{subsubsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}My dataset (i.e. $n$) is too large!!!}{28}{subsubsection.3.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces How we might expect the convergence of each method to look.}}{29}{figure.caption.10}\protected@file@percent }
\newlabel{fig:gradient_descent_types}{{9}{29}{How we might expect the convergence of each method to look}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}Batch + Layer normalisation}{30}{subsubsection.3.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Momentum and Adaptive Learning Rates}{31}{subsection.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces How we might expect the convergence to look with momentum.}}{32}{figure.caption.17}\protected@file@percent }
\newlabel{fig:gradient_descent_momentum}{{10}{32}{How we might expect the convergence to look with momentum}{figure.caption.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Hyperparameter Tuning}{32}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\setcounter {tocdepth}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Misc. questions}{33}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\setcounter {tocdepth}{2}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Loss function overview.}}{34}{table.caption.21}\protected@file@percent }
\newlabel{tab:loss_func_choices}{{1}{34}{Loss function overview}{table.caption.21}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Neural Networks}{34}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Multi-Layer Perceptrons (MLPs)}{34}{subsection.4.1}\protected@file@percent }
\newlabel{sec:multi_layer_perceptrons}{{4.1}{34}{Multi-Layer Perceptrons (MLPs)}{subsection.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces A multi-layer perceptron (MLP) with $k=3$ hidden layers.}}{35}{figure.caption.22}\protected@file@percent }
\newlabel{fig:MLP}{{11}{35}{A multi-layer perceptron (MLP) with $k=3$ hidden layers}{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces A neural network with one hidden layer ($k=1$).}}{37}{figure.caption.23}\protected@file@percent }
\newlabel{fig:neural_nets_simple_example}{{12}{37}{A neural network with one hidden layer ($k=1$)}{figure.caption.23}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Constructive example: The algebra}{37}{subsubsection.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Constructive example: Subbing values in}{38}{subsubsection.4.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Universal Function Approximation for MLPs}{38}{subsubsection.4.1.3}\protected@file@percent }
\newlabel{subsubsec:universal_function_approximation_theorem}{{4.1.3}{38}{Universal Function Approximation for MLPs}{subsubsection.4.1.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.4}Advocating for depth $>$ width}{42}{subsubsection.4.1.4}\protected@file@percent }
\newlabel{subsubsec:deep_MLPs}{{4.1.4}{42}{Advocating for depth $>$ width}{subsubsection.4.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Backpropagation for MLPs}{43}{subsection.4.2}\protected@file@percent }
\newlabel{subsec:backprop}{{4.2}{43}{Backpropagation for MLPs}{subsection.4.2}{}}
\newlabel{eq:back_prop_statement_2}{{4.2}{45}{Backpropagation for MLPs}{Item.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Convolutional Neural Networks (CNNs)}{46}{subsection.4.3}\protected@file@percent }
\newlabel{subsec:conv_neural_networks}{{4.3}{46}{Convolutional Neural Networks (CNNs)}{subsection.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces A binary classification CNN processing an image of a frog.}}{46}{figure.caption.26}\protected@file@percent }
\newlabel{fig:CNN_frog}{{13}{46}{A binary classification CNN processing an image of a frog}{figure.caption.26}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Convolutions}{46}{subsubsection.4.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Convolution without bias.}}{47}{figure.caption.27}\protected@file@percent }
\newlabel{fig:convolution_operation}{{14}{47}{Convolution without bias}{figure.caption.27}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Pooling}{47}{subsubsection.4.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces The core flavours of pooling.}}{48}{figure.caption.28}\protected@file@percent }
\newlabel{fig:pooling}{{15}{48}{The core flavours of pooling}{figure.caption.28}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Output dimensions after convolving and pooling}{48}{subsubsection.4.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Recurrent Neural Networks (RNNs)}{48}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Long Short-Term Memory (LSTMs)}{48}{subsubsection.4.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Gated Recurrent Units (GRUs)}{48}{subsubsection.4.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Transformers}{48}{subsection.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.1}Token embedding}{49}{subsubsection.4.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.2}Positional encoding}{49}{subsubsection.4.5.2}\protected@file@percent }
\@writefile{toc}{\setcounter {tocdepth}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Misc. questions}{49}{subsection.4.6}\protected@file@percent }
\@writefile{toc}{\setcounter {tocdepth}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Training and validation losses over 50 epochs for some model.}}{50}{figure.caption.29}\protected@file@percent }
\newlabel{fig:early_stopping}{{16}{50}{Training and validation losses over 50 epochs for some model}{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Dropout to prevent overdependence on given neurons.}}{50}{figure.caption.30}\protected@file@percent }
\newlabel{fig:dropout}{{17}{50}{Dropout to prevent overdependence on given neurons}{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Examples of samples belonging to classes that are not linearly separable.}}{51}{figure.caption.31}\protected@file@percent }
\newlabel{fig:non_linearly_separable}{{18}{51}{Examples of samples belonging to classes that are not linearly separable}{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces The image of our non-linear transformation.}}{52}{figure.caption.32}\protected@file@percent }
\newlabel{fig:non_lin_transformation}{{19}{52}{The image of our non-linear transformation}{figure.caption.32}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Deep Generative Models}{53}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Variational Autoencoders (VAEs)}{54}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Autoencoders: no variation yet!}{54}{subsubsection.5.1.1}\protected@file@percent }
\newlabel{sec:autoencoders}{{5.1.1}{54}{Autoencoders: no variation yet!}{subsubsection.5.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces An autoencoder in which $\theta $ and $\phi $ are fit using multi-layer perceptrons (MLPs).}}{55}{figure.caption.33}\protected@file@percent }
\newlabel{fig:autoencoder}{{20}{55}{An autoencoder in which $\theta $ and $\phi $ are fit using multi-layer perceptrons (MLPs)}{figure.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces The output of the decoder of an autoencoder with randomly generated latent points as input. Gibberish output.}}{56}{figure.caption.35}\protected@file@percent }
\newlabel{fig:autoencoder_generation_1}{{21}{56}{The output of the decoder of an autoencoder with randomly generated latent points as input. Gibberish output}{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces The output of the decoder of an autoencoder points relatively close to the latent embedding of a given training sample.}}{57}{figure.caption.36}\protected@file@percent }
\newlabel{fig:autoencoder_generation_2}{{22}{57}{The output of the decoder of an autoencoder points relatively close to the latent embedding of a given training sample}{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Left: an encoder which outputs parameters of the latent distribution $q_{\theta }(\mathbf  {z}|\mathbf  {x})=\mathcal  {N}(\mu _{\theta },\Sigma _{\theta })$. Right: a decoder which outputs parameters of the reconstruction distribution $p_{\phi }(\mathbf  {x}|\mathbf  {z}')=\mathcal  {N}(\mu _{\phi },\Sigma _{\phi })$ in which $\mathbf  {z}'\sim q_{\theta }(\mathbf  {z}|\mathbf  {x})$.}}{58}{figure.caption.38}\protected@file@percent }
\newlabel{fig:VAE_architecture}{{23}{58}{Left: an encoder which outputs parameters of the latent distribution $q_{\theta }(\z |\x )=\mathcal {N}(\mu _{\theta },\Sigma _{\theta })$. Right: a decoder which outputs parameters of the reconstruction distribution $p_{\phi }(\x |\z ')=\mathcal {N}(\mu _{\phi },\Sigma _{\phi })$ in which $\z '\sim q_{\theta }(\z |\x )$}{figure.caption.38}{}}
\newlabel{eq:marginal_likelihood}{{1}{60}{Formulating VAEs}{equation.5.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Backpropagation for VAEs}{61}{subsubsection.5.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Generative Adversarial Networks (GANs)}{61}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Flow-based Models}{61}{subsection.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Johann Bernoulli being denoised in line with the backward process of a diffusion model.}}{62}{figure.caption.39}\protected@file@percent }
\newlabel{fig:diffusion_process}{{24}{62}{Johann Bernoulli being denoised in line with the backward process of a diffusion model}{figure.caption.39}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Diffusion Models}{62}{subsection.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.1}U-Net denoisers}{63}{subsubsection.5.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces \centering  Example U-Net architecture.}}{63}{figure.caption.40}\protected@file@percent }
\newlabel{fig:u_net_arch}{{25}{63}{\centering Example U-Net architecture}{figure.caption.40}{}}
\@writefile{toc}{\setcounter {tocdepth}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Misc. questions}{64}{subsection.5.5}\protected@file@percent }
\@writefile{toc}{\setcounter {tocdepth}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Object Detection Models}{64}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}(Fast/Faster) R-CNN}{64}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}YOLO}{65}{subsection.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}DETR}{65}{subsection.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Appendices}{66}{section*.41}\protected@file@percent }
\@writefile{toc}{\setcounter {tocdepth}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Probability Theory Things}{66}{appendix.a.A}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}From Bernoulli to Binomial}{66}{subsection.a.A.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}From Categorical to Multinomial}{66}{subsection.a.A.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Negative Binomial and Geometric}{68}{subsection.a.A.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4}The Law of Large Numbers \textcolor {red}{TODO}}{68}{subsection.a.A.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.5}The Central Limit Theorem (CLT)}{69}{subsection.a.A.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.6}Entropy}{69}{subsection.a.A.6}\protected@file@percent }
\newlabel{app:entropy}{{A.6}{69}{Entropy}{subsection.a.A.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.6.1}Kullback–Leibler Divergence (KL-divergence)}{70}{subsubsection.a.A.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.6.2}Distribution fitting via cross-entropy}{70}{subsubsection.a.A.6.2}\protected@file@percent }
\newlabel{subsubsec:dist_fit_CE}{{A.6.2}{70}{Distribution fitting via cross-entropy}{subsubsection.a.A.6.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.6.3}Distribution fitting via maximum likelihood estimation \textcolor {red}{TODO: move this to stats part?}}{72}{subsubsection.a.A.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.7}Mutual Information}{73}{subsection.a.A.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.8}Quality of distribution fitting = Ability to encode?}{73}{subsection.a.A.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B}Classical Statistics Things}{73}{appendix.a.B}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Sample Independence}{73}{subsection.a.B.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Overfitting and underfitting}{73}{subsection.a.B.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}Pearson Correlation}{74}{subsection.a.B.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.4}The EM Algorithm}{74}{subsection.a.B.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {C}Philosophy Things}{74}{appendix.a.C}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}Thoughts on reasoning (Summer 2024)}{74}{subsection.a.C.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C.2}The Principled Measure-Explainability Tradeoff}{75}{subsection.a.C.2}\protected@file@percent }
\gdef\svg@ink@ver@settings{{\m@ne }{inkscape}{\m@ne }}
\gdef \@abspage@last{76}
