\relax 
\providecommand{\transparent@use}[1]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Supervised Learning}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Linear Regression}{3}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Statistical Motivation}{6}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Example}{7}{subsubsection.2.1.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Patient data pertaining to their age, BMI and cholesterol levels.}}{7}{table.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:lin_reg_example}{{1}{7}{Patient data pertaining to their age, BMI and cholesterol levels}{table.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Each feature against the output (left) and the dataset in the feature space (right).}}{8}{figure.caption.4}\protected@file@percent }
\newlabel{fig:not_at_all_the_same}{{1}{8}{Each feature against the output (left) and the dataset in the feature space (right)}{figure.caption.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Training dataset.}}{8}{table.caption.5}\protected@file@percent }
\newlabel{tab:lin_reg_example_train}{{2}{8}{Training dataset}{table.caption.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Testing dataset.}}{8}{table.caption.5}\protected@file@percent }
\newlabel{tab:lin_reg_example_test}{{3}{8}{Testing dataset}{table.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Model performance on the training set (left) and testing set (right).}}{9}{figure.caption.6}\protected@file@percent }
\newlabel{fig:lin_reg_example_on_train_and_test}{{2}{9}{Model performance on the training set (left) and testing set (right)}{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Regularisation}{10}{subsubsection.2.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4}$R^2$ value}{10}{subsubsection.2.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.5}Terminology}{11}{subsubsection.2.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.6}Opportunity to Demonstrate High Quality Data}{11}{subsubsection.2.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Logistic Regression}{12}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Statistical Motivation}{13}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Support Vector Machines (SVMs)}{14}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Hard-margin SVMs}{15}{subsubsection.2.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A hard-margin SVM in two dimensions.}}{16}{figure.caption.7}\protected@file@percent }
\newlabel{fig:SVM_hard_margin}{{3}{16}{A hard-margin SVM in two dimensions}{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Soft-margin SVMs}{17}{subsubsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}Non-linear SVMs}{17}{subsubsection.2.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces A soft-margin SVM in two dimensions. Samples are labelled according to their true class.}}{18}{figure.caption.8}\protected@file@percent }
\newlabel{fig:SVM_soft_margin}{{4}{18}{A soft-margin SVM in two dimensions. Samples are labelled according to their true class}{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Decision Trees and Random Forests}{18}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Handcrafted Features}{18}{subsection.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}The Bias-Variance Tradeoff}{18}{subsection.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.1}Derivation with MSE loss}{19}{subsubsection.2.6.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The graph of learning method error against model complexity in terms of bias and variance.}}{22}{figure.caption.9}\protected@file@percent }
\newlabel{fig:bias_variance}{{5}{22}{The graph of learning method error against model complexity in terms of bias and variance}{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.2}\textcolor {red}{TODO:} How it motivates regularisation}{23}{subsubsection.2.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.3}Beyond MSE}{23}{subsubsection.2.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.4}Double descent}{23}{subsubsection.2.6.4}\protected@file@percent }
\@writefile{toc}{\setcounter {tocdepth}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Misc. questions}{23}{subsection.2.7}\protected@file@percent }
\@writefile{toc}{\setcounter {tocdepth}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \dots  }}{24}{figure.caption.10}\protected@file@percent }
\newlabel{fig:double_descent}{{6}{24}{\dots }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Gradient Descent and its Optimisation}{25}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Gradient Descent}{26}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Momentum}{26}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Hyperparameter Tuning}{26}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\setcounter {tocdepth}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Misc. questions}{26}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\setcounter {tocdepth}{2}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Loss function choices.}}{27}{table.caption.11}\protected@file@percent }
\newlabel{tab:loss_func_choices}{{4}{27}{Loss function choices}{table.caption.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Neural Networks}{28}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Multi-Layer Perceptrons (MLPs)}{28}{subsection.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces A multi-layer perceptron (MLP) with $k=3$ hidden layers.}}{29}{figure.caption.12}\protected@file@percent }
\newlabel{fig:MLP}{{7}{29}{A multi-layer perceptron (MLP) with $k=3$ hidden layers}{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Constructive example: The algebra}{30}{subsubsection.4.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces A neural network with $k=1$ hidden layer.}}{31}{figure.caption.13}\protected@file@percent }
\newlabel{fig:neural_nets_simple_example}{{8}{31}{A neural network with $k=1$ hidden layer}{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Constructive example — Subbing values in}{31}{subsubsection.4.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Universal function approximation}{32}{subsubsection.4.1.3}\protected@file@percent }
\newlabel{subsubsec:universal_function_approximation_theorem}{{4.1.3}{32}{Universal function approximation}{subsubsection.4.1.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.4}Depth $>$ Width}{32}{subsubsection.4.1.4}\protected@file@percent }
\newlabel{subsubsec:deep_MLPs}{{4.1.4}{32}{Depth $>$ Width}{subsubsection.4.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Backpropagation for MLPs}{33}{subsection.4.2}\protected@file@percent }
\newlabel{subsec:backprop}{{4.2}{33}{Backpropagation for MLPs}{subsection.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Convolutional Neural Networks (CNNs)}{34}{subsection.4.3}\protected@file@percent }
\newlabel{subsec:conv_neural_networks}{{4.3}{34}{Convolutional Neural Networks (CNNs)}{subsection.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces A binary classification CNN processing an image of a frog.}}{34}{figure.caption.14}\protected@file@percent }
\newlabel{fig:CNN_frog}{{9}{34}{A binary classification CNN processing an image of a frog}{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Convolutions}{34}{subsubsection.4.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Convolution without bias.}}{35}{figure.caption.15}\protected@file@percent }
\newlabel{fig:convolution_operation}{{10}{35}{Convolution without bias}{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Pooling}{35}{subsubsection.4.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces The core flavours of pooling.}}{36}{figure.caption.16}\protected@file@percent }
\newlabel{fig:pooling}{{11}{36}{The core flavours of pooling}{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Backpropagation for CNNs}{36}{subsubsection.4.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Recurrent Neural Networks (RNNs)}{36}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Long Short-Term Memory (LSTMs)}{37}{subsubsection.4.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Gated Recurrent Units (GRUs)}{37}{subsubsection.4.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.3}Skip connections/residual networks}{37}{subsubsection.4.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Autoencoders}{37}{subsection.4.5}\protected@file@percent }
\newlabel{sec:autoencoders}{{4.5}{37}{Autoencoders}{subsection.4.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces An autoencoder in which $\theta $ and $\phi $ are fit using multi-layer perceptrons (MLPs).}}{38}{figure.caption.18}\protected@file@percent }
\newlabel{fig:autoencoder}{{12}{38}{An autoencoder in which $\theta $ and $\phi $ are fit using multi-layer perceptrons (MLPs)}{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Training and validation losses over 50 epochs for some model.}}{39}{figure.caption.19}\protected@file@percent }
\newlabel{fig:early_stopping}{{13}{39}{Training and validation losses over 50 epochs for some model}{figure.caption.19}{}}
\@writefile{toc}{\setcounter {tocdepth}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Misc. questions}{39}{subsection.4.6}\protected@file@percent }
\@writefile{toc}{\setcounter {tocdepth}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Dropout to prevent overdependence on given neurons.}}{40}{figure.caption.20}\protected@file@percent }
\newlabel{fig:dropout}{{14}{40}{Dropout to prevent overdependence on given neurons}{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Examples of samples belonging to classes that are not linearly-separable.}}{41}{figure.caption.21}\protected@file@percent }
\newlabel{fig:non_linearly_separable}{{15}{41}{Examples of samples belonging to classes that are not linearly-separable}{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces The image of our non-linear transformation.}}{41}{figure.caption.22}\protected@file@percent }
\newlabel{fig:non_lin_transformation}{{16}{41}{The image of our non-linear transformation}{figure.caption.22}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Deep Generative Models}{43}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Variational Autoencoders (VAEs)}{43}{subsection.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces The output of the decoder of an autoencoder with randomly generated latent points as input. Gibberish output.}}{44}{figure.caption.23}\protected@file@percent }
\newlabel{fig:autoencoder_generation_1}{{17}{44}{The output of the decoder of an autoencoder with randomly generated latent points as input. Gibberish output}{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces The output of the decoder of an autoencoder points relatively close to the latent embedding of a given training sample.}}{45}{figure.caption.24}\protected@file@percent }
\newlabel{fig:autoencoder_generation_2}{{18}{45}{The output of the decoder of an autoencoder points relatively close to the latent embedding of a given training sample}{figure.caption.24}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Formulating VAEs}{45}{subsubsection.5.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Left: an encoder which outputs parameters of the latent distribution $q_{\theta }(\mathbf  {z}|\mathbf  {x})=\mathcal  {N}(\mu _{\theta },\Sigma _{\theta })$. Right: a decoder which outputs parameters of the reconstruction distribution $p_{\phi }(\mathbf  {x}|\mathbf  {z}')=\mathcal  {N}(\mu _{\phi },\Sigma _{\phi })$ in which $\mathbf  {z}'\sim q_{\theta }(\mathbf  {z}|\mathbf  {x})$.}}{46}{figure.caption.25}\protected@file@percent }
\newlabel{fig:VAE_architecture}{{19}{46}{Left: an encoder which outputs parameters of the latent distribution $q_{\theta }(\z |\x )=\mathcal {N}(\mu _{\theta },\Sigma _{\theta })$. Right: a decoder which outputs parameters of the reconstruction distribution $p_{\phi }(\x |\z ')=\mathcal {N}(\mu _{\phi },\Sigma _{\phi })$ in which $\z '\sim q_{\theta }(\z |\x )$}{figure.caption.25}{}}
\newlabel{eq:marginal_likelihood}{{1}{47}{Formulating VAEs}{equation.5.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Backpropagation for VAEs}{49}{subsubsection.5.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Generative Adversarial Networks (GANs)}{49}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Normalising Flow Models}{49}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Diffusion Models}{49}{subsection.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Johann Bernoulli being denoised in line with the backward process of a diffusion model.}}{49}{figure.caption.26}\protected@file@percent }
\newlabel{fig:diffusion_process}{{20}{49}{Johann Bernoulli being denoised in line with the backward process of a diffusion model}{figure.caption.26}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.1}U-Net denoisers}{50}{subsubsection.5.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces \centering  Example U-Net architecture.}}{51}{figure.caption.27}\protected@file@percent }
\newlabel{fig:u_net_arch}{{21}{51}{\centering Example U-Net architecture}{figure.caption.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Transformers}{52}{subsection.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.1}Token embedding}{52}{subsubsection.5.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.2}Positional encoding}{52}{subsubsection.5.5.2}\protected@file@percent }
\@writefile{toc}{\setcounter {tocdepth}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6}Misc. questions}{52}{subsection.5.6}\protected@file@percent }
\@writefile{toc}{\setcounter {tocdepth}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Object Detection Models}{52}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}(Fast/Faster) R-CNN}{53}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Appendices}{54}{section*.28}\protected@file@percent }
\@writefile{toc}{\setcounter {tocdepth}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Probability Theory Things}{54}{appendix.a.A}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}From Bernoulli to Binomial}{54}{subsection.a.A.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}From Categorical to Multinomial}{55}{subsection.a.A.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Negative Binomial and Geometric}{56}{subsection.a.A.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4}The Law of Large Numbers}{57}{subsection.a.A.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.5}Entropy}{57}{subsection.a.A.5}\protected@file@percent }
\newlabel{app:entropy}{{A.5}{57}{Entropy}{subsection.a.A.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.5.1}Kullback–Leibler Divergence (KL-divergence)}{58}{subsubsection.a.A.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.5.2}Distribution fitting using cross-entropy}{59}{subsubsection.a.A.5.2}\protected@file@percent }
\newlabel{subsubsec:dist_fit_CE}{{A.5.2}{59}{Distribution fitting using cross-entropy}{subsubsection.a.A.5.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.5.3}Distribution fitting using maximum likelihood estimation}{60}{subsubsection.a.A.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.6}The Central Limit Theorem (CLT)}{61}{subsection.a.A.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B}Classical Statistics Things}{61}{appendix.a.B}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Independent samples}{61}{subsection.a.B.1}\protected@file@percent }
\gdef\svg@ink@ver@settings{{\m@ne }{inkscape}{\m@ne }}
\gdef \@abspage@last{62}
