\relax 
\providecommand{\transparent@use}[1]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Supervised Learning}{4}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Linear Regression}{5}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Statistical Motivation}{8}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Example}{8}{subsubsection.2.1.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \centering  Patient data pertaining to their age, BMI and cholesterol levels.}}{9}{table.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:lin_reg_example}{{1}{9}{\centering Patient data pertaining to their age, BMI and cholesterol levels}{table.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \centering  Each feature against the output (left) and the dataset in the feature space (right).}}{9}{figure.caption.4}\protected@file@percent }
\newlabel{fig:not_at_all_the_same}{{1}{9}{\centering Each feature against the output (left) and the dataset in the feature space (right)}{figure.caption.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \centering  Training dataset.}}{10}{table.caption.5}\protected@file@percent }
\newlabel{tab:lin_reg_example_train}{{2}{10}{\centering Training dataset}{table.caption.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces \centering  Testing dataset.}}{10}{table.caption.5}\protected@file@percent }
\newlabel{tab:lin_reg_example_test}{{3}{10}{\centering Testing dataset}{table.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \centering  Model performance on the training set (left) and testing set (right).}}{11}{figure.caption.6}\protected@file@percent }
\newlabel{fig:lin_reg_example_on_train_and_test}{{2}{11}{\centering Model performance on the training set (left) and testing set (right)}{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Regularisation}{11}{subsubsection.2.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4}$R^2$ value}{11}{subsubsection.2.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.5}Terminology}{12}{subsubsection.2.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.6}Opportunity to Demonstrate High Quality Data}{13}{subsubsection.2.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Logistic Regression}{13}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Statistical Motivation}{15}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Support Vector Machines (SVMs)}{15}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Hard-margin SVMs}{16}{subsubsection.2.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \centering  A hard-margin SVM in two dimensions.}}{17}{figure.caption.7}\protected@file@percent }
\newlabel{fig:SVM_hard_margin}{{3}{17}{\centering A hard-margin SVM in two dimensions}{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Soft-margin SVMs}{18}{subsubsection.2.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \centering  A soft-margin SVM in two dimensions. Samples are labelled according to their true class.}}{18}{figure.caption.8}\protected@file@percent }
\newlabel{fig:SVM_soft_margin}{{4}{18}{\centering A soft-margin SVM in two dimensions. Samples are labelled according to their true class}{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}Non-linear SVMs}{19}{subsubsection.2.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Decision Trees and Random Forests}{19}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Handcrafted Features}{19}{subsection.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}The Bias-Variance Trade-Off}{19}{subsection.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.1}Double descent}{19}{subsubsection.2.6.1}\protected@file@percent }
\@writefile{toc}{\setcounter {tocdepth}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Misc. questions}{19}{subsection.2.7}\protected@file@percent }
\@writefile{toc}{\setcounter {tocdepth}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Gradient Descent and its Optimisation}{20}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Gradient Descent}{21}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Momentum}{21}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Hyperparameter Tuning}{21}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\setcounter {tocdepth}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Misc. questions}{21}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\setcounter {tocdepth}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Why does $f(\mathbf  {x})$ ascend most in the direction of $\nabla f(\mathbf  {x})$?}{21}{subsubsection.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Choosing a loss function}{22}{subsubsection.3.4.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces \centering  Loss function choices.}}{22}{table.caption.9}\protected@file@percent }
\newlabel{tab:loss_func_choices}{{4}{22}{\centering Loss function choices}{table.caption.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.3}How should we choose the learning rate $\eta $?}{23}{subsubsection.3.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.4}How should we initialise the parameters?}{23}{subsubsection.3.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Neural Networks}{24}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Multi-Layer Perceptrons (MLPs)}{24}{subsection.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \centering  A multi-layer perceptron (MLP) with $k=3$ hidden layers.}}{25}{figure.caption.10}\protected@file@percent }
\newlabel{fig:MLP}{{5}{25}{\centering A multi-layer perceptron (MLP) with $k=3$ hidden layers}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Constructive example - The algebra}{26}{subsubsection.4.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \centering  A neural network with $k=1$ hidden layer.}}{26}{figure.caption.11}\protected@file@percent }
\newlabel{fig:neural_nets_simple_example}{{6}{26}{\centering A neural network with $k=1$ hidden layer}{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Constructive example - Subbing values in}{27}{subsubsection.4.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Universal function approximation theorem (expressivity)}{28}{subsubsection.4.1.3}\protected@file@percent }
\newlabel{subsubsec:universal_function_approximation_theorem}{{4.1.3}{28}{Universal function approximation theorem (expressivity)}{subsubsection.4.1.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.4}Depth $>$ Width}{28}{subsubsection.4.1.4}\protected@file@percent }
\newlabel{subsubsec:deep_MLPs}{{4.1.4}{28}{Depth $>$ Width}{subsubsection.4.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Backpropagation}{28}{subsection.4.2}\protected@file@percent }
\newlabel{subsec:backprop}{{4.2}{28}{Backpropagation}{subsection.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Convolutional Neural Networks (CNNs)}{29}{subsection.4.3}\protected@file@percent }
\newlabel{subsec:conv_neural_networks}{{4.3}{29}{Convolutional Neural Networks (CNNs)}{subsection.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \centering  A binary classification CNN processing an image of a frog.}}{29}{figure.caption.12}\protected@file@percent }
\newlabel{fig:CNN_frog}{{7}{29}{\centering A binary classification CNN processing an image of a frog}{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Convolutions}{29}{subsubsection.4.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces \centering  Convolution without bias.}}{30}{figure.caption.13}\protected@file@percent }
\newlabel{fig:convolution_operation}{{8}{30}{\centering Convolution without bias}{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Pooling}{30}{subsubsection.4.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces \centering  The core flavours of pooling.}}{31}{figure.caption.14}\protected@file@percent }
\newlabel{fig:pooling}{{9}{31}{\centering The core flavours of pooling}{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Example}{31}{subsubsection.4.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Recurrent Neural Networks (RNNs)}{32}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Long Short-Term Memory (LSTMs)}{32}{subsubsection.4.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Gated Recurrent Units (GRUs)}{32}{subsubsection.4.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Autoencoders}{32}{subsection.4.5}\protected@file@percent }
\newlabel{sec:autoencoders}{{4.5}{32}{Autoencoders}{subsection.4.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces \centering  An autoencoder mapping from the sample space $\Omega _{\mathbf  {X}}$ to the latent space $\mathbb  {R}^d$ then back to $\Omega _{\mathbf  {X}}$.}}{33}{figure.caption.16}\protected@file@percent }
\newlabel{fig:autoencoder}{{10}{33}{\centering An autoencoder mapping from the sample space $\OX $ to the latent space $\R ^d$ then back to $\OX $}{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.1}Example autoencoder (visualisation bonus)}{33}{subsubsection.4.5.1}\protected@file@percent }
\@writefile{toc}{\setcounter {tocdepth}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Skip Connections/Residual Networks}{33}{subsection.4.6}\protected@file@percent }
\@writefile{toc}{\setcounter {tocdepth}{2}}
\@writefile{toc}{\setcounter {tocdepth}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}Misc. questions}{34}{subsection.4.7}\protected@file@percent }
\@writefile{toc}{\setcounter {tocdepth}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Training and validation losses over 50 epochs for some model.}}{34}{figure.caption.17}\protected@file@percent }
\newlabel{fig:early_stopping}{{11}{34}{Training and validation losses over 50 epochs for some model}{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Dropout to prevent overdependence.}}{35}{figure.caption.18}\protected@file@percent }
\newlabel{fig:dropout}{{12}{35}{Dropout to prevent overdependence}{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces \centering  Examples of samples belonging to classes that are not linearly-separable.}}{36}{figure.caption.19}\protected@file@percent }
\newlabel{fig:non_linearly_separable}{{13}{36}{\centering Examples of samples belonging to classes that are not linearly-separable}{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces \centering  The image of our non-linear transformation.}}{36}{figure.caption.20}\protected@file@percent }
\newlabel{fig:non_lin_transformation}{{14}{36}{\centering The image of our non-linear transformation}{figure.caption.20}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Generative Models}{38}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Variational Autoencoders (VAEs)}{38}{subsection.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces \centering  The output of the decoder of an autoencoder with randomly generated latent points as input. Gibberish output.}}{39}{figure.caption.21}\protected@file@percent }
\newlabel{fig:autoencoder_generation_1}{{15}{39}{\centering The output of the decoder of an autoencoder with randomly generated latent points as input. Gibberish output}{figure.caption.21}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}think of different subsubsection name}{39}{subsubsection.5.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces \centering  The output of the decoder of an autoencoder points relatively close to the latent embedding of a given training sample.}}{40}{figure.caption.22}\protected@file@percent }
\newlabel{fig:autoencoder_generation_2}{{16}{40}{\centering The output of the decoder of an autoencoder points relatively close to the latent embedding of a given training sample}{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces \centering  Example architecture of a variational autoencoder in which the input distribution is two-dimensional.}}{40}{figure.caption.23}\protected@file@percent }
\newlabel{fig:VAE_architecture}{{17}{40}{\centering Example architecture of a variational autoencoder in which the input distribution is two-dimensional}{figure.caption.23}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}The Reparameterization Trick}{42}{subsubsection.5.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3}VAE architecture for MNIST}{42}{subsubsection.5.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Generative Adversarial Networks (GANs)}{42}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Normalising Flow Models}{42}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Diffusion Models}{42}{subsection.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Transformers}{42}{subsection.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.1}Token embedding}{43}{subsubsection.5.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.2}Positional encoding}{43}{subsubsection.5.5.2}\protected@file@percent }
\@writefile{toc}{\setcounter {tocdepth}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6}Misc. questions}{43}{subsection.5.6}\protected@file@percent }
\@writefile{toc}{\setcounter {tocdepth}{2}}
\@writefile{toc}{\contentsline {section}{Appendices}{44}{section*.24}\protected@file@percent }
\@writefile{toc}{\setcounter {tocdepth}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Probability Theory Things}{44}{appendix.a.A}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}From Bernoulli to Binomial}{44}{subsection.a.A.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}From Categorical to Multinomial}{45}{subsection.a.A.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Negative Binomial and Geometric}{46}{subsection.a.A.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4}Entropy}{47}{subsection.a.A.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.4.1}Example of using entropy}{48}{subsubsection.a.A.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.5}The Central Limit Theorem (CLT)}{49}{subsection.a.A.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B}Statistics Things}{49}{appendix.a.B}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Independent samples}{49}{subsection.a.B.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Assumptions of Normality}{49}{subsection.a.B.2}\protected@file@percent }
\gdef\svg@ink@ver@settings{{\m@ne }{inkscape}{\m@ne }}
\gdef \@abspage@last{49}
