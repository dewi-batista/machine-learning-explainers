\relax 
\providecommand{\transparent@use}[1]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}What is Machine Learning?}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Supervised Learning}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Linear Regression}{2}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Statistical Motivation}{5}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Goodness of fit: $R^2$}{6}{subsubsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Why call it `regression'?}{7}{subsubsection.2.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Logistic Regression}{8}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Statistical Motivation}{9}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Support Vector Machines (SVMs)}{10}{subsection.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A hard-margin SVM in two dimensions. Samples lying on the margin are referred to as support vectors.}}{11}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:SVM_hard_margin}{{1}{11}{A hard-margin SVM in two dimensions. Samples lying on the margin are referred to as support vectors}{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Hard-margin SVMs}{11}{subsubsection.2.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A soft-margin SVM in two dimensions. Samples are labelled according to their true class.}}{13}{figure.caption.3}\protected@file@percent }
\newlabel{fig:SVM_soft_margin}{{2}{13}{A soft-margin SVM in two dimensions. Samples are labelled according to their true class}{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Soft-margin SVMs}{13}{subsubsection.2.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Non-linearly separable data being transformed into linearly separable data.}}{14}{figure.caption.4}\protected@file@percent }
\newlabel{fig:SVM_non_linear}{{3}{14}{Non-linearly separable data being transformed into linearly separable data}{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}Non-linear SVMs}{14}{subsubsection.2.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Decision Trees and Random Forests}{15}{subsection.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces A random forest.}}{16}{figure.caption.5}\protected@file@percent }
\newlabel{fig:random_forest}{{4}{16}{A random forest}{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}The Bias-Variance Tradeoff}{16}{subsection.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.1}Underfitting and Overfitting}{16}{subsubsection.2.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Lots of data, little noise.}}{17}{figure.caption.6}\protected@file@percent }
\newlabel{fig:fitting_under}{{5}{17}{Lots of data, little noise}{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Little data, lots of noise.}}{17}{figure.caption.7}\protected@file@percent }
\newlabel{fig:fitting_over}{{6}{17}{Little data, lots of noise}{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.2}Derivation with MSE loss}{17}{subsubsection.2.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Expected risk, squared bias and variance against architecture complexity.}}{20}{figure.caption.8}\protected@file@percent }
\newlabel{fig:bias_variance}{{7}{20}{Expected risk, squared bias and variance against architecture complexity}{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.3}Double descent}{21}{subsubsection.2.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The double descent phenomenon, $n=|D_{\text  {train}}|$.}}{22}{figure.caption.9}\protected@file@percent }
\newlabel{fig:double_descent}{{8}{22}{The double descent phenomenon, $n=|D_{\text {train}}|$}{figure.caption.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Parameter Exploration and its Optimisation}{22}{section.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The graph $\{(\theta ,R_D(\theta ))|\theta \in \mathbb  {R}^2\}$ of a complex loss landscape and a visualisation of how me might hope gradient descent looks.}}{23}{figure.caption.10}\protected@file@percent }
\newlabel{fig:loss_landscape_complex}{{9}{23}{The graph $\{(\theta ,R_D(\theta ))|\theta \in \R ^2\}$ of a complex loss landscape and a visualisation of how me might hope gradient descent looks}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Gradient Descent}{23}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}In which direction does $f$ most ascend locally from $\mathbf  {x}^{(t)}$?}{24}{subsubsection.3.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces How the convergence of each method might look.}}{25}{figure.caption.11}\protected@file@percent }
\newlabel{fig:gradient_descent_types}{{10}{25}{How the convergence of each method might look}{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Sensitivity to $\mathbf  {x}^{(0)}$}{25}{subsubsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Batch learning}{25}{subsubsection.3.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}Batch + Layer normalisation}{26}{subsubsection.3.1.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Motivating regularisation. Two fits, both with 0 test error.}}{28}{figure.caption.17}\protected@file@percent }
\newlabel{fig:regularisation_two_plots}{{11}{28}{Motivating regularisation. Two fits, both with 0 test error}{figure.caption.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Regularisation}{28}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}L1 regularisation (LASSO)}{28}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Laplace pdf with means 0 and decreasing variances according to $\lambda \in \{0.5,1,2,3\}$.}}{29}{figure.caption.18}\protected@file@percent }
\newlabel{fig:laplace_pdf}{{12}{29}{Laplace pdf with means 0 and decreasing variances according to $\lambda \in \{0.5,1,2,3\}$}{figure.caption.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}L2 regularisation (Ridge)}{29}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Training and validation losses over 50 epochs. Lowest validation obtained at epoch 28.}}{30}{figure.caption.19}\protected@file@percent }
\newlabel{fig:early_stopping}{{13}{30}{Training and validation losses over 50 epochs. Lowest validation obtained at epoch 28}{figure.caption.19}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Early stopping}{30}{subsubsection.3.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces D