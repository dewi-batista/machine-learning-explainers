\relax 
\providecommand{\transparent@use}[1]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}What is Machine Learning?}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Supervised Learning}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Linear Regression}{2}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Statistical Motivation}{5}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Goodness of fit: $R^2$}{6}{subsubsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Why call it `regression'?}{7}{subsubsection.2.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Logistic Regression}{8}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Statistical Motivation}{9}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Support Vector Machines (SVMs)}{10}{subsection.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A hard-margin SVM in two dimensions. Samples lying on the margin are referred to as support vectors.}}{11}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:SVM_hard_margin}{{1}{11}{A hard-margin SVM in two dimensions. Samples lying on the margin are referred to as support vectors}{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Hard-margin SVMs}{11}{subsubsection.2.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A soft-margin SVM in two dimensions. Samples are labelled according to their true class.}}{13}{figure.caption.3}\protected@file@percent }
\newlabel{fig:SVM_soft_margin}{{2}{13}{A soft-margin SVM in two dimensions. Samples are labelled according to their true class}{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Soft-margin SVMs}{13}{subsubsection.2.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Non-linearly separable data being transformed into linearly separable data.}}{14}{figure.caption.4}\protected@file@percent }
\newlabel{fig:SVM_non_linear}{{3}{14}{Non-linearly separable data being transformed into linearly separable data}{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}Non-linear SVMs}{14}{subsubsection.2.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Decision Trees and Random Forests}{15}{subsection.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces A random forest.}}{16}{figure.caption.5}\protected@file@percent }
\newlabel{fig:random_forest}{{4}{16}{A random forest}{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}The Bias-Variance Tradeoff}{16}{subsection.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.1}Underfitting and Overfitting}{16}{subsubsection.2.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Lots of data, little noise.}}{17}{figure.caption.6}\protected@file@percent }
\newlabel{fig:fitting_under}{{5}{17}{Lots of data, little noise}{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Little data, lots of noise.}}{17}{figure.caption.7}\protected@file@percent }
\newlabel{fig:fitting_over}{{6}{17}{Little data, lots of noise}{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.2}Derivation with MSE loss}{17}{subsubsection.2.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Expected risk, squared bias and variance against architecture complexity.}}{20}{figure.caption.8}\protected@file@percent }
\newlabel{fig:bias_variance}{{7}{20}{Expected risk, squared bias and variance against architecture complexity}{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.3}Double descent}{21}{subsubsection.2.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The double descent phenomenon, $n=|D_{\text  {train}}|$.}}{22}{figure.caption.9}\protected@file@percent }
\newlabel{fig:double_descent}{{8}{22}{The double descent phenomenon, $n=|D_{\text {train}}|$}{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.4}\textcolor {red}{\textbf  {TODO: }}Grokking}{22}{subsubsection.2.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Parameter Exploration and its Optimisation}{22}{section.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The graph $\{(\theta ,R_D(\theta ))|\theta \in \mathbb  {R}^2\}$ of a complex loss landscape and a visualisation of how me might hope gradient descent looks.}}{23}{figure.caption.10}\protected@file@percent }
\newlabel{fig:loss_landscape_complex}{{9}{23}{The graph $\{(\theta ,R_D(\theta ))|\theta \in \R ^2\}$ of a complex loss landscape and a visualisation of how me might hope gradient descent looks}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Gradient Descent}{23}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}In which direction does $f$ most ascend locally from $\mathbf  {x}^{(t)}$?}{24}{subsubsection.3.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces How the convergence of each method might look.}}{25}{figure.caption.11}\protected@file@percent }
\newlabel{fig:gradient_descent_types}{{10}{25}{How the convergence of each method might look}{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Sensitivity to $\mathbf  {x}^{(0)}$}{25}{subsubsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Batch learning}{25}{subsubsection.3.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}Batch + Layer normalisation}{27}{subsubsection.3.1.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Motivating regularisation. Two fits, both with 0 test error.}}{28}{figure.caption.17}\protected@file@percent }
\newlabel{fig:regularisation_two_plots}{{11}{28}{Motivating regularisation. Two fits, both with 0 test error}{figure.caption.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Regularisation}{28}{subsection.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Laplace pdf with means 0 and decreasing variances according to $\lambda \in \{0.5,1,2,3\}$.}}{29}{figure.caption.18}\protected@file@percent }
\newlabel{fig:laplace_pdf}{{12}{29}{Laplace pdf with means 0 and decreasing variances according to $\lambda \in \{0.5,1,2,3\}$}{figure.caption.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}L1 regularisation (LASSO)}{29}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Training and validation losses over 50 epochs. Lowest validation obtained at epoch 28.}}{30}{figure.caption.19}\protected@file@percent }
\newlabel{fig:early_stopping}{{13}{30}{Training and validation losses over 50 epochs. Lowest validation obtained at epoch 28}{figure.caption.19}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}L2 regularisation (Ridge)}{30}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Early stopping}{31}{subsubsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4}Dropout}{31}{subsubsection.3.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Momentum + Adaptive Learning Rates}{31}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Momentum}{31}{subsubsection.3.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Dropout.}}{32}{figure.caption.20}\protected@file@percent }
\newlabel{fig:dropout}{{14}{32}{Dropout}{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces How we might hope convergence would look using momentum.}}{32}{figure.caption.21}\protected@file@percent }
\newlabel{fig:gradient_descent_momentum}{{15}{32}{How we might hope convergence would look using momentum}{figure.caption.21}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Adaptive learning rates}{33}{subsubsection.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Warmup and cosine decay}{33}{subsubsection.3.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.4}Gradient accumulation and clipping}{33}{subsubsection.3.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Hyperparameter Tuning}{34}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Random search}{34}{subsubsection.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}$k-$fold cross-validation}{34}{subsubsection.3.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.3}Honourable mention: benchmark overfitting}{34}{subsubsection.3.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Neural Networks}{35}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Multi-Layer Perceptrons (MLPs)}{35}{subsection.4.1}\protected@file@percent }
\newlabel{sec:multi_layer_perceptrons}{{4.1}{35}{Multi-Layer Perceptrons (MLPs)}{subsection.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces A multi-layer perceptron (MLP) with $k=3$ hidden layers.}}{36}{figure.caption.22}\protected@file@percent }
\newlabel{fig:MLP}{{16}{36}{A multi-layer perceptron (MLP) with $k=3$ hidden layers}{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces A neural network with one hidden layer ($k=1$).}}{38}{figure.caption.23}\protected@file@percent }
\newlabel{fig:neural_nets_simple_example}{{17}{38}{A neural network with one hidden layer ($k=1$)}{figure.caption.23}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Universal Function Approximation with MLPs}{39}{subsubsection.4.1.1}\protected@file@percent }
\newlabel{subsubsec:universal_function_approximation_theorem}{{4.1.1}{39}{Universal Function Approximation with MLPs}{subsubsection.4.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces The $2-$probability simplex coloured according to argmax in the three coordinates.}}{43}{figure.caption.28}\protected@file@percent }
\newlabel{fig:2_prob_simplex}{{18}{43}{The $2-$probability simplex coloured according to argmax in the three coordinates}{figure.caption.28}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Posing classification as a regression problem}{43}{subsubsection.4.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Backpropagation for MLPs}{44}{subsection.4.2}\protected@file@percent }
\newlabel{subsec:backprop}{{4.2}{44}{Backpropagation for MLPs}{subsection.4.2}{}}
\newlabel{eq:back_prop_statement_2}{{4.2}{46}{Backpropagation for MLPs}{Item.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces A binary classification CNN processing an image of a frog.}}{47}{figure.caption.29}\protected@file@percent }
\newlabel{fig:CNN_frog}{{19}{47}{A binary classification CNN processing an image of a frog}{figure.caption.29}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Convolutional Neural Networks (CNNs)}{47}{subsection.4.3}\protected@file@percent }
\newlabel{subsec:conv_neural_networks}{{4.3}{47}{Convolutional Neural Networks (CNNs)}{subsection.4.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Convolutions}{47}{subsubsection.4.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Convolution with a vertical edge detection filter with 0 bias.}}{48}{figure.caption.30}\protected@file@percent }
\newlabel{fig:convolution_operation}{{20}{48}{Convolution with a vertical edge detection filter with 0 bias}{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces The level of abstraction of a CNN's learned filters. Earlier layers pertain to low-level features like edges or textures. Later layers pertain to higher-level features like faces or wheels.}}{48}{figure.caption.31}\protected@file@percent }
\newlabel{fig:filter_levels}{{21}{48}{The level of abstraction of a CNN's learned filters. Earlier layers pertain to low-level features like edges or textures. Later layers pertain to higher-level features like faces or wheels}{figure.caption.31}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Pooling}{49}{subsubsection.4.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Common flavours of pooling.}}{49}{figure.caption.33}\protected@file@percent }
\newlabel{fig:pooling}{{22}{49}{Common flavours of pooling}{figure.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces A decoder-only language transformer consisting of $N$ transformer blocks.}}{50}{figure.caption.34}\protected@file@percent }
\newlabel{fig:transformer-block}{{23}{50}{A decoder-only language transformer consisting of $N$ transformer blocks}{figure.caption.34}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}\textcolor {red}{\textbf  {TODO: }}Backpropagation for CNNs}{50}{subsubsection.4.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}\textcolor {red}{\textbf  {TODO: }}Recurrent Neural Networks (RNNs)}{50}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Long Short-Term Memory (LSTMs)}{50}{subsubsection.4.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Gated Recurrent Units (GRUs)}{50}{subsubsection.4.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}\textcolor {red}{\textbf  {TODO: }}Transformers}{50}{subsection.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.1}Tokenisation}{50}{subsubsection.4.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.2}\textcolor {red}{\textbf  {TODO: }}Embedding}{51}{subsubsection.4.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.3}Positional encoding (for text, learned absolute positional)}{51}{subsubsection.4.5.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Bit-reversed binary representations of sequence indeces.}}{53}{table.caption.35}\protected@file@percent }
\newlabel{tab:binary-encodings}{{1}{53}{Bit-reversed binary representations of sequence indeces}{table.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Binary signals (top) and sin signals (bottom).}}{54}{figure.caption.37}\protected@file@percent }
\newlabel{fig:bin-and-sin-signals}{{24}{54}{Binary signals (top) and sin signals (bottom)}{figure.caption.37}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.4}Attention}{54}{subsubsection.4.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.5}MLP component}{54}{subsubsection.4.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.6}Final transformer block output}{54}{subsubsection.4.5.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Non-linearly separable data (left) and its image under a non-linear transformation (right).}}{55}{figure.caption.38}\protected@file@percent }
\newlabel{fig:non_linearly_separable}{{25}{55}{Non-linearly separable data (left) and its image under a non-linear transformation (right)}{figure.caption.38}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.7}Example transformer with numbers}{55}{subsubsection.4.5.7}\protected@file@percent }
\@writefile{toc}{\setcounter {tocdepth}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Misc. questions}{55}{subsection.4.6}\protected@file@percent }
\@writefile{toc}{\setcounter {tocdepth}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Generative Modelling}{57}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}\textcolor {red}{\textbf  {TODO: }}Hopfield networks and Boltzmann machines}{58}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}\textcolor {myYellow}{\textbf  {IMPROVE: }}Bayesian networks}{58}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}\textcolor {red}{\textbf  {TODO: }}Learning $\mathcal  {G}$ and $\theta $ from data}{59}{subsubsection.5.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}\textcolor {red}{\textbf  {TODO: }}Inference complexity and tree-width}{59}{subsubsection.5.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces \textcolor {red}{\textbf  {TODO: }}make own figure}}{61}{figure.caption.39}\protected@file@percent }
\newlabel{fig:tree_decomp}{{26}{61}{\TODO make own figure}{figure.caption.39}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces \textcolor {red}{\textbf  {TODO: }}make own figure. also, what is triangulation?}}{62}{figure.caption.40}\protected@file@percent }
\newlabel{fig:DAG_moralisation}{{27}{62}{\TODO make own figure. also, what is triangulation?}{figure.caption.40}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Variational Autoencoders (VAEs)}{62}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}Autoencoders: no variation yet!}{62}{subsubsection.5.3.1}\protected@file@percent }
\newlabel{sec:autoencoders}{{5.3.1}{62}{Autoencoders: no variation yet!}{subsubsection.5.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces An autoencoder in which $\phi $ and $\theta $ are fit using multi-layer perceptrons (MLPs).}}{63}{figure.caption.44}\protected@file@percent }
\newlabel{fig:autoencoder}{{28}{63}{An autoencoder in which $\phi $ and $\theta $ are fit using multi-layer perceptrons (MLPs)}{figure.caption.44}{}}
\newlabel{ex:autoencoder}{{5.3}{63}{Autoencoders: no variation yet!}{example.5.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2}Motivating VAEs}{64}{subsubsection.5.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces Intuitive ideas for using an autoencoder as a generative model which fail.}}{65}{figure.caption.45}\protected@file@percent }
\newlabel{fig:autoencoder_generation}{{29}{65}{Intuitive ideas for using an autoencoder as a generative model which fail}{figure.caption.45}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.3}Formulating VAEs}{65}{subsubsection.5.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces Left: an encoder which outputs parameters of the latent distribution $q_{\phi }(\mathbf  {z}|\mathbf  {x})=\mathcal  {N}(\mu _{\phi },\Sigma _{\phi })$. Right: a decoder which outputs parameters of the reconstruction distribution $p_{\theta }(\mathbf  {x}|\mathbf  {z}')=\mathcal  {N}(\mu _{\theta },\Sigma _{\theta })$ in which $\mathbf  {z}'\sim q_{\phi }(\mathbf  {z}|\mathbf  {x})$.}}{66}{figure.caption.46}\protected@file@percent }
\newlabel{fig:VAE_architecture}{{30}{66}{Left: an encoder which outputs parameters of the latent distribution $q_{\phi }(\z |\x )=\mathcal {N}(\mu _{\phi },\Sigma _{\phi })$. Right: a decoder which outputs parameters of the reconstruction distribution $p_{\theta }(\x |\z ')=\mathcal {N}(\mu _{\theta },\Sigma _{\theta })$ in which $\z '\sim q_{\phi }(\z |\x )$}{figure.caption.46}{}}
\newlabel{eq:marginal_likelihood}{{1}{68}{Formulating VAEs}{equation.5.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.4}\textcolor {red}{\textbf  {TODO: }}Backpropagation for VAEs (the reparameterisation trick)}{69}{subsubsection.5.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.5}\textcolor {red}{\textbf  {TODO: }}Quirks of VAEs}{69}{subsubsection.5.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}\textcolor {myYellow}{\textbf  {IMPROVE: }}Generative Adversarial Networks (GANs)}{70}{subsection.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.1}Quirks of GANs}{70}{subsubsection.5.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.2}Flavours of GANs}{70}{subsubsection.5.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces Example GAN architecture.}}{71}{figure.caption.47}\protected@file@percent }
\newlabel{fig:GAN_architecture}{{31}{71}{Example GAN architecture}{figure.caption.47}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}\textcolor {red}{\textbf  {TODO: }}Normalising Flows}{71}{subsection.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6}Diffusion Models}{71}{subsection.5.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces Johann Bernoulli being denoised in line with the backward process of a diffusion model.}}{72}{figure.caption.48}\protected@file@percent }
\newlabel{fig:diffusion_process}{{32}{72}{Johann Bernoulli being denoised in line with the backward process of a diffusion model}{figure.caption.48}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.6.1}U-Net denoisers}{72}{subsubsection.5.6.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {33}{\ignorespaces Example U-Net architecture.}}{73}{figure.caption.49}\protected@file@percent }
\newlabel{fig:u_net_arch}{{33}{73}{Example U-Net architecture}{figure.caption.49}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7}\textcolor {red}{\textbf  {TODO: }}Evaluating Generative Models}{73}{subsection.5.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Appendices}{74}{section*.50}\protected@file@percent }
\@writefile{toc}{\setcounter {tocdepth}{0}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Probability Theory \& Statistics Things}{74}{appendix.a.A}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}\textcolor {myYellow}{\textbf  {IMPROVE: }}The Curse of Dimensionality}{74}{subsection.a.A.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Jensen's Inequality}{75}{subsection.a.A.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Entropy and its Friend KL-divergence}{75}{subsection.a.A.3}\protected@file@percent }
\newlabel{app:entropy}{{A.3}{75}{Entropy and its Friend KL-divergence}{subsection.a.A.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.3.1}Kullback-Leibler Divergence (KL-divergence)}{76}{subsubsection.a.A.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {34}{\ignorespaces Our geometric distribution $p$ and Poisson fit $q$.}}{78}{figure.caption.52}\protected@file@percent }
\newlabel{fig:poi_geo_plot}{{34}{78}{Our geometric distribution $p$ and Poisson fit $q$}{figure.caption.52}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4}The Expectation-Maximisation (EM) Algorithm}{79}{subsection.a.A.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.5}\textcolor {red}{\textbf  {TODO: }}Quality of fit $\approx $ Encoding quality?}{81}{subsection.a.A.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B}Philosophy Things}{82}{appendix.a.B}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Thoughts on reasoning (Summer 2024)}{82}{subsection.a.B.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}\textcolor {red}{\textbf  {TODO: }}The Sufficiency of Internet-Scraped Data}{83}{subsection.a.B.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}\textcolor {red}{\textbf  {TODO: }}The Principled Measure-Explainability Tradeoff}{83}{subsection.a.B.3}\protected@file@percent }
\gdef\svg@ink@ver@settings{{\m@ne }{inkscape}{\m@ne }}
\gdef \@abspage@last{84}
