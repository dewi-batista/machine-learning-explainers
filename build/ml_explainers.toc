\contentsline {section}{\numberline {1}What is Machine Learning?}{1}{section.1}%
\contentsline {section}{\numberline {2}Supervised Learning}{2}{section.2}%
\contentsline {subsection}{\numberline {2.1}Linear Regression}{2}{subsection.2.1}%
\contentsline {subsubsection}{\numberline {2.1.1}Statistical Motivation}{5}{subsubsection.2.1.1}%
\contentsline {subsubsection}{\numberline {2.1.2}Goodness of fit: $R^2$}{6}{subsubsection.2.1.2}%
\contentsline {subsubsection}{\numberline {2.1.3}Why call it `regression'?}{7}{subsubsection.2.1.3}%
\contentsline {subsection}{\numberline {2.2}Logistic Regression}{8}{subsection.2.2}%
\contentsline {subsubsection}{\numberline {2.2.1}Statistical Motivation}{9}{subsubsection.2.2.1}%
\contentsline {subsection}{\numberline {2.3}Support Vector Machines (SVMs)}{10}{subsection.2.3}%
\contentsline {subsubsection}{\numberline {2.3.1}Hard-margin SVMs}{11}{subsubsection.2.3.1}%
\contentsline {subsubsection}{\numberline {2.3.2}Soft-margin SVMs}{13}{subsubsection.2.3.2}%
\contentsline {subsubsection}{\numberline {2.3.3}Non-linear SVMs}{14}{subsubsection.2.3.3}%
\contentsline {subsection}{\numberline {2.4}Decision Trees and Random Forests}{15}{subsection.2.4}%
\contentsline {subsection}{\numberline {2.5}The Bias-Variance Tradeoff}{16}{subsection.2.5}%
\contentsline {subsubsection}{\numberline {2.5.1}Underfitting and Overfitting}{16}{subsubsection.2.5.1}%
\contentsline {subsubsection}{\numberline {2.5.2}Derivation with MSE loss}{17}{subsubsection.2.5.2}%
\contentsline {subsubsection}{\numberline {2.5.3}Double descent}{21}{subsubsection.2.5.3}%
\contentsline {section}{\numberline {3}Parameter Exploration and its Optimisation}{22}{section.3}%
\contentsline {subsection}{\numberline {3.1}Gradient Descent}{23}{subsection.3.1}%
\contentsline {subsubsection}{\numberline {3.1.1}In which direction does $f$ most ascend locally from $\mathbf {x}^{(t)}$?}{24}{subsubsection.3.1.1}%
\contentsline {subsubsection}{\numberline {3.1.2}Sensitivity to $\mathbf {x}^{(0)}$}{25}{subsubsection.3.1.2}%
\contentsline {subsubsection}{\numberline {3.1.3}Batch learning}{25}{subsubsection.3.1.3}%
\contentsline {subsubsection}{\numberline {3.1.4}Batch + Layer normalisation}{26}{subsubsection.3.1.4}%
\contentsline {subsection}{\numberline {3.2}Regularisation}{28}{subsection.3.2}%
\contentsline {subsection}{\numberline {3.3}Momentum + Adaptive Learning Rates}{31}{subsection.3.3}%
\contentsline {subsection}{\numberline {3.4}\textcolor {myYellow}{\textbf {IMPROVE: }}Hyperparameter Tuning}{33}{subsection.3.4}%
\contentsline {section}{\numberline {4}Neural Networks}{34}{section.4}%
\contentsline {subsection}{\numberline {4.1}Multi-Layer Perceptrons (MLPs)}{34}{subsection.4.1}%
\contentsline {subsubsection}{\numberline {4.1.1}Example}{37}{subsubsection.4.1.1}%
\contentsline {subsubsection}{\numberline {4.1.2}Universal Function Approximation with MLPs}{39}{subsubsection.4.1.2}%
\contentsline {subsubsection}{\numberline {4.1.3}Posing classification as a regression problem}{42}{subsubsection.4.1.3}%
\contentsline {subsection}{\numberline {4.2}Backpropagation for MLPs}{43}{subsection.4.2}%
\contentsline {subsection}{\numberline {4.3}Convolutional Neural Networks (CNNs)}{46}{subsection.4.3}%
\contentsline {subsubsection}{\numberline {4.3.1}Convolutions}{47}{subsubsection.4.3.1}%
\contentsline {subsubsection}{\numberline {4.3.2}Pooling}{48}{subsubsection.4.3.2}%
\contentsline {subsection}{\numberline {4.4}\textcolor {red}{\textbf {TODO: }}Recurrent Neural Networks (RNNs)}{49}{subsection.4.4}%
\contentsline {subsubsection}{\numberline {4.4.1}Long Short-Term Memory (LSTMs)}{49}{subsubsection.4.4.1}%
\contentsline {subsubsection}{\numberline {4.4.2}Gated Recurrent Units (GRUs)}{49}{subsubsection.4.4.2}%
\contentsline {subsection}{\numberline {4.5}\textcolor {red}{\textbf {TODO: }}Transformers}{49}{subsection.4.5}%
\contentsline {subsubsection}{\numberline {4.5.1}Tokens and Embedding}{49}{subsubsection.4.5.1}%
\contentsline {subsubsection}{\numberline {4.5.2}Positional encoding}{50}{subsubsection.4.5.2}%
\contentsline {subsubsection}{\numberline {4.5.3}Attention}{50}{subsubsection.4.5.3}%
\contentsline {subsubsection}{\numberline {4.5.4}Transformer blocks}{50}{subsubsection.4.5.4}%
\setcounter {tocdepth}{1}
\contentsline {subsection}{\numberline {4.6}Misc. questions}{50}{subsection.4.6}%
\setcounter {tocdepth}{2}
\contentsline {section}{\numberline {5}Generative Models}{53}{section.5}%
\contentsline {subsection}{\numberline {5.1}\textcolor {myYellow}{\textbf {IMPROVE: }}Bayesian networks (BNs)}{53}{subsection.5.1}%
\contentsline {subsubsection}{\numberline {5.1.1}Formulating BNs}{54}{subsubsection.5.1.1}%
\contentsline {subsubsection}{\numberline {5.1.2}Learning BNs from data}{54}{subsubsection.5.1.2}%
\contentsline {subsection}{\numberline {5.2}Variational Autoencoders (VAEs)}{54}{subsection.5.2}%
\contentsline {subsubsection}{\numberline {5.2.1}Autoencoders: no variation yet!}{54}{subsubsection.5.2.1}%
\contentsline {subsubsection}{\numberline {5.2.2}Motivating VAEs}{56}{subsubsection.5.2.2}%
\contentsline {subsubsection}{\numberline {5.2.3}Formulating VAEs}{57}{subsubsection.5.2.3}%
\contentsline {subsubsection}{\numberline {5.2.4}\textcolor {purple}{\textbf {REVIEW: }}Backpropagation for VAEs}{61}{subsubsection.5.2.4}%
\contentsline {subsubsection}{\numberline {5.2.5}\textcolor {red}{\textbf {TODO: }}Quirks of VAEs}{61}{subsubsection.5.2.5}%
\contentsline {subsection}{\numberline {5.3}\textcolor {myYellow}{\textbf {IMPROVE: }}Generative Adversarial Networks (GANs)}{61}{subsection.5.3}%
\contentsline {subsubsection}{\numberline {5.3.1}Quirks of GANs}{62}{subsubsection.5.3.1}%
\contentsline {subsubsection}{\numberline {5.3.2}Flavours of GANs}{63}{subsubsection.5.3.2}%
\contentsline {subsection}{\numberline {5.4}\textcolor {red}{\textbf {TODO: }}Normalising Flows}{63}{subsection.5.4}%
\contentsline {subsection}{\numberline {5.5}Diffusion Models}{63}{subsection.5.5}%
\contentsline {subsubsection}{\numberline {5.5.1}U-Net denoisers}{64}{subsubsection.5.5.1}%
\contentsline {subsection}{\numberline {5.6}\textcolor {red}{\textbf {TODO: }}Evaluating Generative Models}{65}{subsection.5.6}%
\contentsline {section}{Appendices}{66}{section*.49}%
\setcounter {tocdepth}{0}
\contentsline {section}{\numberline {A}Probability Theory Things}{66}{appendix.a.A}%
\contentsline {subsection}{\numberline {A.1}Derivations Related to Common Distributions}{66}{subsection.a.A.1}%
\contentsline {subsection}{\numberline {A.2}Motivating Variance}{70}{subsection.a.A.2}%
\contentsline {subsection}{\numberline {A.3}\textcolor {red}{\textbf {TODO: }}Flavours of Convergence of Random Variables}{70}{subsection.a.A.3}%
\contentsline {subsection}{\numberline {A.4}The Law of Large Numbers}{71}{subsection.a.A.4}%
\contentsline {subsection}{\numberline {A.5}The (univariate) Central Limit Theorem (CLT)}{71}{subsection.a.A.5}%
\contentsline {subsection}{\numberline {A.6}Jensen's Inequality}{72}{subsection.a.A.6}%
\contentsline {subsection}{\numberline {A.7}Entropy and its Friend KL-divergence}{72}{subsection.a.A.7}%
\contentsline {subsubsection}{\numberline {A.7.1}Kullback-Leibler Divergence (KL-divergence)}{73}{subsubsection.a.A.7.1}%
\contentsline {subsection}{\numberline {A.8}Pearson Correlation and Mutual Information}{76}{subsection.a.A.8}%
\contentsline {subsection}{\numberline {A.9}Quality of fit $\approx $ Encoding quality?}{77}{subsection.a.A.9}%
\contentsline {section}{\numberline {B}Classical Statistics Things}{77}{appendix.a.B}%
\contentsline {subsection}{\numberline {B.1}Sample Independence and Terminology}{77}{subsection.a.B.1}%
\contentsline {subsection}{\numberline {B.2}Estimators and their Bias}{77}{subsection.a.B.2}%
\contentsline {subsubsection}{\numberline {B.2.1}Sampling bias}{79}{subsubsection.a.B.2.1}%
\contentsline {subsubsection}{\numberline {B.2.2}Efficiency, Consistency and }{79}{subsubsection.a.B.2.2}%
\contentsline {subsection}{\numberline {B.3}\textcolor {red}{\textbf {TODO: }}Hypothesis Testing}{79}{subsection.a.B.3}%
\contentsline {subsection}{\numberline {B.4}\textcolor {red}{\textbf {TODO: }}The Curse of Dimensionality}{79}{subsection.a.B.4}%
\contentsline {subsection}{\numberline {B.5}\textcolor {red}{\textbf {TODO: }}Markov Chain Monte Carlo (MCMC) Methods}{79}{subsection.a.B.5}%
\contentsline {subsubsection}{\numberline {B.5.1}Markov Chains}{79}{subsubsection.a.B.5.1}%
\contentsline {subsubsection}{\numberline {B.5.2}The Metropolis-Hastings algorithm}{81}{subsubsection.a.B.5.2}%
\contentsline {subsubsection}{\numberline {B.5.3}Gibbs sampling}{81}{subsubsection.a.B.5.3}%
\contentsline {subsection}{\numberline {B.6}Hidden Markov Models (HMMs)}{81}{subsection.a.B.6}%
\contentsline {subsection}{\numberline {B.7}The Expectation-Maximisation (EM) Algorithm}{81}{subsection.a.B.7}%
\contentsline {section}{\numberline {C}Bayesian Persuasion}{83}{appendix.a.C}%
\contentsline {subsection}{\numberline {C.1}Posetriors Over Parameters}{84}{subsection.a.C.1}%
\contentsline {subsection}{\numberline {C.2}Belief Propagation}{84}{subsection.a.C.2}%
\contentsline {subsection}{\numberline {C.3}Variational Bayesian inference}{84}{subsection.a.C.3}%
