\contentsline {section}{\numberline {1}What is Machine Learning?}{1}{section.1}%
\contentsline {section}{\numberline {2}Supervised Learning}{2}{section.2}%
\contentsline {subsection}{\numberline {2.1}Linear Regression}{2}{subsection.2.1}%
\contentsline {subsubsection}{\numberline {2.1.1}Statistical Motivation}{5}{subsubsection.2.1.1}%
\contentsline {subsubsection}{\numberline {2.1.2}Goodness of fit: $R^2$}{6}{subsubsection.2.1.2}%
\contentsline {subsubsection}{\numberline {2.1.3}Why call it `regression'?}{7}{subsubsection.2.1.3}%
\contentsline {subsection}{\numberline {2.2}Logistic Regression}{8}{subsection.2.2}%
\contentsline {subsubsection}{\numberline {2.2.1}Statistical Motivation}{9}{subsubsection.2.2.1}%
\contentsline {subsection}{\numberline {2.3}Support Vector Machines (SVMs)}{10}{subsection.2.3}%
\contentsline {subsubsection}{\numberline {2.3.1}Hard-margin SVMs}{11}{subsubsection.2.3.1}%
\contentsline {subsubsection}{\numberline {2.3.2}Soft-margin SVMs}{13}{subsubsection.2.3.2}%
\contentsline {subsubsection}{\numberline {2.3.3}Non-linear SVMs}{14}{subsubsection.2.3.3}%
\contentsline {subsection}{\numberline {2.4}Decision Trees and Random Forests}{15}{subsection.2.4}%
\contentsline {subsection}{\numberline {2.5}The Bias-Variance Tradeoff}{16}{subsection.2.5}%
\contentsline {subsubsection}{\numberline {2.5.1}Underfitting and Overfitting}{16}{subsubsection.2.5.1}%
\contentsline {subsubsection}{\numberline {2.5.2}Derivation with MSE loss}{17}{subsubsection.2.5.2}%
\contentsline {subsubsection}{\numberline {2.5.3}Double descent}{21}{subsubsection.2.5.3}%
\contentsline {section}{\numberline {3}Parameter Exploration and its Optimisation}{22}{section.3}%
\contentsline {subsection}{\numberline {3.1}Gradient Descent}{23}{subsection.3.1}%
\contentsline {subsubsection}{\numberline {3.1.1}In which direction does $f$ most ascend locally from $\mathbf {x}^{(t)}$?}{24}{subsubsection.3.1.1}%
\contentsline {subsubsection}{\numberline {3.1.2}Sensitivity to $\mathbf {x}^{(0)}$}{25}{subsubsection.3.1.2}%
\contentsline {subsubsection}{\numberline {3.1.3}Batch learning}{25}{subsubsection.3.1.3}%
\contentsline {subsubsection}{\numberline {3.1.4}Batch + Layer + RMS normalisation}{26}{subsubsection.3.1.4}%
\contentsline {subsection}{\numberline {3.2}Regularisation}{28}{subsection.3.2}%
\contentsline {subsubsection}{\numberline {3.2.1}L1 regularisation (LASSO)}{29}{subsubsection.3.2.1}%
\contentsline {subsubsection}{\numberline {3.2.2}L2 regularisation (Ridge)}{29}{subsubsection.3.2.2}%
\contentsline {subsubsection}{\numberline {3.2.3}Early stopping}{30}{subsubsection.3.2.3}%
\contentsline {subsubsection}{\numberline {3.2.4}Dropout}{31}{subsubsection.3.2.4}%
\contentsline {subsection}{\numberline {3.3}Momentum + Adaptive Learning Rates}{31}{subsection.3.3}%
\contentsline {subsubsection}{\numberline {3.3.1}Momentum}{32}{subsubsection.3.3.1}%
\contentsline {subsubsection}{\numberline {3.3.2}Adaptive learning rates}{33}{subsubsection.3.3.2}%
\contentsline {subsubsection}{\numberline {3.3.3}\textcolor {myYellow}{\textbf {NEXT: }}Warmup + decay}{33}{subsubsection.3.3.3}%
\contentsline {subsubsection}{\numberline {3.3.4}Gradient accumulation + clipping}{34}{subsubsection.3.3.4}%
\contentsline {subsection}{\numberline {3.4}Hyperparameter Tuning}{34}{subsection.3.4}%
\contentsline {subsubsection}{\numberline {3.4.1}Random search}{34}{subsubsection.3.4.1}%
\contentsline {subsubsection}{\numberline {3.4.2}$k-$fold cross-validation}{34}{subsubsection.3.4.2}%
\contentsline {subsubsection}{\numberline {3.4.3}Honourable mention: benchmark overfitting}{34}{subsubsection.3.4.3}%
\contentsline {subsection}{\numberline {3.5}\textcolor {red}{\textbf {TODO: }}Quantisation}{35}{subsection.3.5}%
\contentsline {section}{\numberline {4}Neural Network Architectures}{35}{section.4}%
\contentsline {subsection}{\numberline {4.1}Multi-Layer Perceptrons (MLPs)}{35}{subsection.4.1}%
\contentsline {subsubsection}{\numberline {4.1.1}Universal Function Approximation with MLPs}{39}{subsubsection.4.1.1}%
\contentsline {subsubsection}{\numberline {4.1.2}Posing classification as a regression problem}{43}{subsubsection.4.1.2}%
\contentsline {subsection}{\numberline {4.2}Convolutional Neural Networks (CNNs)}{44}{subsection.4.2}%
\contentsline {subsubsection}{\numberline {4.2.1}Convolutions}{44}{subsubsection.4.2.1}%
\contentsline {subsubsection}{\numberline {4.2.2}Pooling}{46}{subsubsection.4.2.2}%
\contentsline {subsection}{\numberline {4.3}\textcolor {red}{\textbf {TODO: }}Recurrent Neural Networks (RNNs)}{46}{subsection.4.3}%
\contentsline {subsection}{\numberline {4.4}\textcolor {myYellow}{\textbf {NEXT: }}Transformers}{47}{subsection.4.4}%
\contentsline {subsubsection}{\numberline {4.4.1}Tokenisation}{47}{subsubsection.4.4.1}%
\contentsline {subsubsection}{\numberline {4.4.2}Embedding}{48}{subsubsection.4.4.2}%
\contentsline {subsubsection}{\numberline {4.4.3}Positional encoding/embedding}{49}{subsubsection.4.4.3}%
\contentsline {subsubsection}{\numberline {4.4.4}\textcolor {myYellow}{\textbf {NEXT: }}Attention + MLP component}{51}{subsubsection.4.4.4}%
\contentsline {subsubsection}{\numberline {4.4.5}Example: GPT-2 small ($\sim $123M parameters)}{52}{subsubsection.4.4.5}%
\contentsline {subsection}{\numberline {4.5}Backpropagation}{52}{subsection.4.5}%
\contentsline {subsubsection}{\numberline {4.5.1}Reverse-mode auto-differentiation (backprop)}{53}{subsubsection.4.5.1}%
\contentsline {subsubsection}{\numberline {4.5.2}Backpropagation for MLPs}{55}{subsubsection.4.5.2}%
\setcounter {tocdepth}{1}
\contentsline {subsection}{\numberline {4.6}Misc. questions}{58}{subsection.4.6}%
\setcounter {tocdepth}{2}
\contentsline {section}{\numberline {5}Deep Generative Modelling}{61}{section.5}%
\contentsline {subsection}{\numberline {5.1}Bayesian networks}{62}{subsection.5.1}%
\contentsline {subsubsection}{\numberline {5.1.1}Learning them from data}{63}{subsubsection.5.1.1}%
\contentsline {subsubsection}{\numberline {5.1.2}Why aren't they employed in deep learning?}{64}{subsubsection.5.1.2}%
\contentsline {subsection}{\numberline {5.2}Variational Autoencoders (VAEs)}{64}{subsection.5.2}%
\contentsline {subsubsection}{\numberline {5.2.1}Autoencoders: no variation yet!}{65}{subsubsection.5.2.1}%
\contentsline {subsubsection}{\numberline {5.2.2}Motivating VAEs}{66}{subsubsection.5.2.2}%
\contentsline {subsubsection}{\numberline {5.2.3}Formulating VAEs}{67}{subsubsection.5.2.3}%
\contentsline {subsubsection}{\numberline {5.2.4}Backpropagation for VAEs (the reparameterisation trick)}{72}{subsubsection.5.2.4}%
\contentsline {subsubsection}{\numberline {5.2.5}Blurry reconstructions}{74}{subsubsection.5.2.5}%
\contentsline {subsubsection}{\numberline {5.2.6}Working in latent space}{75}{subsubsection.5.2.6}%
\contentsline {subsection}{\numberline {5.3}Generative Adversarial Networks (GANs)}{76}{subsection.5.3}%
\contentsline {subsubsection}{\numberline {5.3.1}\textcolor {myYellow}{\textbf {NEXT: }}Flavours of GANs}{77}{subsubsection.5.3.1}%
\contentsline {subsection}{\numberline {5.4}\textcolor {red}{\textbf {TODO: }}Normalising Flows}{77}{subsection.5.4}%
\contentsline {subsection}{\numberline {5.5}Diffusion Models}{77}{subsection.5.5}%
\contentsline {subsubsection}{\numberline {5.5.1}U-Net denoisers}{79}{subsubsection.5.5.1}%
\contentsline {subsection}{\numberline {5.6}\textcolor {red}{\textbf {TODO: }}Generative Modelling $\approx $ Compression?}{80}{subsection.5.6}%
\contentsline {subsection}{\numberline {5.7}\textcolor {myYellow}{\textbf {NEXT: }}Evaluating Generative Models}{80}{subsection.5.7}%
\contentsline {subsubsection}{\numberline {5.7.1}Density estimation}{80}{subsubsection.5.7.1}%
\contentsline {subsubsection}{\numberline {5.7.2}Sampled images}{80}{subsubsection.5.7.2}%
\contentsline {subsubsection}{\numberline {5.7.3}Sampled text}{80}{subsubsection.5.7.3}%
\contentsline {section}{Appendices}{81}{section*.48}%
\setcounter {tocdepth}{0}
\contentsline {section}{\numberline {A}Probability Theory \& Statistics Things}{81}{appendix.a.A}%
\contentsline {subsection}{\numberline {A.1}Jensen's Inequality}{81}{subsection.a.A.1}%
\contentsline {subsection}{\numberline {A.2}Entropy and its Friend KL-divergence}{81}{subsection.a.A.2}%
\contentsline {subsubsection}{\numberline {A.2.1}Kullback-Leibler Divergence (KL-divergence)}{82}{subsubsection.a.A.2.1}%
\contentsline {subsection}{\numberline {A.3}The Expectation-Maximisation (EM) Algorithm}{85}{subsection.a.A.3}%
\contentsline {section}{\numberline {B}Miscellaneous Thoughts}{87}{appendix.a.B}%
\contentsline {subsection}{\numberline {B.1}LLM reasoning [Summer 2024]}{88}{subsection.a.B.1}%
\contentsline {subsection}{\numberline {B.2}Backwards-compatible definition of AI [Winter 2026]}{89}{subsection.a.B.2}%
\contentsline {subsection}{\numberline {B.3}\textcolor {myYellow}{\textbf {NEXT: }}The principled measure-explainability tradeoff}{90}{subsection.a.B.3}%
\contentsline {subsection}{\numberline {B.4}\textcolor {red}{\textbf {TODO: }}The sufficiency of internet-scraped data}{90}{subsection.a.B.4}%
