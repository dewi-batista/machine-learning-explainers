\contentsline {section}{\numberline {1}What is Machine Learning?}{1}{section.1}%
\contentsline {section}{\numberline {2}Supervised Learning}{2}{section.2}%
\contentsline {subsection}{\numberline {2.1}Linear Regression}{3}{subsection.2.1}%
\contentsline {subsubsection}{\numberline {2.1.1}Statistical Motivation}{5}{subsubsection.2.1.1}%
\contentsline {subsubsection}{\numberline {2.1.2}Goodness of fit: $R^2$}{7}{subsubsection.2.1.2}%
\contentsline {subsubsection}{\numberline {2.1.3}Why call it `regression'?}{8}{subsubsection.2.1.3}%
\contentsline {subsection}{\numberline {2.2}Logistic Regression}{8}{subsection.2.2}%
\contentsline {subsubsection}{\numberline {2.2.1}Statistical Motivation}{10}{subsubsection.2.2.1}%
\contentsline {subsection}{\numberline {2.3}Support Vector Machines (SVMs)}{11}{subsection.2.3}%
\contentsline {subsubsection}{\numberline {2.3.1}Hard-margin SVMs}{12}{subsubsection.2.3.1}%
\contentsline {subsubsection}{\numberline {2.3.2}Soft-margin SVMs}{13}{subsubsection.2.3.2}%
\contentsline {subsubsection}{\numberline {2.3.3}Non-linear SVMs}{15}{subsubsection.2.3.3}%
\contentsline {subsection}{\numberline {2.4}Decision Trees and Random Forests}{16}{subsection.2.4}%
\contentsline {subsection}{\numberline {2.5}\textcolor {myYellow}{\textbf {REVIEW}: }The Bias-Variance Tradeoff}{17}{subsection.2.5}%
\contentsline {subsubsection}{\numberline {2.5.1}\textcolor {red}{\textbf {TODO}: }Underfitting and Overfitting}{17}{subsubsection.2.5.1}%
\contentsline {subsubsection}{\numberline {2.5.2}Derivation with MSE loss}{17}{subsubsection.2.5.2}%
\contentsline {subsubsection}{\numberline {2.5.3}\textcolor {myYellow}{\textbf {REVIEW}: }Beyond MSE}{21}{subsubsection.2.5.3}%
\contentsline {subsubsection}{\numberline {2.5.4}\textcolor {myYellow}{\textbf {REVIEW}: }Double descent}{21}{subsubsection.2.5.4}%
\contentsline {section}{\numberline {3}Parameter Exploration and its Optimisation}{23}{section.3}%
\contentsline {subsection}{\numberline {3.1}Gradient Descent}{24}{subsection.3.1}%
\contentsline {subsubsection}{\numberline {3.1.1}In which direction does $f$ most ascend locally from $\mathbf {x}^{(t)}$?}{25}{subsubsection.3.1.1}%
\contentsline {subsubsection}{\numberline {3.1.2}Sensitivity to $\mathbf {x}^{(0)}$}{26}{subsubsection.3.1.2}%
\contentsline {subsubsection}{\numberline {3.1.3}My dataset (i.e. $n$) is too large!!!}{26}{subsubsection.3.1.3}%
\contentsline {subsubsection}{\numberline {3.1.4}Batch + Layer normalisation}{27}{subsubsection.3.1.4}%
\contentsline {subsection}{\numberline {3.2}Regularisation}{29}{subsection.3.2}%
\contentsline {subsection}{\numberline {3.3}Momentum + Adaptive Learning Rates}{33}{subsection.3.3}%
\contentsline {subsection}{\numberline {3.4}Hyperparameter Tuning}{35}{subsection.3.4}%
\contentsline {section}{\numberline {4}Neural Networks}{36}{section.4}%
\contentsline {subsection}{\numberline {4.1}Multi-Layer Perceptrons (MLPs)}{36}{subsection.4.1}%
\contentsline {subsubsection}{\numberline {4.1.1}Example}{39}{subsubsection.4.1.1}%
\contentsline {subsubsection}{\numberline {4.1.2}The Universal Function Approximation of MLPs}{41}{subsubsection.4.1.2}%
\contentsline {subsubsection}{\numberline {4.1.3}Classification via neural networks}{44}{subsubsection.4.1.3}%
\contentsline {subsection}{\numberline {4.2}Backpropagation for MLPs}{45}{subsection.4.2}%
\contentsline {subsection}{\numberline {4.3}Convolutional Neural Networks (CNNs)}{48}{subsection.4.3}%
\contentsline {subsubsection}{\numberline {4.3.1}Convolutions}{49}{subsubsection.4.3.1}%
\contentsline {subsubsection}{\numberline {4.3.2}Pooling}{50}{subsubsection.4.3.2}%
\contentsline {subsubsection}{\numberline {4.3.3}Output dimensions after convolving and pooling}{51}{subsubsection.4.3.3}%
\contentsline {subsubsection}{\numberline {4.3.4}Inductive biase}{51}{subsubsection.4.3.4}%
\contentsline {subsection}{\numberline {4.4}\textcolor {red}{\textbf {TODO}: }Recurrent Neural Networks (RNNs)}{51}{subsection.4.4}%
\contentsline {subsubsection}{\numberline {4.4.1}Long Short-Term Memory (LSTMs)}{51}{subsubsection.4.4.1}%
\contentsline {subsubsection}{\numberline {4.4.2}Gated Recurrent Units (GRUs)}{51}{subsubsection.4.4.2}%
\contentsline {subsection}{\numberline {4.5}\textcolor {red}{\textbf {TODO}: }Transformers}{51}{subsection.4.5}%
\contentsline {subsubsection}{\numberline {4.5.1}Tokens and Embedding}{52}{subsubsection.4.5.1}%
\contentsline {subsubsection}{\numberline {4.5.2}Positional encoding}{52}{subsubsection.4.5.2}%
\contentsline {subsubsection}{\numberline {4.5.3}Architecture}{52}{subsubsection.4.5.3}%
\setcounter {tocdepth}{1}
\contentsline {subsection}{\numberline {4.6}Misc. questions}{52}{subsection.4.6}%
\setcounter {tocdepth}{2}
\contentsline {section}{\numberline {5}Generative Models}{55}{section.5}%
\contentsline {subsection}{\numberline {5.1}\textcolor {red}{\textbf {TODO} }Bayesian networks}{56}{subsection.5.1}%
\contentsline {subsection}{\numberline {5.2}Variational Autoencoders (VAEs)}{56}{subsection.5.2}%
\contentsline {subsubsection}{\numberline {5.2.1}Autoencoders: no variation yet!}{56}{subsubsection.5.2.1}%
\contentsline {subsubsection}{\numberline {5.2.2}Motivating VAEs}{58}{subsubsection.5.2.2}%
\contentsline {subsubsection}{\numberline {5.2.3}Formulating VAEs}{59}{subsubsection.5.2.3}%
\contentsline {subsubsection}{\numberline {5.2.4}Backpropagation for VAEs}{63}{subsubsection.5.2.4}%
\contentsline {subsection}{\numberline {5.3}\textcolor {red}{\textbf {TODO}: }Generative Adversarial Networks (GANs)}{64}{subsection.5.3}%
\contentsline {subsection}{\numberline {5.4}\textcolor {red}{\textbf {TODO}: }Normalising Flows}{64}{subsection.5.4}%
\contentsline {subsection}{\numberline {5.5}Diffusion Models}{64}{subsection.5.5}%
\contentsline {subsubsection}{\numberline {5.5.1}U-Net denoisers}{65}{subsubsection.5.5.1}%
\contentsline {subsection}{\numberline {5.6}\textcolor {red}{\textbf {TODO}: }Evaluating Generative Models}{66}{subsection.5.6}%
\contentsline {section}{\numberline {6}Object Detection Models}{66}{section.6}%
\contentsline {subsection}{\numberline {6.1}\textcolor {red}{\textbf {TODO}: }(Fast/Faster) R-CNN}{66}{subsection.6.1}%
\contentsline {subsection}{\numberline {6.2}\textcolor {red}{\textbf {TODO}: }YOLO}{67}{subsection.6.2}%
\contentsline {subsection}{\numberline {6.3}\textcolor {red}{\textbf {TODO}: }DETR}{67}{subsection.6.3}%
\contentsline {section}{Appendices}{68}{section*.47}%
\setcounter {tocdepth}{0}
\contentsline {section}{\numberline {A}Probability Theory and Statistics Things}{68}{appendix.a.A}%
\contentsline {subsection}{\numberline {A.1}Sample Independence}{68}{subsection.a.A.1}%
\contentsline {subsection}{\numberline {A.2}Derivations Related to Common Distributions}{68}{subsection.a.A.2}%
\contentsline {subsection}{\numberline {A.3}The Law of Large Numbers}{72}{subsection.a.A.3}%
\contentsline {subsection}{\numberline {A.4}The (univariate) Central Limit Theorem (CLT)}{72}{subsection.a.A.4}%
\contentsline {subsection}{\numberline {A.5}Jensen's Inequality}{73}{subsection.a.A.5}%
\contentsline {subsection}{\numberline {A.6}Motivating Variance and the Bias of Sample Variance}{73}{subsection.a.A.6}%
\contentsline {subsection}{\numberline {A.7}Entropy and its Friend KL-divergence}{73}{subsection.a.A.7}%
\contentsline {subsubsection}{\numberline {A.7.1}Kullback-Leibler Divergence (KL-divergence)}{74}{subsubsection.a.A.7.1}%
\contentsline {subsection}{\numberline {A.8}Pearson Correlation and Mutual Information}{77}{subsection.a.A.8}%
\contentsline {subsection}{\numberline {A.9}Quality of fit $\approx $ Encoding quality?}{77}{subsection.a.A.9}%
\contentsline {subsection}{\numberline {A.10}The Expectation-Maximisation (EM) Algorithm}{77}{subsection.a.A.10}%
\contentsline {section}{\numberline {B}Philosophy Things}{80}{appendix.a.B}%
\contentsline {subsection}{\numberline {B.1}Thoughts on reasoning (Summer 2024)}{80}{subsection.a.B.1}%
\contentsline {subsection}{\numberline {B.2}Sufficiency of Internet-Scraped Data}{81}{subsection.a.B.2}%
\contentsline {subsection}{\numberline {B.3}The Principled Measure-Explainability Tradeoff}{81}{subsection.a.B.3}%
