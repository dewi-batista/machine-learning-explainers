\documentclass[11pt]{article}
\input{header}

\begin{document}

% title
\begin{center}
\hspace*{\fill}\rule{0.95\linewidth}{0.1pt}\hspace*{\fill}\\[0.6cm]
% \phantom{123}\\[0.4cm]
{\huge\bfseries Machine Learning Explainers}\\[0.1cm]
\textbf{Last updated:} \today\\
\hspace*{\fill}\rule{0.95\linewidth}{0.1pt}\hspace*{\fill}\\
\end{center}

% abstract
\begin{abstract}
    \noindent Summaries of some machine/deep learning-related topics. My main motive in writing these summaries is as a reminder for my future self.
\end{abstract}

% table of contents
\setcounter{tocdepth}{2}{\footnotesize\tableofcontents}

% \tiny
% \scriptsize
% \footnotesize
% \small
% \normalsize
% \large
% \Large
% \LARGE
% \huge
% \Huge

%% ==

% title page number not counted
\thispagestyle{empty}
\setcounter{page}{0}
\newpage

% intro
\section{What is Machine Learning?}
I think of machine learning as the broad discipline of studying methods of fitting models to data — like statistics, as a discipline, if it demanded far less rigour. For a description of machine learning which helps to clarify why it's difficult to define as a term, consider the following excerpt from Herbert Jaeger's lecture notes for his Machine Learning course at the University of Groningen during the academic year 2023/24:
\begin{quotation}
    ML as a ﬁeld, which perceives itself as a ﬁeld under this name, is relatively young, say, about 40 years (related research was called “pattern recognition” earlier). It is interdisciplinary and has historical and methodological connections to neuroscience, cognitive science, linguistics, mathematical statistics, AI, signal processing and control; it uses mathematical methods from statistics (of course), information theory, signal processing and control, dynamical systems theory, mathematical logic and numerical mathematics; and it has a very wide span of applications. This diversity in traditions, methods and applications makes it diﬃcult to study ``Machine Learning". Any given textbook, even if it is very thick, will reﬂect the author's individual view and knowledge of the ﬁeld and will be partially blind to other perspectives. This is quite diﬀerent from other areas in computer science, say for example formal languages/theory of computation/computational complexity where a widely shared repertoire of standard themes and methods cleanly deﬁne the ﬁeld.
\end{quotation}

My interests in machine learning lie in its rapid development since the era of deep learning began in 2012 and the mysteries of why its methods are so effective. At their core, many methods in machine learning are statistically-principled, corresponding to maximum likelihood estimation. The fact that something as simple as maximum likelihood estimation can be used to fit very complex distributions to at least a small extent isn't too surprising. What fascinates me is the notable extent to which it does so in practice. It performs so well that we are able to produce models which take natural language as input and output entirely realistic corresponding images/videos. What right does maximum likelihood estimation have to facilitate such effective fitting of complex distributions in practice? Why are deep learning architectures able to encode such rich function classes? We only have partial answers to the many natural questions like these and so our understanding is far from complete. What an interesting time to be alive. :)

% supervised learning
\section{Supervised Learning}
Supervised learning methods are those which fit functions $f_{\theta}:\Omega_{\X}\to\Omega_{\mathbf{Y}}$ from model variables $\X=(X_1,\dots,X_q)$ to output variables $\mathbf{Y}=(Y_1,\dots,Y_r)$ (often $r=1$ in which case we write $\mathbf{Y}=Y$) using concrete examples $\{(\x_i,\mathbf{y}_i)\}_{i=1}^n$ of what the to-be-learned function might yield as output $\mathbf{y}=f_{\theta}(\x)$ for a given input $\x$. These model and output variables can be continuous or discrete in nature, or a mix.

An example of a task which supervised learning is appropriate for includes regression. Regression tasks can be thought of as those in which the output variables $\mathbf{Y}$ are continuous. Other than regression, the other vanilla supervised learning task is classification, in which $\mathbf{Y}$ are discrete. A well known example of binary classification is whether or not a given passenger survived the titanic wreck given their ticket class, sex, age, port of embarkation and a ton more. The bread and butter example of multi-class classification is handwritten digit recognition. It's amazing that something which was difficult as recently as 2005 is trivial now for various levels of machine learing understanding.

Other than the types of tasks which we consider in this section, it's worth mentioning how we deal with our data in general. In training a model on some dataset, we need some idea of how well the model generalises to new data. This is done by splitting the given dataset $D$ into a training set $D_{\text{train}}$ and a testing set $D_{\text{test}}$. How exactly this split is done depends on the task at hand but an example is 50/50 uniformly randomly splitting the original dataset $D$. It can be important that this is done uniformly randomly. For example, if one is tackling a binary classification task and $D$ consists of $1000$ samples in which $500$ pertain to one of the class labels and the remaining $500$ pertain to the other then it's important to not split in a way that causes egregiously imbalanced representations of these classes. Training a model on $500$ samples which pertain to the first class label then testing it on $500$ samples peratin to the other wouldn't make much sense. Similar idea for regression tasks but with ensuring a reasonable balance in representation of values belonging to given intervals corresponding to the output space. As long as their is sufficient representation of $\Omega_{\mathbf{Y}}$ then splitting $D$ unifromly randomly is no issue. With a reasonable split we train the model on $D_{\text{train}}$. It is all the model knows — $D_{\text{train}}$ is the model's universe. After training, we measure the model's ability to generalise to new samples by seeing how well it performs on $D_{\text{test}}$. Essentially comparing its output given some sample to the ideal/true output value of said sample.

% linear regression
\subsection{Linear Regression}

Perhaps the simplest example of supervised learning is linear regression. Suppose we are given a dataset $D=\{(\x_i,y_i)\}_{i=1}^{2n}$ where
$$
\x_i=(x_{i,1},\dots,x_{i, q})\in \Omega_{X_1}\times\dots\times \Omega_{X_q}=:\Omega_{\X}\subseteq\R^q
$$
are the feature values of the $i^{\text{th}}$ sample and $y_i\in \Omega_{Y}\subseteq\R$ is the corresponding output. The reason I chose $|D|=2n$ is that it results in being able to 50/50 split $D$ into $D_{\text{train}}$ and $D_{\text{test}}$ with $|D_{\text{train}}|=|D_{\text{test}}|=n$. A linear regression models fits a linear function (in the parameters)
\begin{align*}
    f_{\theta}:\Omega_{\X}&\rightarrow \Omega_Y\\
    \x&\mapsto
    \theta^{\top}
    \begin{bmatrix}
        1\\
        \x
    \end{bmatrix}
    =
    \theta_0+\theta_1x_1+\dots+\theta_qx_q    
\end{align*}
where $\theta=(\theta_0,\theta_1\dots,\theta_q)\in\R^{q+1}$ are the model parameters, i.e. the values we can tweak to our heart's desire until the corresponding model is sufficiently well according to some metric. Some people refer to the paramter $\theta_0$, which dictates the elevation of the hyperplane corresponding to $f_{\theta}(\x)=0$, as the bias of the model which I find very confusing as there are a bunch of other intended meanings of the term `bias' in statistics and machine learning. I prefer to refer to it as the elevation. Anyway, once such a linear function has been fit, given feature values $\x\in\Omega_{\X}$ our model predict the corresponding output as $y=f_{\theta}(\x)$.

\begin{tcolorbox}[title={\centering\textbf{What does the `linear' in linear regression acrually refer to?}}, colback=myLightBlue, colbacktitle=myDarkBlue, colframe=myDarkBlue, coltitle=white]
    Casella Berger, as well as other pieces of literature, define linear regression in a way that ensures linearity in parameters. By this definition, linear regression, as a term, would encompass polynomial regression models and other regression models with non-linear basis functions. This confuses me as `linear regression' is most often intended to mean models that fit a hyperplane to data.
\end{tcolorbox}

An intuitive approach to finding the `optimal' parameters for a linear regression model, which we denote by $\theta^{*}$, is to split $\D$ into training and testing datasets $D_{\text{train}}$ and $D_{\text{test}}$ (each consisting of $n$ sample in our case) and minimising some pre-determined loss function of said parameters over $D_{\text{train}}$. Essentially, minimising something like
$$
\mathcal{L}(\theta)=\sum_{i=1}^nd(f_{\theta}(\x_i), y_i)
$$
where $f_{\theta}(\x_i)$ is the model's prediction for feature values $\x_i$ and $d:\Omega_Y\times\Omega_Y\rightarrow\R_{\geq0}$ is some goodness-of-prediction metric. Said loss function gives one an idea of how well $\theta$ fits the true underlying relationship which we wish to model. A common choice for the loss function is the mean square error
$$
\text{MSE}(\theta)=\frac{1}{n}\sum_{i=1}^n(y_i-f_{\theta}(\x_i))^2
$$
which pertains to taking $d(f_{\theta}(\x_i), y_i)=(y_i-f_{\theta}(\x_i))^2$. Note that the factor of $1/n$ is often left out when discussing MSE as minimising the expression in $\theta$ is invariant to the inclusion of the factor. Also, leaving the factor out is notationally convenience. So in our case, we seek to compute
$$
\theta^{*}=\argmin_{\theta\in\R^{q+1}}\l[\sum_{i=1}^n(y_i-f_{\theta}(\x_i))^2\r].
$$

\noindent We know that in the context of linear regression, the optimal paramters $\theta^{*}$ are typically taken to be those which minimise the mean square error over $D_{\text{train}}$ but how do we actually compute $\theta^{*}$? This could be done through numerical methods, which is often the case in machine learning, e.g. using gradient descent in computing the optimal parameters of a logistic regression model, but linear regression has a closed form solution. This is pretty cool since it's not so common for such closed form solutions to exist. The informal method which I use to remember the closed form solution for the optimal paramters of a linear regression model is
$$
X\theta^{*}=y \implies X^{\top}X\theta^{*}=X^{\top}y \implies \theta^{*}=(X^{\top}X)^{-1}X^{\top}y.
$$
In practice, if this matrix $X^{\top}X$ is singular then just add some small values to its diagonal. That is, instead compute
$$
\theta^{*}=(X^{\top}X+\delta I)^{-1}X^{\top}y
$$
for some small $\delta\in\R$. That said, what's given above is not rigorous, so let's derive it. Note that
\begin{align*}
    \text{MSE}(\theta)
    &=
    \sum_{i=1}^n(y_i-f_{\theta}(\x_i))^2\\
    &=
    \begin{bmatrix}
        y_1-f_{\theta}(\x_1) & \cdots & y_n-f_{\theta}(\x_n)
    \end{bmatrix}
    \begin{bmatrix}
        y_1-f_{\theta}(\x_1)\\
        \vdots\\
        y_n-f_{\theta}(\x_n)
    \end{bmatrix}\\
    &=(y-X\theta)^{\top}(y-X\theta)\\
    &=||y-X\theta||^2
\end{align*}
where
$$
y
=
\begin{bmatrix}
    y_1\\
    \vdots\\
    y_n
\end{bmatrix}
\in\R^n,
X
=
\begin{bmatrix}
    1 & x_{1,1} & \cdots & x_{1,q}\\
    \vdots & \vdots & \ddots & \vdots\\
    1 & x_{n,1} & \cdots & x_{n,q}
\end{bmatrix}
\in\R^{n\times (q+1)},
\theta
=
\begin{bmatrix}
    \theta_0\\
    \theta_1\\
    \vdots\\
    \theta_q
\end{bmatrix}
\in\R^{q+1}
$$
and $x_{i,j}$ denotes the $j$th element of the $i^{\text{th}}$ sample. To find the minimiser(s) of $\text{MSE}(\theta)$, i.e. the optimal parameters $\theta^{*}$, we compute its gradient with respect to $\theta$, set it to $0$ and solve for $\theta^{*}$. This of course only works when our function is both differentiable and convex, which is the case here. To see convexity, simply compute the Hessian of $\text{MSE}(\theta)$ and see that it is semi-positive definite. On that note, we have
\begin{align*}
    \nabla\text{MSE}(\theta)
    &=
    \nabla||y-X\theta||^2\\
    &=
    \nabla(y-X\theta)^{\top}(y-X\theta)\\
    &=
    \nabla\l[\theta^{\top}X^{\top}X\theta-\theta^{\top}X^{\top}y-y^{\top}X\theta+y^{\top}y\r]\\
    &=
    \nabla\l[\theta^{\top}X^{\top}X\theta-2y^{\top}X\theta+y^{\top}y\r]\\
    &=
    2X^{\top}X\theta-2X^{\top}y
\end{align*}
and so the optimiser $\theta^{*}$ is given by
$$
\theta^{*}=(X^{\top}X)^{-1}X^{\top}y.
$$

\begin{tcolorbox}[title={\textbf{Funny misuse of linear regression: Momentous sprint at the 2156 Olympics?}}, colback=myLightBlue, colbacktitle=myDarkBlue, colframe=myDarkBlue, coltitle=white]
    Read this Quora answer\footnote{\url{https://qr.ae/pslbEN}} based on a paper\footnote{\url{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3173856/}} published in Nature. The top comment is worth reading too. Related xkcd comic\footnote{\url{https://xkcd.com/1007/}}.
\end{tcolorbox}

\subsubsection{Statistical Motivation}

The idea of minimising the mean square error of the model over $D_{\text{train}}$ has a rigorous statistical motivation, corresponding to maximum likelihood estimation. Assume that the residuals corresponding of the model's output over $D_{\text{train}}$ are independent and identically normally distributed with mean 0. That is, assume
$$
E_i
=
Y_i-f_{\theta}(\X_i)\sim\mathcal{N}(0, \sigma^2)
$$
for $i=1,\dots,n$ are i.i.d. This assumption is reasonable in practice due to the central limit theorem (CLT). We have $Y_i|\X_i\sim\mathcal{N}(f_{\theta}(\x_i), \sigma^2)$ and so
$$
p_{\theta}(y_i|\x_i)=\frac{1}{\sqrt{2\pi}\sigma}\exp\l(-\frac{(y_i-f_{\theta}(\x_i))^2}{2\sigma^2}\r).
$$
The maximum likelihood estimate $\theta_{\text{MLE}}$ for the parameters of our linear regression model is that which maximise our log-likelihood. That is,
\begin{align*}
    \theta_{\text{MLE}}
    &=
    \argmax_{\theta\in\R^{q+1}}\l[\log\l(\prod_{i=1}^n p_{\theta}(y_i|\x_i)\r)\r]\\
    &=
    \argmax_{\theta\in\R^{q+1}}\l[\sum_{i=1}^n\log(p_{\theta}(y_i|\x_i))\r]\\
    &=
    \argmax_{\theta\in\R^{q+1}}\l[n\log\l(\frac{1}{\sqrt{2\pi}\sigma}\r)-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-f_{\theta}(\x_i))^2\r]\\
    &=
    \argmin_{\theta\in\R^{q+1}}\l[\sum_{i=1}^n(y_i-f_{\theta}(\x_i))^2\r].
\end{align*}
As such, the maximum likelihood estimator of the parameters of a linear regression model are precisely those which minimise the mean square error of the model over $D_{\text{train}}$.

\begin{tcolorbox}[title={\centering\textbf{Are residuals really normally distributed?}}, colback=myLightBlue, colbacktitle=myDarkBlue, colframe=myDarkBlue, coltitle=white]
    \footnote{\url{https://en.wikipedia.org/wiki/Central\_limit\_theorem\#Regression}}Central limit theorem to the rescue: ``Regression analysis, and in particular ordinary least squares, specifies that a dependent variable depends according to some function upon one or more independent variables, with an additive error term. Various types of statistical inference on the regression assume that the error term is normally distributed. This assumption can be justified by assuming that the error term is actually the sum of many independent error terms; even if the individual error terms are not normally distributed, by the central limit theorem their sum can be well approximated by a normal distribution."
\end{tcolorbox}

\subsubsection{Goodness of fit: $R^2$}
If we'd like a way to measure the goodness-of-fit of a linear regresison model beyond test MSE, a natural avenue is to assess what portion of the sample variance is explained by the model. As usual, we do this over some sample $D=\{(\x_i,y_i)\}_{i=1}^n\subset\Omega_{\X}\times\Omega_{Y}$. That is, computing
$$
\frac{\text{Var}(f_{\theta}(\X))}{\text{Var}(Y)}
\approx
\frac{\frac{1}{n}\sum_{i=1}^n\l(f_{\theta}(\x_i)-\bar{y}\r)^2}{\frac{1}{n}\sum_{i=1}^n\l(y_i-\bar{y}\r)^2}
=:
R^2
$$
where $\bar{y}=\frac{1}{n}\sum_{i=1}^ny_i$. The reason that this quantity is denoted by $R^2$ is that it can be shown that it is equal to the square of the Pearson/multiple (one predictor/multiple predictors) correlation coefficient between $Y$ and $f_{\theta}(\X)$. So for one predictor, i.e. $q=1$, said Pearson correlation coefficient is given by
$$
R
=
\frac{\sum_{i=1}^n\l(f_{\theta}(\x_i)-\bar{f}_{\theta}\r)(y_i-\bar{y})}{\sqrt{\sum_{i=1}^n\l(f_{\theta}(\x_i)-\bar{f}_{\theta}\r)^2}\sqrt{\sum_{i=1}^n\l(y_i-\bar{y}\r)^2}}.
$$
The derivation of this statement is boring, so I'll leave it out. Worth noting that $R^2$ is typically expressed as
$$
R^2
=
1-\frac{\sum_{i=1}^n\l(y_i-f_{\theta}(\x_i)\r)^2}{\sum_{i=1}^n\l(y_i-\bar{y}\r)^2}
=
1-\frac{s_e^2}{s_Y^2}
$$
where $s_e^2$ and $s_Y^2$ denote the sample variances of the residuals and $Y$ respectively. I prefer the ratio of explained variance to underlying model variance though; is more intuitive to me, especially in terms of how its motivated and doesn't feel like it comes outta nowhere. Note that this notion of the portion of explained variance can be extended to logistic regression but the details are a bit much for this document.

\begin{tcolorbox}[title={\centering\textbf{How can we get an idea of the extent to which our features and output are linearly related without plotting?}}, colback=myLightBlue, colbacktitle=myDarkBlue, colframe=myDarkBlue, coltitle=white]
    To be honest, I thought a streamline test for this existed. As per this Stack Exchange answer\footnote{\url{https://stats.stackexchange.com/a/239142}}, a good method for this is to simply fit a linear and a non-linear model (e.g. a cubic spline smoother model) and see which explains a larger amount of the variance of the output via ANOVA tests.
\end{tcolorbox}

\subsubsection{Why call it `regression'?}
Nowadays, regression models are those which predict a value belonging to some continuous space but from where did the term originate? In 1886, Francis Galton authored ``Regression Towards Mediocrity in Hereditary Stature'' which is where the `regression towards the mean' term comes from. Loosely speaking, Galton noticed that tall fathers tend to have short sons, short fathers tend to have have tall sons and average height fathers tend to have average height sons. Taken from a Stack Exchange comment: ``Galton derived a linear approximation to estimate a son's height from the father's height in that paper. His equation was fitted so an average height father would have an average height son, but a taller than average father would have a son that is taller than average by 2/3 the amount his father is. Same with shorter than average. This could be argued to be a simple linear regression."

Put mathematically, suppose random variables $X$ and $Y$ are related via $Y=\alpha+\beta X+\epsilon$ where $\alpha,\beta\in\R$ are regression coefficients and $\epsilon$ is noise with mean $0$. Then $\E[Y|X]=\alpha+\beta X$, so if $X$ and $Y$ are centred then $E[Y|X]=\rho X$ where $\rho$ is the correlation coefficient between $X$ and $Y$. Since $|\rho|\leq1$ we know that the expected value of $Y$ is closer to the mean than $X$ unless $\rho=1$. So extreme values of $X$ tend to correspond to values of $Y$ that are closer to the mean, i.e. one has regression towards the mean.

To see why this does not apply to all modern `regression' models, consider logistic regression in which one, roughly speaking, predicts probabilities of binary outcomes. In such cases, the dependent variable is binary, so `regressing towards the mean' has no meaningful interpretation.

The `linear' part refers to the output variable being linear in the parameters. This is sometimes confusing in linear regression we typically deal with basis functions that give line-like surfaces (lines, planes, etc.) but we could always have non-linear basis functions. For example, $y=\beta_0+\beta_1 e^x$ is linear in $\beta=(\beta_0,\beta_1)$ but $y=\beta_0+e^{\beta_1}x$ is not. A consequence is that an estimate $\hat{\beta}$ of the model parameters can be written as $\hat{\beta}=\sum_{i=1}^k w_iy_i$ where $w_i$ are the determined weights and $y_i$ are the chosen basis functions.

% logistic regression
\subsection{Logistic Regression}

Logistic regression is to binary classification what linear regression is to regression. That is, both are the first method learned when introduced to regression and binary classification. The name might seem strange at first as regression models predict continuously distributed things and binary classification models predict either 0 or 1. The reason regression appears in its name is that it classifies samples by transforming them to $(0,1)$ in some way and imposing a threshold-based decision rule to output either 0 or 1.

Before detailing precisely how a logistic regression model classifies samples, how might one classify a sample at all? A natural idea is to fit a hyperplane to feature space which separates samples of the two classes. A hyperplane is an $(n-1)$-dimensional plane-like object embedded in $n$-dimensional space. For example, a hyperplane in $\R^2$ is a line and in $\R^3$ it is a plane. As an object, a hyperplane in $(q+1)-$dimensional space is the set of points $(x_1,\dots,x_q)\in\R^q$ which satisfy %% for future Dewi: there is a reason we do not write \x=(x_1,\dots,x_q). Read next paragraph to see why
$$
\theta_0+\theta_1x_1+\dots+\theta_qx_q
=
0
$$
where $\theta=(\theta_0,\dots,\theta_q)\in\R^{q+1}$ are parameters which characterise the hyperplane. For brevity, we denote the function whose set of roots is the hyperplane by
\begin{align*}
    f_{\theta}:\{1\}&\times\R^q\to\R\\
    \x&\mapsto \theta_0+\theta_1x_1+\dots+\theta_qx_q=:\theta^\top \x.
\end{align*}
The purpose of the $1$ in the first index of $\x$ is similar to its purpose in linear regression: it accounts for the elevation term $\theta_0$ and makes notation a lot cleaner. After learning the parameters of a hyperplane, we may classify a sample $\x$ according to which side of the hyperplane $f_{\theta}(\x)=0$ it lies. For example, samples `below' the hyperplane, i.e. $f_{\theta}(\x)\leq0$, could be classified as $0$ and samples `above', i.e. $f_{\theta}(\x)>0$, could be classified as $1$. That is, classify samples according to $C_{\theta}(\x)=\mathbbm{1}(f_{\theta}(\x)>0)$.

To obtain a notion of the probability that the class label of a sample is 1, consider how far it deviates from the plane. To make this idea concrete, apply a logistic (sigmoid) transformation to the output $f_{\theta}(\x)$ as in
$$
h_{\theta}(\x)
=
\sigma(f_{\theta}(\x))
=
\frac{1}{1+\exp(-f_{\theta}(\x))}\in(0,1).
$$
From here, the class label of a sample $\x$ is $1$, according to the model, if $h_{\theta}(\x)>\delta$ for some threshold $\delta\in(0,1)$. Note that $\delta=0.5$ corresponds precisely to the `above/below the hyperplane' approach above. The advantage of this broad decision rule is that we can select $\delta$ in a way that improves precision at the expense of recall and vice versa. For example, for higher precision and lower recall, $\delta>0.5$ and classify samples according to $C_{\theta}^{\delta}(\x)=\mathbbm{1}(h_{\theta}(\x)>\delta)$.

See that for the model to match the underlying probability that the class label of a given sample is 1, we require that
\begin{align*}
    &h_{\theta}(\x)=p(Y=1|\X=\x)\\
    \iff & \frac{1}{1+\exp(-f_{\theta}(\x))}=p(Y=1|\X=\x)\\
    \iff & f_{\theta}(\x) = \log\l(\frac{p(Y=1|\X=\x)}{1-p(Y=1|\X=\x)}\r)\\
    \iff & f_{\theta}(\x) = \log\l(\frac{p(Y=1|\X=\x)}{p(Y=0|\X=\x)}\r)
\end{align*}
and so logistic regression models can be seen as fitting a linear regression model to the log-odds of each sample being of class $1$.

While their motivation is intuitive, how might we learn suitable parameters of logistic regression models from data? Fortunately, as with linear regression, a statistically-grounded method exists.

\subsubsection{Statistical Motivation}
Like in linear regression, for a statistical motivation we'll make some assumptions regarding the class labels to derive a way of finding the optimal parameters $\theta^{*}$. Suppose that $Y|(\X=\x)\sim\text{Bernoulli}(h_{\theta}(\x))$. In this case
$$
p(y|\x;\theta)=(h_{\theta}(\x))^{y}(1-h_{\theta}(\x))^{1-y}.
$$
We construct the log-likelihood over $D_{\text{train}}=\{(\x_i,y_i)\}_{i=1}^n$, assuming its samples are i.i.d., as
\begin{align*}
    l(\theta)
    &=
    \sum_{i=1}^ny\log(h_{\theta}(\x_i))+(1-y_i)\log(1-h_{\theta}(\x_i))\\
    &=
    \sum_{i=1}^n\l[y_i\log\l(\frac{1}{1+e^{-\theta^{\top}\x_i}}\r)+(1-y_i)\log\l(1-\frac{1}{1+e^{-\theta^{\top}\x_i}}\r)\r]\\
    &=
    \sum_{i=1}^n\l[y_i\log\l(\frac{1}{1+e^{-\theta^{\top}\x_i}}\r)+(1-y_i)\l(-\theta^{\top}\x_i+\log\l(\frac{1}{1+e^{-\theta^{\top}\x_i}}\r)\r)\r]\\
    &=
    \sum_{i=1}^n\l[-\log\l(1+e^{-\theta^{\top}\x_i}\r)-(1-y_i)\theta^{\top}\x_i\r]
\end{align*}
which we maximise numerically in $\theta$, e.g. via gradient descent, to obtain the optimal paramateres
$$
\theta^{*}
=
\argmax_{\theta\in\R^{q+1}}\l[\sum_{i=1}^n\l[-\log\l(1+e^{-\theta^{\top}\x_i}\r)-(1-y_i)\theta^{\top}\x_i\r]\r].
$$
Note that, in practice, it's unlikely that any samples will lie on the hyperplane $f_{\theta^*}(\x)=0$ itself.

% support vector machines
\subsection{Support Vector Machines (SVMs)}
Logistic regression and SVMs both fit a hyperplane to feature space. The distinction is the metric of goodness of said hyperplanes. In logistic regression, the metric of goodness is how much the parameters of said hyperplane maximise the relevant log-likelihood. SVMs, however, look to maximise the distance between the hyperplane and the nearest samples. So in some sense, logistic regression is a probabilistic approach while SVMs take a raw constraint-based approach.

The authors' insight came from structural risk minimization in which instead of focusing on minimising training error (as neural networks and decision trees were doing at the time), one focuses on minimising an upper bound on the generalisation error. The inclusion of `support vector' becomes clear from their construction but `machine' stood out to me as a bit odd. It turns out that at time of their development, around 1960, it was common to use `machine' when referring to algorithms that learned from data, i.e. algorithms belonging to statistical learning theory. This reflects Arthur Samuel's coining of machine learning as a term in 1959 while working at IBM.

\begin{tcolorbox}[title={\centering\textbf{Inconsistent terminology surrounding SVMs}}, colback=myLightBlue, colbacktitle=myDarkBlue, colframe=myDarkBlue, coltitle=white]
    Some pieces of literature describe the decision boundary learned by an SVM as strictly linear, others allow for a non-linear decision boundary. It'd be nice if authors stuck to the former and explicitly stated `non-linear SVM' when describing the latter. In this section, the term refers to those which correspond to linear decision boundaries. I'll state explicitly when considering non-linear SVMs.
\end{tcolorbox}

\subsubsection{Hard-margin SVMs}
Since we aim to fit a hyperplane to feature space, i.e. $\R^{q+1}$, we use the same notation for the function $f_{\theta}(\x)$ corresponding to the linear decision boundary (i.e. hyperplane) used to motivate logistic regression. To make notation a bit easier, let $\theta_+=(\theta_1,\dots,\theta_q)$ so that $\theta$ is the ordered concatentation of $\theta_0$ and $\theta_+$. Further, let $\x_i$ denote the $i^{\text{th}}$ sample's features and $y_i\in\{-1,1\}$ its class such that $y_i$ is 1 if $\theta_+^\top \x_i+\theta_0>0$ and $-1$ otherwise.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\columnwidth]{./figures/supervised_learning/SVM_hard_margin.pdf}
    \caption{A hard-margin SVM in two dimensions. Samples lying on the margin are referred to as support vectors.}
    \label{fig:SVM_hard_margin}
\end{figure}

Let $\mathbf{p}_i$ denote the projection of $\x_i$ onto the hyperplane and let $d_i$ denote the distance between $\mathbf{p}_i$ and $\x_i$. Noting that the normal to the hyperplane is $\theta_+$, we have $\mathbf{p}_i=\x_i-\frac{\theta_+}{||\theta_+||}d_i$ and so
$$
\theta_+^\top \l(\x_i-\frac{\theta_+}{||\theta_+||}d_i\r)+\theta_0
=
0
$$
which, after some rearranging, yields
$$
d_i
=
\frac{\theta_+^\top \x_i+\theta_0}{||\theta_+||}.
$$
Similarly, taking a point $\x_i$ whose class is $-1$ yields $d_i=-\frac{\theta_+^\top \x_i+\theta_0}{||\theta_+||}$ and so a neater way of writing this distance for an arbitrary sample $\x_i$ is $d_i=\frac{y_i(\theta_+^\top \x_i+\theta_0)}{||\theta_+||}$. Hard-margin SVMs are only applicable to linearly sparable data and their construction, from data, effectively boils down to finding which parameters maximise $\min_{i=1,\dots,n}d_i$. That is, we seek to compute
\begin{align*}
    \theta^{*}
    &=
    \argmax_{(\theta_0,\theta_+)}\l[\min_{i=1,\dots,n}d_i\r]\\
    &=
    \argmax_{(\theta_0,\theta_+)}\l[\min_{i=1,\dots,n}\frac{y_i(\theta_+^\top \x_i+\theta_0)}{||\theta_+||}\r]
\end{align*}
which we'd like to translate into a convex optimisation problem. First, notice that computing $\theta^{*}$ is equivalent to solving
$$
\max_{(\theta_0,\theta_+)}\frac{r}{||\theta_+||}\text{ s.t. } y_i\l(\theta_+^\top \x_i+\theta_0\r)\geq r\text{ } (i=1,\dots,n).
$$
in which $r$ may be scaled arbitrarily by positives, so it is equivalent to
$$
\max_{(\theta_0,\theta_+)}\frac{1}{||\theta_+||}\text{ s.t. } y_i\l(\theta_+^\top \x_i+\theta_0\r)\geq 1\text{ } (i=1,\dots,n).
$$
This problem is still non-convex so we make a convenient switcheroo in realising that it is equivalent to solving
$$
\min_{(\theta_0,\theta_+)}||\theta_+||^2\text{ s.t. } y_i\l(\theta_+^\top \x_i+\theta_0\r)\geq 1\text{ } (i=1,\dots,n)
$$
which is solved in the usual convex problem solving ways.

\subsubsection{Soft-margin SVMs}

Hard-margin SVMs are rarely applicable. In practice, samples are not entirely linearly separable and so allowing for some misclassification is pragmatic. Going from hard-margin to soft-margin is pretty straightforward, just include some slack variables $\xi=(\xi_1,\dots,\xi_n)$ that ultimately allow the model to violate the constraints while penalising said violations. More precisely, it involves solving
$$
\min_{(\theta_0,\theta_+,\xi)}\l[||\theta_+||^2+\lambda\sum_{i=1}^n\xi_i\r]\text{s.t. } y_i\l(\theta_+^\top \x_i+\theta_0\r)\geq 1-\xi_i,\text{ }\xi_i\geq0
$$
for $i=1,\dots,n$ where $\lambda\geq0$ is a regularisation parameter that influences the tradeoff of margin size and misclassification rate. Larger $\lambda$ corresponds to prioritising a larger margin while smaller $\lambda$ corresponds to prioritising the minimisation of misclassification.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\columnwidth]{./figures/supervised_learning/SVM_soft_margin.pdf}
    \caption{A soft-margin SVM in two dimensions. Samples are labelled according to their true class.}
    \label{fig:SVM_soft_margin}
\end{figure}

As illustrated in \autoref{fig:SVM_soft_margin}, it allows for samples to be closer to the decision boundary than the margin as well as outright misclassifications. Again, it is typically solved in the usual convex problem solving ways. Going a step further, it turns out that this can be reduced to computing
$$
\argmin_{(\theta_0,\theta_+)}\l[||\theta_+||^2+\lambda\sum_{i=1}^n\max\l(0,1-y_i\l(\theta_+^\top \x_i+\theta_0\r)\r)\r]
$$
which can be done using gradient descent.

\subsubsection{Non-linear SVMs}
Motivating non-linear SVMs is straightforward: we'd sometimes like to separate samples by a non-linear decision boundary. With this in mind, a super intuitive approach is to find a transformation $\phi$ which maps samples to a space in which they are linearly separable. In said space, employ a linear SVM.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\columnwidth]{./figures/supervised_learning/SVM_non_linear.pdf}
    \caption{Non-linearly separable samples being transformed in such a way that they become linearly separable. Almost like punching the curve into a line. I'm unsure why the empty set symbol above the arrow is present. \TODO{: Make own figure.}}
    \label{fig:SVM_non_linear}
\end{figure}

With this idea in mind, we seek to solve
$$
\min_{(\theta_0,\theta_+,\xi)}\l[\frac{1}{2}||\theta_+||^2+\lambda\sum_{i=1}^n\xi_i\r]\text{s.t. } y_i\l(\phi(\theta_+)^\top\x_i+\theta_0\r)\geq 1-\xi_i,\text{ }\xi_i\geq0
$$
which is difficult if the dimension of the image of $\phi$ is large. To make things easier, the dual of problem is optimised instead. My understanding of primal problems and their dual problems isn't great, so I'll just give the dual outright without deriving it:
$$
\max_{(\alpha_1,\dots,\alpha_n)\in[0,\lambda]^n}\l[\sum_{i=1}^n\alpha_i-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jy_iy_jK(\x_i,\x_j)\r]\text{s.t. }\sum_{i=1}^n\alpha_iy_i=0
$$
where $K(\x,\z)=\langle\phi(\x),\phi(\z)\rangle$ is a pre-chosen kernel. See how it never requires an explicit computation involving $\phi$ as long as a closed form of $K(\x,\z)$ not involving $\phi$ is available. This is referred to as the kernel trick and reduces having to solve an optimisation problem in a space whose dimension is the number of features to a space whose dimension is the number of training samples.

At inference time, given some $\x$, we seek to compute
\begin{align*}
    y
    &=
    \text{sign}\l(\theta_+^\top\phi(\x)+\theta_0\r)\\
    &=
    \text{sign}\l(\sum_{i=1}^n\alpha_iy_iK(\x_i,\x)+b_k\r)
\end{align*}
where $b_k=y_k-\sum_{i=1}^n\alpha_iy_iK(\x_i,\x_k)$ for some $(\x_k,y_k)$ that satisfies $\alpha_k\in(0,\lambda)$. Note that the second line in the equation above is derived using to argument made in deriving the dual.

The two simplest kernels are polynomial kernels and the RBF kernel. \TODO{: Complete a paragraph on common kernels.}

% decision trees and random forests
\subsection{Decision Trees and Random Forests}
I find decision trees and random forests so boring that I'm not going to write about them and will instead include a plot which illustrates random forests well. This section mostly exists so that the list of subsections in this section form a well-rounded list of supervised learning methods.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{./figures/supervised_learning/random_forest.pdf}
    \caption{A random forest.}
    \label{fig:random_forest}
\end{figure}

Despite my disinterest in them, learning about decision trees encourages you to understand entropy which is good. There are other topics which encourage the same thing though, e.g. Variational Autoencoders, through the use of KL-divergences. Also, random forests are a nice introduction to ensemble methods which nicely demonstrate how to prevent overfitting by reducing variance — directly illustrating the importance of the bias-variance tradeoff! Oh, and bootstrapping. One day I'll write this subsection properly.

\begin{tcolorbox}[title={\centering\textbf{LR $>$ SVM/RF when?}}, colback=myLightBlue, colbacktitle=myDarkBlue, colframe=myDarkBlue, coltitle=white]
    This is a pretty natural question once shown more sophisticated methods which deal with linearly separable and non-linearly separable data. The largest benefit is that it allows for statistical tests on parameters, e.g. statistical significance tests. It also helps that its implementation and interpretation are straightforward.
\end{tcolorbox}

% the bias-variance tradeoff
\subsection{\REV{: }The Bias-Variance Tradeoff}
The bias-variance tradeoff is a statement pertaining to how well one can perform supervised learning from data. Crudely put, if the model family used to fit is too simple to represent the underlying distribution to which the samples belong then the learning method is highly-biased, i.e. the outcome of the learning method is almost invariant with respect to the training data. Think of fitting a line to a high-degree polynomial. On the other end, if the model family is too complex (e.g. overparameterised) then the excess parameters cause the model to fit the noise present in the training data as opposed to the underlying structure. In this case, the learning method is highly-variant with respect to the training data. Think of polynomial regression models trained on noisy samples.

\subsubsection{\TODO{: }Underfitting and Overfitting}
I was unable to find rigorous definitions in books like Casella Berger but The Cambridge Dictionary of Statistics offers them. That said, in practice, over and underfitting are often spoken about in terms of the training and validation losses observed during training for a given problem.

\subsubsection{Derivation with MSE loss}
Recall that sample targets pertaining to datasets from which supervised models are learned are effected by noise, i.e. $Y|(\X=\x)=f(\x)+\epsilon(\x)$ where $\epsilon(\x)\sim\mathcal{N}(0,\sigma^2(\x))$ and so
$$
Y|(\X=\x)
\sim
\mathcal{N}\l(f(\x), \sigma^2(\x)\r).
$$
We seek to learn an estimate $\hat{f}_{D}$ of $f(\x)=\E_Y\l[Y|\X=\x\r]$ from the dataset $D\subset(\Omega_{\X}\times\Omega_Y)^n$. In other words, the model/estimate $\hat{f}_{D}$ is determined by the realisation $D$ of $\D$ where $\D$ is distributed according to $p(\x,y)^{\otimes n}$ for some fixed $n$. As is common in supervised learning contexts, we make use of the expected risk of an estimate $\hat{f}_D$ which is given by
$$
R\l(\hat{f}_D\r)
=
\E_{(\X,Y)}\l[\l(Y-\hat{f}_D(\X)\r)^2\r]
$$
in which mean square error (MSE) is used as loss. The quantity pertaining to the quality of a learning procedure used to determine $\hat{f}_{D}$ from $D\in\Omega_{\D}$ is given by
\begin{align*}
    &\hspace{14.52pt}
    \E_{\D}\l[R\l(\hat{f}_{\D}\r)\r]\\
    &=
    \E_{\D}\l[\E_{(\X,Y)}\l[\l(Y-\hat{f}_{\D}(\X)\r)^2\r]\r]\nonumber\\
    &=
    \E_{\D}\l[\E_{(\X,Y)}\l[\l(Y-f(\X)+f(\X)-\hat{f}_{\D}(\X)\r)^2\r]\r]\nonumber\\
    &=
    \textcolor{myYellow}{\E_{\D}\l[\E_{(\X,Y)}\l[\l(Y-f(X)\r)^2\r]\r]}+
    \textcolor{myGreen}{\E_{\D}\l[\E_\X\l[\l(f(\X)-\hat{f}_{\D}(\X)\r)^2\r]\r]}\\
    &+\textcolor{myLightRed}{\E_{\D}\l[\E_{(\X,Y)}\l[2(Y-f(\X))\l(f(\X)-\hat{f}_{\D}(\X)\r)\r]\r]}.
\end{align*}
Let's address this term-by-term beginning with the term in red. See that
\begin{align*}
    &\hspace{14.52pt}
    \textcolor{myLightRed}{\E_{\D}\l[\E_{(\X,Y)}\l[2(Y-f(\X))\l(f(\X)-\hat{f}_{\D}(\X)\r)\r]\r]}\\
    &=
    2\E_{\D}\l[\E_\X\l[\E_{Y}\l[(Y-f(\X))\l(f(\X)-\hat{f}_{\D}(\X)\r)\Big|\X\r]\r]\r]\\
    &=
    2\E_{\D}\l[\E_\X\l[\l(f(\X)-\hat{f}_{\D}(\X)\r)\E_{Y}\l[Y-f(\X)\vert\X\r]\r]\r]\\
    &=
    2\E_{\D}\l[\E_\X\l[\l(f(\X)-\hat{f}_{\D}(\X)\r)\cdot0\r]\r]\\
    &=
    0
\end{align*}
in which the tower property of conditional expectations
\begin{align*}
    \E_{(\X,Y)}[g(\X,Y)]
    &=
    \iint g(\x,y)f_{(\X,Y)}(\x,y)dxdy\\
    &=
    \int \l(\int g(\x,y)f_{Y|\X}(y|\x)dy\r)f_\X(\x)dx\\
    &=
    \E_\X\l[\E_{Y}\l[g(\X,Y)|\X\r]\r]
\end{align*}
is applied. As such, the quantity of interest reduces to only two terms as in
$$
\E_{\D}\l[R\l(\hat{f}_{\D}\r)\r]
=
\textcolor{myYellow}{\E_{\D}\l[\E_{(\X,Y)}\l[\l(Y-f(\X)\r)^2\r]\r]}+\textcolor{myGreen}{\E_{\D}\l[\E_\X\l[\l(f(\X)-\hat{f}_{\D}(\X)\r)^2\r]\r]}
$$
of which we will now address the first in orange. Noting that
$$
\E_{Y}\l[Y-f(\x)|X=x\r]
=
0
$$
for all $x\in\Omega_X$ and that what is inside the expectation over $\D$ is independent of $\D$, we see that the first term reduces to
\begin{align*}
    \textcolor{myYellow}{\E_{\D}\l[\E_{(\X,Y)}\l[(Y-f(\X))^2\r]\r]}
    &=
    \E_{(\X,Y)}\l[(Y-f(\X))^2\r]\\
    &=
    \E_\X\l[\E_{Y}\l[(Y-f(\X))^2|\X\r]\r]\\
    &=
    \E_\X\l[\E_{Y}\l[\l(Y-\E_Y[Y|\X]\r)^2|\X\r]\r]\\
    &=
    \E_\X\l[\text{Var}_Y\l(Y|\X\r)\r]\\
    &=
    \E_\X[\sigma^2(\X)]
\end{align*}
This is simply the expected noise, e.g. due to imperfect calibration in the instruments used to obtain samples, and is typically taken to be constant. Usually, as a constant, it is simply denoted by $\sigma^2$.

For the term in green, let $\bar{f}(\x)=\E_{\D}\l[\hat{f}_{\D}(\x)\r]$ for all $\x\in\Omega_\X$ and see that
\begin{align*}
    &\hspace{14.52pt}
    \textcolor{myGreen}{\E_{\D}\l[\E_\X\l[\l(f(\X)-\hat{f}_{\D}(\X)\r)^2\r]\r]}\\
    &=
    \E_{\D}\l[\E_\X\l[\l(f(\X)-\bar{f}(\X)+\bar{f}(\X)-\hat{f}_{\D}(\X)\r)^2\r]\r]\\
    &=
    \E_\X\l[\l(f(\X)-\bar{f}(\X)\r)^2\r]+\E_{\D}\l[\E_\X\l[\l(\bar{f}(\X)-\hat{f}_{\D}(\X)\r)^2\r]\r]\\
    &+
    2\E_{\D}\l[\E_\X\l[\l(f(\X)-\bar{f}(\X)\r)\l(\bar{f}(\X)-\hat{f}_{\D}(\X)\r)\r]\r].
\end{align*}
See that the final term vanishes as $\E_{\D}\l[\bar{f}(\x)-\hat{f}_{\D}(\x)\r]=0$ for all $\x\in\Omega_{\X}$ and so
\begin{align*}
    &\hspace{14.52pt}
    \textcolor{myGreen}{\E_{\D}\l[\E_\X\l[\l(f(X)-\hat{f}_{\D}(\X)\r)^2\r]\r]}\\
    &=
    \E_\X\l[\l(f(\X)-\bar{f}(\X)\r)^2\r]+\E_{\D}\l[\E_\X\l[\l(\bar{f}(\X)-\hat{f}_{\D}(\X)\r)^2\r]\r]\\
    &=
    \E_\X\l[\l(f(\X)-\bar{f}(\X)\r)^2+\E_{\D}\l[\l(\bar{f}(\X)-\hat{f}_{\D}(\X)\r)^2\r]\r]\\
    &=
    \E_\X\l[\l(f(\X)-\E_{\D}\l[\hat{f}_{\D}(X)\r]\r)^2+\E_{\D}\l[\l(\hat{f}_{\D}(\X)-\E_{\D}\l[\hat{f}_{\D}(\X)\r]\r)^2\r]\r].
\end{align*}
Put together, we obtain
\begin{align*}
    &\hspace{14.52pt}
    \E_{\D}\l[R\l(\hat{f}_{\D}\r)\r]\\
    &=
    \E_\X\l[\underbrace{\l(f(\X)-\E_{\D}\l[\hat{f}_{\D}(\X)\r]\r)^2}_{\text{square of bias of estimator}}+\underbrace{\E_{\D}\l[\l(\hat{f}_{\D}(\X)-\E_{\D}\l[\hat{f}_{\D}(\X)\r]\r)^2\r]}_{\text{variance of estimator}}\r]+\underbrace{\sigma^2}_{\text{noise}}
\end{align*}
where $\sigma^2=\E_\X\l[\sigma^2(\X)\r]$ (ignore the poor choice of notation). We can crudely summarise it as the sum of the weighted average, according to $p(x)$, of the square of the bias of the learning method, the variance of the estimate according to the dataset from which it is determined and irreducible noise. The irreducible noise is invariant to the learning method and so it's usually ignored in discussions surrounding the bias-variance tradeoff.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{./figures/supervised_learning/bias_variance.png}
    \caption{The graph of learning method error against model complexity in terms of bias and variance. \TODO{: Make own figure.}}
    \label{fig:bias_variance}
\end{figure}

In brief, if the learned estimate varies a lot as we vary the dataset from which it is determined then the learning method has high variance. This is often due to the estimates in question overfitting the dataset on which they were learned which itself is often due to overparameterisation. If the number of parameters of a model is far higher than is needed then one can easily fit the individual samples from the dataset used during learning. Think of fitting a high degree polynomial to a line. The dataset will consist of points which roughly form a line but the high degree polynomial, if not regularised, can easily fit the samples present in the dataset used to learn (and in doing so fits the noise) resulting in an overfit model.

The flip side of this is an underparameterised model. Since such a model has little hope of fitting the dataset from which it is learned, you'd expect it to rarely overfit (it can't) and to instead be almost invariant to the dataset. That is, no matter which dataset you gave it to learn from, you get a similar model each time. In other words, the learning method (the method of going from dataset to model) is biased. Think of fitting a line to high degree polynomial — no matter what dataset you use, the best fit line will likely not change too much (which is usually a good thing) but the fit itself will never be anything but linear, i.e. always a bad fit. 

With this in mind, when considering which learning method (i.e. model family) to consider when tackling a problem, thought should be given to how complex (i.e. parameter number) the model family should be. Too simple (i.e. too few parameters) and you underfit (high bias, low variance). Too complex (i.e. too many parameters) and you overfit (low bias, high variance). This is nicely illustrated in \autoref{fig:bias_variance} in which we see that there exists a sweet spot in model complexity. A spot in which said model complexity yields some square of bias and some variance but the minimum some of said quantities, i.e. the lowest learning method error.

\subsubsection{\REV{: }Beyond MSE}
Natural question: MSE is a common choice of loss when dealing with regression but not classification, in which cross-entropies are used for loss, so why does every derivation of the bias-variance tradeoff only use MSE? The quick answer is that it's the only choice which yields a relatively simple derivation of this nice decomposition of learning method error into two orthogonal characteristics of the learning method: bias and variance. It turns out that if you instead consider specifically probabilistic classification methods then using expected cross-entropy as loss yields a decomposition into bias and variance but with a different form.

\subsubsection{\REV{: }Double descent}
Double descent refers to a behaviour in which overparameterised models ignore fly in the face of the bias-variance tradeoff. As the number of parameters (i.e. the model complexity) increases beyond a certain point the test error begins to drop. What makes this a phenomenon is that it seems that this threshold is the number of samples used to determine the model. This is illustrated nicely in \autoref{fig:double_descent}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{./figures/supervised_learning/double_descent.png}
    \caption{How double descent typically looks. \TODO{: Make own figure.}}
    \label{fig:double_descent}
\end{figure}

A consequence of this is that very deep neural networks learned using gradient descent do exceptionally well. It flies in the face of conventional wisdom. I'm unaware of anyone that this doesn't surprise. It isn't well-understood. It's related to grokking.

\begin{tcolorbox}[title={\centering\textbf{What makes MSE so special in general?}}, colback=myLightBlue, colbacktitle=myDarkBlue, colframe=myDarkBlue, coltitle=white]
    Differentiability is a common but unsatisfying answer to this question. Exponents of $2.05, 3$ or $\pi$ would yield a differentiable loss function but we never see those, so what's up? A more satisfying answer is given by this answer to a Stack Exchange\footnote{\url{https://math.stackexchange.com/a/63245}}:`'
    \begin{quote}
        The variance is the mean squared deviation from the average. The variance of the sum of 10,000 random variables is the sum of their variances. That doesn't work for other powers of the absolute value of the deviation. That means if you roll a die 6,000 times, so that the expected number of 1s you get is 1,000, then you also know that the variance of 1s is $6000\cdot\frac{1}{6}\cdot\frac{5}{6}$ so if you want the probability that the numbner of 1s is between 990 and 1,020, you can approximate the distribution by the normal distribution with the same mean and variance. You couldn't do that if you didn't know the variance, and you couldn't know the variances without additivity of variances, and if the exponent were anything besides 2, then you don't have that. (Oddly, you do have additivity with cubes of the deviations, but not cubes of the absolute values of the deviations).
    \end{quote}
\end{tcolorbox}

% parameter exploration and its optimisation
\section{Parameter Exploration and its Optimisation}
In fitting a parametric model $f_{\theta}:\Omega_{\X}\to\Omega_{Y}$, the loss induced by a choice of model parameters $\theta$ for a sample $(\x,y)$ is given by $\mathcal{L}\l(y,f_{\theta}(\x)\r)$. In line with this, let the empirical risk $R_D$ over a dataset $D=\{(\x_i,y_i)\}_{i=1}^n$ be
\begin{align*}
    R_D:\Theta&\to\R_{\geq0}\\
    \theta&\mapsto\frac{1}{n}\sum_{i=1}^n\mathcal{L}(y_i,f_{\theta}(\x_i))
\end{align*}
In fitting our paremtric model $f_{\theta}$, we seek to compute
$$
\theta^{*}
=
\argmin_{\theta\in\Theta}R_D(\theta),
$$
i.e. to minimise the empirical risk over the dataset in the parameters. As such, methods of function minimisation are of interest, of which many exist.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\columnwidth]{./figures/gradient_descent/loss_landscape.png}
    \caption{The graph of a complex loss landscape $\{(\theta,R_D(\theta))|\theta\in\R^2\}$ whose minimiser we seek. \TODO{: Make own figure.}}
    \label{fig:loss_landscape_complex}
\end{figure}

Tons of function minimisation methods exist: Newton's method, genetic algorithms, gradient descent, etc. but most are horribly slow/unstable in high dimensions; not gradient descent though! As a result, gradient descent is \textit{the} method for computing good parameters numerically in machine learning, hence the inclusion of this section.

\begin{tcolorbox}[title={\centering\textbf{Loss landscapes are dataset-dependent!}}, colback=myLightBlue, colbacktitle=myDarkBlue, colframe=myDarkBlue, coltitle=white]
    When loss landscapes are illustrated, they are really plots of the graph $\{(\theta, R_D(\theta))|\theta\in\Theta\}$. That is, they are shaped by the fixed dataset $D$. As such, computations of the form $R_{D'}(\theta)$ for $D'\subsetneq D$, e.g. computations related to mini-batch gradient descent, don't necessarily correspond exactly with $R_D(\theta)$ but this isn't usually an issue.
\end{tcolorbox}

% vanilla gradient descent
\subsection{Gradient Descent}
Since gradient descent is a general function minimisation method and not specific to machine learning, I'll use more general mathsy notation than $\theta$ and $R_D$, etc.

Anyway, say we're interested in minimising $f(\x)$. A natural idea is to start with a guess $\x^{(0)}$ and send it in the direction in which $f$ most decsends locally from $\x^{(0)}$, or equivalently the opposite of the direction in which $f$ most ascends locally from $\x^{(0)}$, i.e. computing
$$
\argmax_{||\mathbf{v}^{(0)}||=1}f\l(\x^{(0)}+\eta\mathbf{v}^{(0)}\r),
$$
for some small $\eta>0$, and updating our guess to $\x^{(1)}=\x^{(0)}-\eta\mathbf{v}^{(0)}$. Iteratively applying this idea yields the usual gradient descent rule
$$
\x^{(t+1)}
=
\x^{(t)}-\eta\mathbf{v}^{(t)}.
$$
Eventually, after taking a ton of these small steps, we should reach the bottom of the mountain, i.e. a minima of $f$.

\subsubsection{In which direction does $f$ most ascend locally from $\x^{(t)}$?}
In the following physicist-like argument, we consider only unit vectors $\mathbf{v}$. This is because we care only about the direction of the vector in question and an equation later on simplifies quite nicely due to its unit length. 

In gradient ascent, we are looking for the unit vector $\mathbf{v}$ such that $f(\mathbf{x})$ increases most in the direction of $\mathbf{v}$, i.e. we seek to compute
$$\argmax_{||\mathbf{v}||=1}\Big[f(\mathbf{x}+\eta\mathbf{v})-f(\mathbf{x})\Big]$$
where $\eta>0$. If $\eta$ is small then approximating $f(\mathbf{x}+\eta\mathbf{v})$ using its Taylor expansion about $\x$ yields
$$
f(\mathbf{x}+\eta\mathbf{v})-f(\mathbf{x})
\approx
\nabla f(\mathbf{x})\cdot\eta\mathbf{v}.
$$
Thus, the problem translates to finding the unit vector $\mathbf{v}$ that maximises
$$
\nabla f(\mathbf{x})\cdot\eta\mathbf{v}=||\nabla f(\mathbf{x})||\cdot||\eta\mathbf{v}||\cdot\cos(\theta)=\eta||\nabla f(\mathbf{x})||\cdot\cos(\theta)
$$
where $\theta\in[0,\pi)$ denotes the angle between $\nabla f(\mathbf{x})$ and $\mathbf{v}$. This expression is maximised when $\cos(\theta)=1$, i.e. when $\theta=0$, which necessitates $\mathbf{v}$ having the same direction as $\nabla f(\mathbf{x})$. So $\mathbf{v}$ is a unit vector in the direction of $\nabla f(\mathbf{x})$, i.e. $\mathbf{v}=\frac{\nabla f(\mathbf{x})}{||\nabla f(\mathbf{x})||}$. To see that $f$ ascends after being sent in the direction of $\mathbf{v}=\frac{\nabla f(\mathbf{x})}{||\nabla f(\mathbf{x})||}$ from $\mathbf{x}$ by a distance $\eta>0$, note that
$$
f(\mathbf{x}+\eta\mathbf{v})-f(\mathbf{x})
\approx
\nabla f(\mathbf{x})\cdot\eta\mathbf{v}
=
\eta\frac{\nabla f(\mathbf{x})\cdot\nabla f(\mathbf{x})}{||\nabla f(\mathbf{x})||}
=
\eta||\nabla f(\mathbf{x})||
>
0.
$$
The edge case in which $\nabla f(\mathbf{x})=0$ corresponds to $f(\mathbf{x})$ being a local maximum.

\begin{tcolorbox}[title={\centering\textbf{Multivariate Taylor expansion}}, colback=myLightBlue, colbacktitle=myDarkBlue, colframe=myDarkBlue, coltitle=white]
    Recall that if $f:\R^q\to\R$ then its second order Taylor expansion about $\mathbf{a}\in\R^q$ is given by
    $$
    f(\x)
    =
    f(\mathbf{a})+\nabla f(\mathbf{a})\cdot(\x-\mathbf{a})+\frac{1}{2}(\x-\mathbf{a})\cdot\nabla^2 f(\mathbf{a})(\x-\mathbf{a}).
    $$
\end{tcolorbox}

\subsubsection{Sensitivity to $\x^{(0)}$}
I don't know much about initialisation. I know that it's a bad idea to start at 0 (if symmetries hold then parameters change identically), points of large magnitude (exploding gradients) and points of small magnitude (vanshing gradients). Also, if the function in question is convex then don't worry so much about converging to a nice solution. In practice, empirical risk functions are not convex.

One day when my interest in initialisation sparks up, I'll write this part properly. Simon Price offers an interesting point regarding the importance of parameter initialisation around 01:08:00 in \href{https://www.youtube.com/watch?v=sJXn4Cl4oww&list=PLwFLAA-F1PgpU9lgxOdhjILkMXUagGnea&ab_channel=MachineLearningStreetTalk}{this YouTube video}.

\subsubsection{My dataset (i.e. $n$) is too large!!!}
Do not fear, for it turns out that we can approximate full gradient updates reasonably well using many smaller updates over subsets of the dataset. How well mini-batch learning and stochastic gradient descent perform (and even just \textit{that} they perform) surprised me when I first learned about them.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{./figures/gradient_descent/gradient_descent_types.png}
    \caption{How we might expect the convergence of each method to look. \TODO{: Make own figure.}}
    \label{fig:gradient_descent_types}
\end{figure}

\subsubsection*{1) Full-batch updates}
Full-batch gradient descent requires us one parameter update per epoch as in
$$
\theta^{(t+1)}
=
\theta^{(t)}-\frac{1}{n}\sum_{i=1}^n \nabla_{\theta}\mathcal{L}\l(y_i,f_{\theta^{(t)}}(\x_i)\r).
$$

\subsubsection*{2) Mini-batch updates}
All that gradient info might be too much for your PC's VRAM during training. If so, instead perform mini-batch gradient descent in which the $D$ is split into $m$ disjoint subsets $B_1,\dots,B_m$ (i.e. the dataset is split into $m$ batches) and we compute
$$
\theta^{(t+j)}
=
\theta^{(t+j-1)}-\frac{1}{|B_j|}\sum_{(\x,y)\in B_j} \nabla_{\theta}\mathcal{L}\l(y,f_{\theta^{(t+j-1)}}(\x)\r)
$$
for $j=1,\dots,m$. That is, a full epoch consists of approximately $n/m$ parameter updates and each update is with respect to a single batch of the full dataset.

\subsubsection*{3) Stochastic gradient descent (SGD)}
Turns out that doing mini-batch with $m=n$ (so one sample per update), as in
$$
\theta^{(t+j)}
=
\theta^{(t+j-1)}-\nabla_{\theta}\mathcal{L}\l(y_j,f_{\theta^{(t+j-1)}}(\x_j)\r)
$$
for $j=1,\dots,n$ is viable. Susprising; I would have thought that it'd produce nonsense parameters. In SGD, a full epoch consists of $n$ parameter updates and each is with respect to a single sample. Unsurprisingly, SGD yields relatively unstable convergence, a bit like the steps a drunk man would take in walking down the mountain. It's perhaps unsurprising that mini-batch, and thus SGD, are unbiased but high in variance.

\subsubsection{Batch + Layer normalisation}
These forms of normalisation typically only apply to learning in neural network settings. There's some piece of advice regarding which you should use when batch number is small; was it to use layer norm?

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{./figures/gradient_descent/batch_norm.png}
    \caption{An effect of Batch normalisation. \TODO{: Make own figure.}}
    \label{fig:batch_norm}
\end{figure}

\subsubsection*{1) Batch norm}
For a batch $B=\{\x_1,\dots,\x_m\}$, batch normalisation is the act of normalising the post-activation values of some layer of $d$ neurons in our architecture. Denote the $d$ activation values for the sample $\x_i\in B$ as $(a_{i,1},\dots,a_{i,d})$. The entire batch's activation values can be written as a matrix as in
$$
\begin{bmatrix}
    a_{1,1} & \dots  & a_{1,d}\\
    \vdots  & \ddots & \vdots \\
    a_{m,1} & \dots  & a_{m,d}
\end{bmatrix}.
$$
Batch norm first normalises the columns of $A$ as in
$$
\begin{bmatrix}
    \hat{a}_{1,1} & \dots  & \hat{a}_{1,d}\\
    \vdots        & \ddots & \vdots       \\
    \hat{a}_{m,1} & \dots  & \hat{a}_{m,d}
\end{bmatrix}
=
\begin{bmatrix}
    (a_{1,1}-\mu_1) / \sqrt{\sigma_1^2+\epsilon} & \dots & (a_{1,d}-\mu_d) / \sqrt{\sigma_d^2+\epsilon}\\
    \vdots & \ddots & \vdots \\
    (a_{m,1}-\mu_1) / \sqrt{\sigma_1^2+\epsilon} & \dots & (a_{m,d}-\mu_d) / \sqrt{\sigma_d^2+\epsilon}
\end{bmatrix}
$$
where $\mu_j=\frac{1}{m}\sum_{i=1}^ma_{i,j}$, $\sigma_j^2=\frac{1}{m}\sum_{i=1}^m(a_{i,j}-\mu_j)^2$ and $\epsilon>0$ is tiny. At this point, the columns have mean 0 and variance 1. The output of batch norm for sample $i\in\{1,\dots,m\}$ is given by
$$
(\gamma_1\hat{a}_{i,1}+\beta_1,\dots,\gamma_d\hat{a}_{i,d}+\beta_d)
$$
where $\gamma_1,\dots,\gamma_d$ and $\beta_1,\dots,\beta_d$ are learnable parameters.

I don't think that the effect of batch norm is immediately clear; perhaps helps avoid exploding/vanishing gradients. It was initially thought to reduce something called covariance shift but the consensus has changed and it's now believed that it simply smoothes out the corresponding loss landscape.

\subsubsection*{2) Layer norm}
If batch statistics aren't particularly meaningful or batch size is small then layer norm may be preferable. Layer norm applies to individual samples, as opposed to entire batches, and so normalises over the features instead. Given activation values $a_1,\dots,a_d$, layer norm first normalises the activation values as in
$$
(\hat{a}_1,\dots,\hat{a}_d)
=
\l(\frac{a_1-\mu}{\sqrt{\sigma^2+\epsilon}},\dots,\frac{a_d-\mu}{\sqrt{\sigma^2+\epsilon}}\r)
$$
where $\mu=\frac{1}{d}\sum_{j=1}^da_j$ and $\sigma^2=\frac{1}{d}\sum_{j=1}^d(a_j-\mu)^2$. Then, as in batch norm, we scale and shift to obtain
$$
(\gamma_1\hat{a}_1+\beta_1,\dots,\gamma_d\hat{a}_d+\beta_d)
$$
where $\gamma_1,\dots,\gamma_d$ and $\beta_1,\dots,\beta_d$ are learnable parameters.

% regularisation
\subsection{Regularisation}
Helps reduce overfitting. This subsection feels awkwardly placed but I'm unsure where it'd be better placed.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\columnwidth]{./figures/supervised_learning/regularisation_two_plots_zero_test_error.png}
    \caption{Two fits, both yielding 0 test error. \TODO{: Make own figure.}}
    \label{fig:regularisation_two_plots}
\end{figure}

The most well known regularisation techniques are L1 and L2 regularisation. They both correspond to adding some term to loss functions which encourage some sort of learning behaviour. A million ways of motivating them. The most intuitive for are Bayesian approaches: assume independence of parameters and impose a prior distribution on them according to whatever bias we hope to bake in. Then, instead of regular maximum likelihood, compute
\begin{align*}
    \argmax_{\theta\in\R^{q+1}}\log(p(\theta|D))
    &=
    \argmax_{\theta\in\R^{q+1}}\l[\log(p(D|\theta))+\log(p(\theta))\r]\\
    &=
    \argmin_{\theta\in\R^{q+1}}\l[-\sum_{i=1}^n\log(p(y_i|\x_i,\theta))-\sum_{j=1}^q\log(p(\theta_j))\r].
\end{align*}

\subsubsection*{1) L1 regularisation (LASSO)}
If we prefer our to-be-learned parameters to be close to 0, then it makes sense to choose a prior which has a lot of mass around 0. A great choice for this is the Laplace distribution with mean 0 and variance $2/\lambda$ which yields L1 regularisation. The corresponding density function is given by $p(z)=\lambda\exp(-2\lambda|z|)$ illustrated in \autoref{fig:laplace_pdf}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{./figures/supervised_learning/laplace_pdf.pdf}
    \caption{Laplace pdf with means 0 and decreasing variances according to $\lambda\in\{0.5,1,2,3\}$.}
    \label{fig:laplace_pdf}
\end{figure}

See how $\lambda$ increases, the variance of the corresponding Laplace distribution decreases and more of the mass becomes centred around 0. The result is that the to-be-learned parameters are further encouraged to flatten out around 0. Anyway, with this prior we obtain an expression for the quantity we seek to compute in
$$
\argmax_{\theta\in\R^{q+1}}\log(p(\theta|D))
=
\argmin_{\theta\in\R^{q+1}}\l[-\sum_{i=1}^n\log(p(y_i|\x_i,\theta))+\lambda\sum_{j=1}^q|\theta_j|\r]
$$

\subsubsection*{2) L2 regularisation (Ridge)}
If we prefer no given parameter to be too large then imposing some sort of MSE penalty on the parameters makes sense. This is achieved by imposing a normal prior and yields L2 regularisation. That is, if $\theta_1,\dots,\theta_q\sim\mathcal{N}(0,\sigma^2)$ i.i.d. then the quantity we seek to compute is given by
$$
\argmax_{\theta\in\R^{q+1}}\log(p(\theta|D))
=
\argmin_{\theta\in\R^{q+1}}\l[-\sum_{i=1}^n\log(p(y_i|\x_i,\theta))+\lambda\sum_{j=1}^q \theta_j^2\r]
$$
where $\lambda=1/2\sigma^2$. I don't have much more to add for L2 as I think it's pretty clear.

\subsubsection*{Visualising L1 and L2}
There's another nice way to think about these forms of regression in terms of loss landscapes: you're adding the original loss landscape to the loss corresponding to this penalty. It's almost like you sink every points in the loss landscape according to how close it is to the centre. It's clear from this visualisation how L1 and L2 reduce variance and increase bias. \TODO{: make a before and after pic for 3D loss landscape. Also, is this idea subject to initial conditions I guess? Like if the original loss landscape is already very sensitive to initial conditions then I'm unsure what regularisation does.}

\subsubsection*{3) Early stopping} Split training into training and validation sets. After each epoch (or every few), compute both training and validation losses. If training loss is decreasing while validation loss is not decreasing then you're at risk of overfitting. For illustration, consider \autoref{fig:early_stopping} in which the training loss continues to decrease in the number of epochs while the validation loss seems to reach its lowest value around epoch 28.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\textwidth]{./figures/neural_nets/REG_early_stopping.pdf}
    \caption{Training and validation losses over 50 epochs for some model. Best validation obtained at epoch 28.}
    \label{fig:early_stopping}
\end{figure}

If you observe this for sufficiently many consecutive epochs then you can stop training and use the parameters pertaining to the epoch at which the validation loss was lowest. To do this, at each epoch during training, save the parameters of the current epoch if they offer a lower validation loss than at any epoch before. This is often referred to as a checkpoint within the training process and the parameters are saved in a \texttt{.ckpt} file.

\subsubsection*{4) Dropout} We don't want to be overly-reliant on any given subset of neurons. To prevent such dependencies, at each epoch, independently set the activation of each neuron to 0 with probability $p$. This way some portion of neurons are silenced during the training epoch. As a result, its corresponding parameters (weights and bias) are not updated during backpropagation — it is truly silenced. This idea is illustrated in \autoref{fig:dropout}. Do not perform dropout at test time.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{./figures/neural_nets/REG_dropout.png}
    \caption{Dropout to prevent overdependence on given neurons. \TODO{: Make own figure.}}
    \label{fig:dropout}
\end{figure}

% momentum and adaptive learning rates
\subsection{Momentum + Adaptive Learning Rates}
There are some intuitive ideas that accelerate gradient descent. One is momentum, which builds up speed if gradients have pointed in the same direction over multiple steps. The other is an adaptive learning rate, where each parameter has its own step size that adapts over time depending on its past gradients.

\subsection*{1) Momentum}
Think of a ball rolling downhill: momentum lets it keep moving in the same direction which smoothes out noisy gradients. If gradients keep pointing in the same direction then momentum accelerates. If gradients oscillate, e.g. traversing like a ravine, then momentum dampens the motion.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{./figures/gradient_descent/momentum.png}
    \caption{How we might expect the convergence to look with momentum. \TODO{: Make own figure.}}
    \label{fig:gradient_descent_momentum}
\end{figure}

We impose this by introducing a velocity vector
$$
\mathbf{v}^{(t+1)}=\mu\mathbf{v}^{(t)}-\eta\nabla R_D\l(\theta^{(t)}\r)
$$
with $\mathbf{v}^{(0)}=0$ and switch up the gradient descent update rule to
$$
\theta^{(t+1)}
=
\theta^{(t)}-\eta\nabla R_D\l(\theta^{(t)}\r)+\mu\mathbf{v}^{(t)}
$$
where $\mu\in[0,1)$ is the momentum coefficient. Typically, $\mu\in\{0.9,0.95,0.99\}$ are taken. To get a sense of how this refelcts a momentum-like idea, see that $\mathbf{v}^{(1)}=-\eta\nabla R_D(\theta^{(0)})$ and so
$$
\mathbf{v}^{(2)}
=
-\eta\l(\mu\nabla R_D(\theta^{(0)})+\nabla R_D(\theta^{(1)})\r).
$$
If $\mu\approx1$ and the gradients $\nabla R_D(\theta^{(0)})$ and $\nabla R_D(\theta^{(1)})$ are roughly equal then $\mathbf{v}^{(2)}$ will confidently boost $\theta^{(3)}$ in the direction of the first two gradients. If they oppose one another then, as intuition would suggest, $\mathbf{v}^{(0)}=0$.

\subsection*{2) Adaptive learning rates}
Why have a fixed learning rate? Why have the same learning rate for each parameter? Adaptive learning rates looks to solve both. Some intuition to fuel how learning rates might change: if up to the $t^{\text{th}}$ parameter update the gradients for a given parameter have been large then it has presumably converged decently so its learning rate would ideally get smaller. The way AdaGrad does this is by storing a per parameter running total of gradients, as in
$$
G_i^{(t)}
=
\sum_{\tau=1}^t\l(\nabla_{\theta_i}R_D(\theta_i^{(\tau)})\r)^2
$$
and switch the parameter update to
$$
\theta_i^{(t+1)}
=
\theta_i^{(t)}-\eta_i\nabla_{\theta_i}^{(t)}R_D(\theta_i^{(t)})
$$
where $\eta_i^{(t)}=\frac{\eta}{\sqrt{G_i^{(t)}}+\epsilon}$ is the $i^{\text{th}}$ parameter's learning rate. There are more elaborate/known adaptive learning rate methods than this, e.g. RMSProp and Adam, but AdaGrad conveys the idea well.

% hyperparam tuning
\subsection{Hyperparameter Tuning}
\TODO{: Review. Mention that the only real reason I included this subsection was to remind myself of cross val}
Hyperparameters are the tunable parts of our model which are not learned during training. Bad hyperparameters will yield training issues like divergence, overfitting or underfitting. The easiest example of a hyperparameter I can think of is the learning rate in vanilla gradient descent. Some choices of learning rate will yield models which generalise better post-training than other learning rates. For a more sophisticated example, consider architecture-related hyperparameters, e.g. the number of hidden layers in a multi-layer perceptron.

\subsection*{1) Random search}
Begin by designating some portion of the training data for validation. Define a finite set of values in which each hyperparmeter may take a value. Since the cardinality of such a set will often be massive, instead of traversing the whole grid of values, consider random search: choose a number of models to train, randomly sample from the finite set for each and choose whichever yielded the lowest validation loss.

\subsection*{2) $k-$fold cross-validation}
For a better idea of how a configuration of hyperparameters will help generalise post-training, consider partitioning the training data into $k$ subsets $D_1,\dots,D_k$. Then, for $i=1,\dots,k$, train a model on $k-1$ of them and validate on the remaining. Average the $k$ validation losses. Choose whichever hyperparameter configuration yields the lowest average validation loss.

More elaborate methods of hyperparameter tuning exist, random search is a straightforward one.

\TODO{: Why not choose hyperparams which minimise test loss?}

% neural nets
\section{Neural Networks}
It's often the case that someone's intended meaning when stating `neural networks' is more precisely multi-layer perceptrons (MLPs), perhaps the simplest kind of neural network. Rigorously formulating MLPs is one of those things that's useful to do at least once. Formulating more sophisticated architectures, like CNNs, RNNs, etc., can be done in a much less rigorous manner if your use-case is solving tasks in an engineery way.

% neural networks
\subsection{Multi-Layer Perceptrons (MLPs)}
\label{sec:multi_layer_perceptrons}

Multi-layer perceptrons (MLPs) are fully-connected feed-forward networks consisting of an input layer (where data is input), hidden layers and an output layer. An MLP with three hidden layers is illustrated in \autoref{fig:MLP}. Each layer is made up of a number of neurons, each of which has a real-valued activation value.  For any neuron in a non-input layer, its activation value is the output of a non-linear activation function given, as input, the neuron's real-valued bias plus a weighted sum of the activation values of the neurons in its preceding layer. Regarding their inspiration from the human brain, consider this footnote\footnote{\url{https://stats.stackexchange.com/a/159172}}.

The application of MLPs to learning functions from data is due to their expressivity, expressive-efficiency and tractability. Their expressivity is known due to a proof of their universal function approximation by Cybenko. To add to this, their high expressive-efficiency has been demonstrated empirically and ensures that the number of model components (hidden layers and neurons) needed to represent arbitrary functions is far lower than competing model classes. As for their tractability, MLPs allow for an evaluation of the approximation of the function of interest with complexity quadratic in the number of neurons of each layer of the MLP (after the heavy simplifcation of assuming a roughly equal number of neurons in each layer).

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{./figures/neural_nets/MLP_1.pdf}
    \caption{A multi-layer perceptron (MLP) with $k=3$ hidden layers.}
    \label{fig:MLP}
\end{figure}

For brevity, given $m,n\in\mathbb{N}$ with $m\leq n$ let $[n]_m=\{m,\dots,n\}$ and let $[n]=[n]_1$. For further ease of notation, given an MLP with $k$ hidden layers, denote the index of the input layer by $0$, the output layer by $k+1$ and by extension the $j^{\text{th}}$ layer by $j\in[k+1]_0$. Additionally, note the following:
\begin{itemize}
    \item Let $n_j\in\mathbb{Z}_{\geq1}$ denote the number of neurons in layer $j$.
    \item Let $a_i^{(j)}\in\mathbb{R}$ denote the activation value of neuron $i$ in layer $j$.
    \item Let $w_{i,l}^{(j)}\in\mathbb{R}$ denote the weight associated with the edge to neuron $i$ in layer $j\in[k+1]$ from neuron $l$ in layer $j-1$.
    \item Let $b_i^{(j)}\in\mathbb{R}$ denote the bias of neuron $i$ in layer $j\in[k+1]$.
    \item Let $\sigma_j:\mathbb{R}\rightarrow\mathbb{R}$ denote the non-linear activation function of layer $j\in[k+1]$.
\end{itemize}

\noindent From here we can express the activation value of any neuron in a non-input layer in terms of the activation values of the neurons in the layer which precedes it as
\begin{align*}
    a_i^{(j+1)}
    &=
    \sigma_{j+1}\l(
    \sum_{l=1}^{n_j} w_{i,l}^{(j+1)}a_l^{(j)} + b_i^{(j+1)}
    \r)\\
    &=
    \sigma_{j+1}\l(
    \l[w_{i,1}^{(j+1)} \dots w_{i,n_j}^{(j+1)}\r]
    \begin{bmatrix}
        a_1^{(j)}\\
        \vdots\\
        a_{n_j}^{(j)}
    \end{bmatrix}
    +
    b_i^{(j+1)}
\r)
\end{align*}
for $j\in[k]_0$. The reason for writing the second equality above, involving the dot product of two vectors, is that it helps us to see how using matrix-vector notation allows us to write an elegant and compact expression for the activation values of all neurons in a non-input layer in terms of the activation values of the neurons belonging to its preceding layer as
$$
\begin{bmatrix}
    a_1^{(j+1)}\\
    \vdots\\
    a_{n_{j+1}}^{(j+1)}
\end{bmatrix}
=
\sigma_{j+1}\l(
\begin{bmatrix}
    w_{1,1}^{(j+1)} & \dots & w_{1,n_j}^{(j+1)}\\
    \vdots & \ddots & \vdots\\
    w_{n_{j+1},1}^{(j+1)} & \dots & w_{n_{j+1},n_j}^{(j+1)}
\end{bmatrix}
\begin{bmatrix}
    a_1^{(j)}\\
    \vdots\\
    a_{n_j}^{(j)}
\end{bmatrix}
+
\begin{bmatrix}
    b_1^{(j+1)}\\
    \vdots\\
    b_{n_{j+1}}^{(j+1)}
\end{bmatrix}
\r)
$$
which we abbreviate to
$$
\mathbf{a}^{(j+1)}=\sigma_{j+1}\l(\mathbf{W}^{(j+1)}\mathbf{a}^{(j)}+\mathbf{b}^{(j+1)}\r)
$$
where the activation function $\sigma_{j+1}$ is applied element-wise.

If we fix the structure and choice of activation functions of an MLP then all that is left to learn are its weights and biases. This is often done using gradient descent to minimise some loss function in which gradients are computed via back-propagation. Such a loss function can be a principled measure, e.g. corresponding to maximum likelihood, but this is not always necessary: ad-hoc loss functions are sometimes employed.

\begin{tcolorbox}[title={\centering\textbf{Advocating for depth over width}}, colback=myLightBlue, colbacktitle=myDarkBlue, colframe=myDarkBlue, coltitle=white]
    The complexity of a forward pass of an MLP can be expressed in terms of the number of neurons in each layer as
    $$
    \mathcal{O}
    \l(
    \sum_{j=1}^{k+1}n_{j-1}\cdot n_j
    \r).
    $$
    Very crudely supposing $n_0=\dots=n_{k+1}=n$ yields a complexity of
    $$
    \mathcal{O}((k+1)n^2),
    $$
    i.e. complexity grows linearly in the number of layers and quadratically in the number of neurons per layer. So we argue for increasing depth over width.
\end{tcolorbox}

\subsubsection{Example}

\begin{figure}[t]
    \centering
    \includegraphics{./figures/neural_nets/MLP_2.pdf}
    \caption{A neural network with one hidden layer ($k=1$).}
    \label{fig:neural_nets_simple_example}
\end{figure}

Consider the neural network in \autoref{fig:neural_nets_simple_example} whose input, hidden and output layers consist of two, one and two neurons respectively. With such a simple neural network, we may explicitly express the output neurons $a_1^{(2)}$ and $a_2^{(2)}$ in terms of the input neurons $a_1^{(0)}$ and $a_2^{(0)}$. We see that $n_0=2$, $n_1=1$ and $n_2=2$ and so
$\mathbf{W}^{(1)}=
\begin{bmatrix}
    w_{1,1}^{(1)} & w_{1,2}^{(1)}
\end{bmatrix}
$
and
$\mathbf{W}^{(2)}=
\begin{bmatrix}
    w_{1,1}^{(2)}\\
    w_{2,1}^{(2)}
\end{bmatrix}.
$
Noting additionally that $\mathbf{b}^{(1)}=b_1^{(1)}$ and
$\mathbf{b}^{(2)}=
\begin{bmatrix}
    b_{1}^{(2)}\\
    b_{2}^{(2)}
\end{bmatrix}$
we can explicitly express the activation of the single neuron in the hidden layer as
\begin{align*}
\mathbf{a}^{(1)}
&=
\sigma_1\l(\mathbf{W}^{(1)}\mathbf{a}^{(0)}+\mathbf{b}^{(1)}\r)\\
&=
\sigma_1\l(w_{1,1}^{(1)}a_1^{(0)}+w_{1,2}^{(1)}a_2^{(0)}+b_1^{(1)}\r)
\end{align*}
which is just the scalar value $a_1^{(1)}$. For the output layer, we have
\begin{align*}
\mathbf{a}^{(2)}
&=
\sigma_2\l(\mathbf{W}^{(2)}\mathbf{a}^{(1)}+\mathbf{b}^{(2)}\r)\\
&=
\sigma_2\l(
\begin{bmatrix}
    w_{1,1}^{(2)}a_1^{(1)} + b_1^{(2)}\\
    w_{2,1}^{(2)}a_1^{(1)} + b_2^{(2)}
\end{bmatrix}
\r)\\
&=
\sigma_2\l(
\begin{bmatrix}
    w_{1,1}^{(2)}\sigma_1\l(w_{1,1}^{(1)}a_1^{(0)}+w_{1,2}^{(1)}a_2^{(0)}+b_1^{(1)}\r) + b_1^{(2)}\\
    w_{2,1}^{(2)}\sigma_1\l(w_{1,1}^{(1)}a_1^{(0)}+w_{1,2}^{(1)}a_2^{(0)}+b_1^{(1)}\r) + b_2^{(2)}
\end{bmatrix}
\r)\\
&=
\begin{bmatrix}
    \sigma_2\l(w_{1,1}^{(2)}\sigma_1\l(w_{1,1}^{(1)}a_1^{(0)}+w_{1,2}^{(1)}a_2^{(0)}+b_1^{(1)}\r) + b_1^{(2)}\r)\\
    \sigma_2\l(w_{2,1}^{(2)}\sigma_1\l(w_{1,1}^{(1)}a_1^{(0)}+w_{1,2}^{(1)}a_2^{(0)}+b_1^{(1)}\r) + b_2^{(2)}\r)
\end{bmatrix}.
\end{align*}

\noindent As such, the activations of the output neurons are given by
\begin{align*}
    a_1^{(2)}&=\sigma_2\l(w_{1,1}^{(2)}\sigma_1\l(w_{1,1}^{(1)}a_1^{(0)}+w_{1,2}^{(1)}a_2^{(0)}+b_1^{(1)}\r) + b_1^{(2)}\r)\\
    a_2^{(2)}&=\sigma_2\l(w_{2,1}^{(2)}\sigma_1\l(w_{1,1}^{(1)}a_1^{(0)}+w_{1,2}^{(1)}a_2^{(0)}+b_1^{(1)}\r) + b_2^{(2)}\r).
\end{align*}

\begin{tcolorbox}[title={\centering\textbf{Further advocating for depth over width}}, colback=myLightBlue, colbacktitle=myDarkBlue, colframe=myDarkBlue, coltitle=white]
    A perhaps more important result is how, geometrically, MLPS split the input space into a bunch of regions. It turns out that the number of regions by which a single-hidden layer MLP with two input neurons and $n$ neurons in its hidden layer splits the space is bounded above by
    $$
    \frac{1}{2}(n^2+n+2)
    $$
    which is quadratic in $n$. For an MLP with $k$ hidden layers of $n$ neurons each, the number of regions by which the space is split is bounded above by
    $$
    \frac{1}{2}(n^2+n+2)\l(\frac{n}{n_0}+1\r)^{n_0(k-1)}
    $$
    which is exponential in $k$. Again, we argue for increasing depth $k$ over width $n$. A natural quesiton is to what extent these bounds are tight in practice.
\end{tcolorbox}

% universal function approximation theorem
\subsubsection{The Universal Function Approximation of MLPs}
\label{subsubsec:universal_function_approximation_theorem}
\TODO{: Add figures.} A class of functions, or function class, $\mathcal{F}_S$ is capable of universal function approximation on $S\subset\R^q$ compact (closed and bounded) if for all continuous $f:S\to\R$ and $\epsilon>0$ there exists some $\hat{f}\in\mathcal{F}_S$ such that
$$
\max_{x\in S}\l|f(x)-\hat{f}(x)\r|<\epsilon.
$$
Note that such a function family can be used to approximate functions whose codomain is $\R^m$ by performing coordinate-wise approximation. 

The family of polynomials is capable of universal function approximation as shown by Weierstrass in 1885. A directly equivalent statement to a function class satisfying the universal function approximation is the function ckass being dense in $C(S)$ with respect to the supremum norm. Here, $C(S)$ denotes the set of all continuous real-valued functions with compact domain $S\subset\R^q$. Quick note: usually the domain of the to-be-approximated function is denoted by $K$ but I denote the number of layers in an MLP using a $k$ so I'd like to avoid the confusion by using $S$ instead. Instead of offering a rigorous proof, I'll motivate the overall idea in three steps starting with a simpler case: functions of the form $f:S\to\R$ where $S\subset\mathbb{N}$ with $\#S<\infty$.

\subsubsection*{Step 1: Motivating the idea for $S$ finite}
Without loss of generality, let $S=[\#S]$ and so we seek to show that for all $\epsilon>0$ there exists an MLP with one hidden layer, whose output is denoted by $\hat{f}(x)$, which satisfies
$$
\max_{x\in [\#S]}\l|f(x)-\hat{f}(x)\r|<\epsilon.
$$
First note that for all $x\in[\#S]$,
\begin{align*}
f(x)
&=
\sum_{s=1}^{\#S}f(k)\mathbbm{1}(x=s)\\
&=
\sum_{s=1}^{\#S}f(k)(\mathbbm{1}(x\geq s)-\mathbbm{1}(x\geq s+1))
\end{align*}
so we already see that a single neuron in the ouput layer of a single hidden layer MLP ($2\cdot\#S$ neurons in hidden layer) with weights
$$
\mathbf{W}^{(2)}
=
\begin{bmatrix}
    f(1) & -f(1) & \dots & f(\#S) & -f(\#S)
\end{bmatrix},
$$
i.e. $w_{1,s}^{(2)}=(-1)^{s+1}f\l(\l\lfloor\frac{s+1}{2}\r\rfloor\r)$, and bias $\mathbf{b}^{(2)}=0$ is sufficient if the $2\cdot\#S$ neurons in the hidden layer approximate
$$
\mathbbm{1}(x\geq1),\mathbbm{1}(x\geq2),\mathbbm{1}(x\geq2),\mathbbm{1}(x\geq3),\dots,\mathbbm{1}(x\geq\#S),\mathbbm{1}(x\geq\#S+1)
$$
with the absolute error of each approximation bounded by $\epsilon$. Note that the final indicator $\mathbbm{1}(x\geq\#S+1)$ is redundant so you only really need $2\cdot\#S-1$ neurons but I'll keep it for the nicer number. Also note that these are just renditions of the Heaviside function but I prefer sticking with the indicator function notation. This exact idea can be realised elegantly using the sigmoid as activation for the hidden layer. To see this, note that
\begin{align*}
    \sigma\l(10^{\#S}(x-n)+10^{\#S-9}\r)
    &=
    \frac{1}{1+\exp\l(-10^{\#S}(x-n)+10^{\#S-9}\r)}\\
    &=
    \frac{1}{1+\exp\l(n-x+10^{-9}\r)^{10^{\#S}}}
\end{align*}
approximates the Heaviside function $H(x-n)$ well with some tiny leakage. To hit certain levels of precision, i.e. attaining the desired $\epsilon$ bound, simply change the $-9$ exponent to something smaller. You can probably express the exponent in terms of $\epsilon$ explicitly (maybe $\epsilon^{-1}$) but the idea is the point here. So what we want is for the $\l\lceil\frac{s+1}{2}\r\rceil^{\text{th}}$ neuron in the hidden layer to approximate $H\l(x-\l\lceil\frac{s+1}{2}\r\rceil\r)$ for $s\in[\#S]$. This is straightforward using a sigmoid activation function in the hidden layer given the approximation above. Simply set the weights and biases accordingly: $w_{s,1}^{(1)}=10^{\#S}$ and $b_s^{(1)}=-10^{\#S}\l(\l\lceil\frac{s+1}{2}\r\rceil-10^{-9}\r)$ and we're done.

\subsubsection*{Step 2: Extending the idea to $S\subset\R$ compact}
If $f\in C(S)$ for $S\subset\R$ compact then (uniform continuity) for all $\epsilon>0$ there exists $\delta>0$ such that
$$
|x-y|<\delta\implies|f(x)-f(y)|<\epsilon.
$$
Note that if we have a Lipschitz constant $L$ for a given $f$ then $\delta=\epsilon/L$ suffices. The reason this $\epsilon-\delta$ statement is useful is that if we split $S$ into $N$ subintervals whose width is bounded by $\delta$ then
$$
\l|f(x)-f(c)\r|<\epsilon
$$
for all $c,x$ in the same subinterval. To realise this idea, if $S=[a,b]$ then split $S$ according to
$$
a=x_0<x_1<\dots<x_N=b
$$
such that $x_{i+1}-x_i<\delta$ and so take $c=(x_i+x_{i+1})/2$ the midpoint of $[x_i,x_{i+1}]$. The number of subintervals $N$ needed depends on $a,b$ and $\delta$, e.g. $N=\l\lceil\frac{b-a}{\delta/2}\r\rceil$ works but the denominator just needs to be less than $\delta$ so $\delta/1.1$ would give a tighter $N$. From this we see that
$$
N
\approx
\frac{b-a}{\delta}
=
\frac{L(b-a)}{\epsilon}.
$$
The higher we take $N$ the more accurate of an approximation we get. This idea of splitting $S$ into $N$ intervals is nice because it requires us to approximate only one point $c_i=(x_i+x_{i+1})/2$ on each subinterval, i.e. we are back to the simpler case which started our motivation of approximating at a fixed number of points.

With this idea in mind, the number of required neurons to realise our idea
$$
2N
=
2\frac{L(b-a)}{\epsilon}
$$
grows linearly in $L$, the interval length $b-a$ and the reciprocal of the desired level of precision $1/\epsilon$. In practice, you only have control of the desired level of precision $\epsilon$.

\subsubsection*{Step 3: Extending the idea to $S\subset\R^q$ compact}
The idea is really not very different. Imagine $S\subset\R^2$ compact. The compactness is very hepful for visualisation. As long as we can create indicator-approximate functions on separate regions of $S$ then use exactly the idea described above: split $S$ into separate regions and approximate $f$ on a single point in each. To see that we can approximate the indicator on regions of $S\subset\R^2$ simply see that we can create wall-like objects using the same idea of creating a 'steep' plane and sigmoiding it. As such, to isolate some region, produce three walls which enclose a region and you're done. If you want a more accurate enclosing of the region then use more walls. Without boundedness, which is given by compactness, this idea wouldn't work. Extend this idea to $\R^q$ and voilà.

\begin{tcolorbox}[title={\centering\textbf{Is this idea practical?}}, colback=myLightBlue, colbacktitle=myDarkBlue, colframe=myDarkBlue, coltitle=white]
    What the idea conveyed shows is \textit{that} (and even \textit{how}) an MLP can approximate any function in $C(S)$ for $S\in\R^q$ compact. It is not at all an indicator of how MLPs learned via gradient descent, in practice, go about approximating functions from data.

    \hspace{14pt} An interesting addition to this note is how this formulation gives an idea of how the weights from the hidden layer to the output explicitly encode the evaluation of the to-be-approximated function. Again, this is not what we'd expect MLPs to do in practice, instead they find a much more efficient representation, sharing information between neurons. Reminds me a lot of the research 3Blue1Brown discussed in one of his videos about transfromers and LLMs in which we spoke about how the LLM `knows' to assign a high probability to the token \texttt{Jordan} when given \texttt{My favourite basketballer is Michael} as input. How does it `know' to do that? It isn't at all implied by the grammar/semantics/whatever of the sentence. Said research concluded things about such pieces of knowledge being stored in the weights of the MLPs of the transformer blocks.
\end{tcolorbox}

\subsubsection{Classification via neural networks}
Which space does one transform their samples into in order to linearly separate them? In the case of neural net approaches, for $K-$classification, it's the $(K-1)$-probability simplex given by
$$
\l\{(x_1,\dots,x_K)\in[0,1]^K|x_1+\dots+x_K=1\r\}.
$$
The $2-$probability simpelx coloured according to argmax is illustrated in \autoref{fig:2_prob_simplex}.
\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{./figures/neural_nets/MLP_simplex_argmax.png}
    \caption{The $2-$probability simplex coloured according to argmax.}
    \label{fig:2_prob_simplex}
\end{figure}
From the figure, it is clear how argmax can computed by splitting the $2$-probability simplex via two planes, e.g. the intersection of $x>y$ and $x>z$ on the simplex yield the red region. As such, neural nets used for classification can be seen as performing regression on the $2-$simplex, as in learning $p(y|\X=\x)$.

% backprop
\subsection{Backpropagation for MLPs}
\label{subsec:backprop}

Backpropagation is a method of computing the gradient of a loss function pertaining to an MLP (or more generally some neural network) with respect to its weights and biases. Said gradients are used to learn suitable MLP parameters via gradient descent. We'll soon see why it's called backpropagation: it computes the gradients of interest in a way that requires going forwards through the MLP and then backwards. As with formulating MLPs, it's good to rigorously derive backpropagation at least once. It's sort of strange that I haven't done that yet (August 2025 is time of writing) and have just trusted that it holds up.

Onto the derivation, which makes use of the notation introduced in \autoref{sec:multi_layer_perceptrons}. Recall that a loss function is of the form
$$
\mathcal{L}:\Omega_{Y}\times\Omega_{Y}\to\R_{\geq0}
$$
and that the loss induced by a choice of model parameters $\theta$ for a sample $(\x,y)$, which we will henceforth crudely denote by $\mathcal{L}$, is given by
$$
\mathcal{L}\l(y,f_{\theta}(\x)\r),
$$
where $f_{\theta}$ is the model itself. Note that taking the gradient of $\mathcal{L}(y,f_{\theta}(\x))$ with respect to $\theta$ is perfectly valid even though the function $\mathcal{L}$ itself is not a function of $\theta$. For an MLP,
$$
\theta
=
\l(\mathbf{W}^{(1)},\dots,\mathbf{W}^{(k+1)},\mathbf{b}^{(1)},\dots,\mathbf{b}^{(k+1)}\r).
$$
Let $\mathbf{z}^{(j)}=\mathbf{W}^{(j)}\mathbf{a}^{(j-1)}+\mathbf{b}^{(j)}$ for $j\in[k+1]$ from which we have $\mathbf{a}^{(j)}=\sigma_{j}\l(\mathbf{z}^{(j)}\r)$. Further, let
$$
\delta^{(j)}_i
=
\frac{\partial\mathcal{L}}{\partial z^{(j)}_i}
$$
for $j\in[k+1]$ and $i\in[n_j]$ where $\l(z_1^{(j)},\dots,z_{n_j}^{(j)}\r)=\mathbf{z}^{(j)}$. We seek to derive four statements:
\begin{enumerate}
    \item $\delta_i^{(k+1)}=\sigma_{k+1}'\l(z_i^{(k+1)}\r)\frac{\partial\mathcal{L}}{\partial a_i^{(k+1)}}$ for $i\in[k+1]$
    \item $\delta_i^{(j)}=\sigma_{j}'\l(z_i^{(j)}\r)\l(\l(\mathbf{W}^{(j+1)}\r)^\top\delta^{(j+1)}\r)_i$ for $j\in[k]$ and $i\in[n_j]$
    \item $\frac{\partial\mathcal{L}}{\partial b^{(j)}_i}=\delta^{(j)}_i$ for $j\in[k+1]$ and $i\in[n_j]$
    \item $\frac{\partial\mathcal{L}}{\partial w_{l,i}^{(j)}}=\delta_l^{(j)}a_i^{(j-1)}$ for $j\in[k+1]$, $i\in[n_{j-1}]$ and $l\in[n_j]$
\end{enumerate}
Before deriving each statement in order, we can see already what was meant earlier by propagating backwards through the MLP: to compute the gradients relevant to performing gradient descent, backpropagation requires the computation of $\mathbf{z}^{(1)},\dots,\mathbf{z}^{(k+1)}$ from which $\delta^{(k+1)}$ can be computed. Note that the $\frac{\partial\mathcal{L}}{\partial a_i^{(k+1)}}$ term in the first statement is often easily computed, e.g. for MSE loss we have
$$
\frac{\partial\mathcal{L}}{\partial a_i^{(k+1)}}
=
a_i^{(k+1)}-y_i.
$$
From there, one can compute $\delta^{(k)},\dots,\delta^{(1)}$ in that order, effectively propagating through the MLP backwards. With $\delta^{(1)},\dots,\delta^{(k+1)}$ and $\mathbf{a}^{(0)},\mathbf{a}^{(1)}=\sigma_1\l(\mathbf{z}^{(1)}\r),\dots,\mathbf{a}^{(k+1)}=\sigma_{k+1}\l(\mathbf{z}^{(k+1)}\r)$, computing the relevant gradients is straightforward.

\vspace{10pt}
\noindent\textbf{Statement 1.} Using the chain rule, see that
\begin{align*}
    \delta_i^{(k+1)}
    &=
    \frac{\partial\mathcal{L}}{\partial z_i^{(k+1)}}\\
    &=
    \frac{\partial\mathcal{L}}{\partial a_i^{(k+1)}}\frac{a_i^{(k+1)}}{\partial z_i^{(k+1)}}\\
    &=
    \sigma_{k+1}'\l(z_i^{(k+1)}\r)\frac{\partial\mathcal{L}}{\partial a_i^{(k+1)}}.
\end{align*}

\noindent\textbf{Statement 2.} First, note that
\begin{align*}
    \l(\mathbf{W}^{(j+1)}\r)^\top\delta^{(j+1)}
    &=
    \begin{bmatrix}
        w_{1,1}^{(j+1)}   & \dots  & w_{n_{j+1},1}^{(j+1)}\\
        \vdots            & \ddots & \vdots               \\
        w_{1,n_j}^{(j+1)} & \dots  & w_{n_{j+1},n_j}^{(j+1)}
    \end{bmatrix}
    \begin{bmatrix}
        \delta_1^{(j+1)}\\
        \vdots\\
        \delta_{n_{j+1}}^{(j+1)}
    \end{bmatrix}\\
    &=
    \sum_{l=1}^{n_{j+1}}\delta_l^{(j+1)}
    \begin{bmatrix}
        w_{l,1}^{(j+1)}\\
        \vdots\\
        w_{l,n_j}^{(j+1)}
    \end{bmatrix}
\end{align*}
and so
$$
\l(\l(\mathbf{W}^{(j+1)}\r)^\top\delta^{(j+1)}\r)_i
=
\sum_{l=1}^{n_{j+1}}\delta_l^{(j+1)}w_{l,i}^{(j+1)}
$$
which reduces the to-be-derived statement to
$$
\delta_i^{(j)}
=
\sigma_j'(z_i^{(j)})\sum_{l=1}^{n_{j+1}}\delta_l^{(j+1)}w_{l,i}^{(j+1)}.
$$
Using the chain rule, we deduce that
\begin{equation*}
    \label{eq:back_prop_statement_2}
    \delta_i^{(j)}
    =
    \frac{\partial\mathcal{L}}{\partial z_i^{(j)}}
    =
    \sum_{l=1}^{n_{j+1}}\frac{\partial\mathcal{L}}{\partial z_l^{(j+1)}}\frac{\partial z_l^{(j+1)}}{\partial z_i^{(j)}}
    =
    \sum_{l=1}^{n_{j+1}}\delta_l^{(j+1)}\frac{\partial z_l^{(j+1)}}{\partial z_i^{(j)}}.
\end{equation*}
To complete our deduction, see that
$$
z_l^{(j+1)}
=
\sum_{i=1}^{n_j}w_{l,i}^{(j+1)}\sigma_{j+1}\l(z_i^{(j)}\r)+b_l^{(j+1)}
$$
from which we obtain
$$
\frac{\partial z_l^{(j+1)}}{\partial z_i^{(j)}}
=
\sigma_{j+1}'(z_i^{(j)})w_{l,i}^{(j+1)}
$$
completing the deduction.

\vspace{10pt}
\noindent\textbf{Statement 3.} This one's a simple application of the chain rule, as in
$$
\frac{\partial\mathcal{L}}{\partial b_i^{(j)}}
=
\sum_{l=1}^{n_{j+1}}\frac{\partial\mathcal{L}}{\partial z_l^{(j)}}\frac{\partial z_l^{(j)}}{\partial b_i^{(j)}}
=
\frac{\partial\mathcal{L}}{\partial z_i^{(j)}}
=
\delta_i^{(j)}.
$$
\textbf{Statement 4.} As before, see that
$$
z_l^{(j)}
=
\sum_{i=1}^{n_j}w_{l,i}^{(j)}a_i^{(j-1)}+b_l^{(j)}
$$
which yields
$$
\frac{\partial z_l^{(j)}}{\partial w_{l,i}^{(j)}}
=
a_i^{(j-1)}.
$$
To complete the derivation, consider
$$
\frac{\partial\mathcal{L}}{\partial w_{l,i}^{(j)}}
=
\frac{\partial\mathcal{L}}{\partial z_l^{(j)}}\frac{\partial z_l^{(j)}}{\partial w_{l,i}^{(j)}}
=
\delta_l^{(j)}a_i^{(j-1)}.
$$

% convolutional neural networks
\subsection{Convolutional Neural Networks (CNNs)}
\label{subsec:conv_neural_networks}
Convolutional neural networks (CNNs) are neural networks whose architectures are designed to process image data. Illustrated in \autoref{fig:CNN_frog}, their architecture can be described on a high level as consisting of two parts. The first part performs feature extraction through convolutions and pooling. The second part is an MLP which uses the extracted feature values to produce the overall model's output, e.g. the probability that the input object belongs to a given class. Feature extraction into MLP, pretty intuitive.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{./figures/neural_nets/CNN_frog.pdf}
    \caption{A binary classification CNN processing an image of a frog.}
    \label{fig:CNN_frog}
\end{figure}

Before these deep learning architectures, researchers would have use handcrafted features and feed the feature values to a given model, e.g. an SVM for classification. Humans turn out to be far worse at knowing what features to use than well-trained machine learning models. So how do these earlier layers in CNN architectures perform feature extraction? Convolution and pooling layers. \TODO{: Clarify that Feature maps != Filters/kernel}

\subsubsection{Convolutions}
Convolution operations involve using a square matrix to, in some sense, summarise the information embedded in the parts of the imaage which it traverses over. This square matrix is referred to as a filter or a kernel. I prefer referring to it as a filter. You can think of this convolution operation as a sort of dot product between the filter and the grid of pixel values it is on top of at that point.

More formally, given a feature map $I\in\mathbb{R}^{C\times W\times H}$ and a filter $K\in\mathbb{R}^{C\times k\times k}$ with $k<W$ and $k<H$, the feature map $F=K*I\in\mathbb{R}^{W'\times H'}$ given by the convolution of $K$ over $I$ is given by
$$
F_{i,j}
=
\sum_{c=1}^C\sum_{x=1}^{k}\sum_{y=1}^{k}K_{c,x,y}\cdot I_{c,i+x,i+y}+b_c
$$
where $b_c$ is the filter's bias in channel $c$. Note that this notation is intended to illustrate what a convolution looks like algebraically. As it stands, what's written above does not account for padding, stride, etc. For a simpler illustration, consider \autoref{fig:convolution_operation}.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{./figures/neural_nets/CNN_convolution.png}
    \caption{Convolution without bias. \TODO{: Make own figure.}}
    \label{fig:convolution_operation}
\end{figure}

Generally, earlier convolutions in a CNN architecture extract higher level features, like edges and textures, while later convolutions extract lower level features which are not human-interpretable. This is observed in \autoref{fig:CNN_frog} where after the first convolutional layer one can still see a resemblance of the input image of a frog. The feature maps produced by the second convolutional layer do not maintain a human-interpretable resemblance of the frog.

\subsubsection{Pooling}
Pooling operations reduce the spatial dimensions of a given feature map by, in some sense, clustering certain parts of the feature map. There's not much point to writing this mathematically rigorously. Instead, consider \autoref{fig:pooling}. Pooling has a ton of flavours: max, min, average and more.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{./figures/neural_nets/CNN_pooling.pdf}
    \caption{The core flavours of pooling. \TODO{: Make own figure.}}
    \label{fig:pooling}
\end{figure}

\noindent I typically just pick max pooling and see how it does. A nice benefit of pooling is improved translation-invariance. Ideally, our model would classify an image in a similar enough way to how it'd classify the same image but slightly shifted. What is meant here is very problem-dependent so take it with a grain of salt. Hopefully the idea is clear anyway. Another benefit to pooling the reduced compute.

\subsubsection{Output dimensions after convolving and pooling}
Given a feature map $F\in\mathbb{R}^{W\times H}$, suppose we compute the convolution $O\in\mathbb{R}^{W'\times H'}$ of the filter $K\in\mathbb{R}^{k\times k}$ over $F$ with uniform padding $P$ and stride $S$. The precise dimensions $W'$ and $H'$ of the new feature map $O$ are given by
$$
W'
=
\l\lfloor
\frac{W+2P-k}{S}
\right\rfloor
+1
$$
and
$$
H'
=
\l\lfloor
\frac{H+2P-k}{S}
\right\rfloor
+1.
$$
It's not too tricky to derive these. It's the kinda thing you do once and never again. \TODO{: complete.}

\begin{tcolorbox}[title={\centering\textbf{Why use a CNN over an MLP?}}, colback=myLightBlue, colbacktitle=myDarkBlue, colframe=myDarkBlue, coltitle=white]
    Improved parameter-efficiency, translation invariance and learning-efficiency\footnote{\href{https://ai.stackexchange.com/questions/27407/what-does-statistical-efficiency-mean-in-this-context}{\texttt{ai.stackexchange.com/questions/27407}}}, i.e. fewer samples needed to learn distributions to similar degrees of precision.
\end{tcolorbox}

\subsubsection{Inductive biase}
Seems like a good point in the document to talk about inductive bias\dots

% recurrent neural networks
\subsection{\TODO{: }Recurrent Neural Networks (RNNs)}

\subsubsection{Long Short-Term Memory (LSTMs)}

\subsubsection{Gated Recurrent Units (GRUs)}

% transformer models
\subsection{\TODO{: }Transformers}
Just another (very effective) architecture for correlation extraction.

\subsubsection{Tokens and Embedding}
Tokenisation is the process of slicing a sample up into tokens. Think of splitting a $512\times512$ image into its four $256\times256$ quadrants or the sentence `The chef cooked a meal for the critic.' into
$$
\colorbox{purple}{The}\colorbox{green}{ chef}\colorbox{brown}{ cooked}\colorbox{red}{ a}\colorbox{teal}{ meal}\colorbox{blue}{ for}\colorbox{green}{ the}\colorbox{brown}{ critic}\colorbox{purple}{.}
$$
The purpose of tokenisation is self-explanatory: we'd like an idea of dependence between different parts of the input as to aid our output prediction.

An embedding function $E:\{1,\dots,|S|\}\to\mathbb{R}^d$, where $d\in\mathbb{N}$ is the embedding dimension, offers high dimensional numeric representations of tokens. What makes token embedding functions interesting is how well they can capture underlying semantics, e.g. of natural language. For example, an embedding function learned on pieces of text might satisfy
$$
E(\text{Germany})+E(\text{fascist})-E(\text{Mussolini})
\approx
E(\text{Hitler})
$$
for which the tokenisations occur.

\subsubsection{Positional encoding}
Use sinusoids to form a position vector $P_k$ for the $k$th token in an input sequence of length $L$. Add $P_k$ to the word embedding $E_k$ for $k=0,\dots,L-1$ to obtain the input $[E_0+P_0,\dots,E_{L-1}+P_{L-1}]$ fed to the model.

\subsubsection{Architecture}
\dots

% misc. questions
\addtocontents{toc}{\protect\setcounter{tocdepth}{1}}
\subsection{Misc. questions}
\addtocontents{toc}{\protect\setcounter{tocdepth}{2}}

Some questions to challenge one's understanding.

\begin{center}
    \textbf{\textcolor{blue}{Q1:} What is the purpose of activation functions?}
\end{center}
Without the application of at least one activation function, the values of the output neurons would simply be the result of matrix-vector multiplication and vector addition. That is to say, the output of the neural network, without an any activation function, would be a purely linear transformation of the input. The issue with this is that most problems require some degree of non-linearity. The staple example of this is the classification of two-dimensional samples which are not linearly separable. Two such cases are illustrated in \autoref{fig:non_linearly_separable}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.40\columnwidth]{./figures/neural_nets/NLA_xor.png}
    \hspace{20pt}
    \includegraphics[width=0.40\columnwidth]{./figures/neural_nets/NLA_circle.png}
    \caption{Examples of samples belonging to classes that are not linearly separable. \TODO{: Make properly.}}
    \label{fig:non_linearly_separable}
\end{figure}

The point is that there is no line that would separate the classes in either case: non-linearity is needed! In the latter case, it is desirable to be able to somehow punch the interior of the unit disc to form a blue mountain with the surrounding ground covered in red. This can be achieved by the non-linear transformation $\phi(x,y)=\max(0,1-(x^2+y^2))$ which is illustrated in \autoref{fig:non_lin_transformation}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.40\columnwidth]{./figures/neural_nets//NLA_non_lin_transform.png}
    \caption{The image of our non-linear transformation. \TODO{: Make properly.}}
    \label{fig:non_lin_transformation}
\end{figure}

From here, a natural linear decision boundary is the plane $z=0$ (i.e. the red ground surrounding the blue mountain). Any sample above the plane, i.e. any sample whose $z-$coordinate is positive, is classified as blue. It is otherwise classified as red. We can be more clever than this though. This transformation required us to map samples to 3D — how about mapping to just 1D? To do this, a complete decision function is given by
$$
D(x,y)=\l\lceil\frac{\l\lfloor x^2+y^2\right\rfloor}{x^2+y^2}\right\rceil
$$
and define $D(0,0)=0$ to account for the removable singularity.

\begin{center}
    \textbf{\textcolor{blue}{Q2:} Why not just one activation function in the output layer?}
\end{center}
If there were a single activation function towards the end of the architecture then everything that came before would be a series of linear transformations of the input. The composition of linear transformations is just a linear transformation. As such, said model would be equivalent to a single layer perceptron, i.e. linear transformation + non-linear activation function, which would yield linear decision boundaries only.

To be super pedantic, single-layer perceptrons yielding only linear decision boundaries is the case if the activation function used is monotonic, which is often the case. If you allow for weird, non-monotonic activations then this linear decision boundary result doesn't necessary hold but, again, this is a pretty pedantic detail.

\begin{center}
    \textbf{\textcolor{blue}{Q3:} Must activation functions be monotonic?}
\end{center}
Nope, it just makes things related to optimisation easier while having a sufficiently-small effect on performance in practice. Some results surrounding neural networks necessitate monotonic activation functions though.

\begin{center}
    \textbf{\textcolor{blue}{Q4:} Which activation functions should be used when?}
\end{center}
Roughly speaking, I wouldn't worry too much about this in practice. The usual choices are all nice and differentiable (with small caveats, like with ReLU at 0) facilitating backpropagation. You should sometimes care about the output given the problem at hand. For example, if you need outputs in $[-1,1]$ then $\tanh$ is a natural choice. Note that some choices, like Leaky ReLU, introduce hyperparameters to the model which is sometimes undesirable, e.g. if one seeks to minimise the number of hyperparameters for a strength of their result (if there's no hyperparameter to cherry pick then your result is more trustworthy).

\begin{center}
    \textbf{\textcolor{blue}{Q5:} How does one prevent vanishing/exploding gradients?}
\end{center}

\noindent Skip connections help. Architectures which use them are sometimes called residual networks.
\begin{itemize}
    \item Most intuitive benefit: it helps prevent some layer in the architecture from degrading the input entirely. It's like a nice reminder to the model what it was working from before it got to this point
    \item Reduces vanishing or exploding gradients: opens the door to far deeper architectures
    \item Ensures that the model has to learn the residual as opposed to the full underlying function: faster convergence usually
    \item Takes pressure off parameter initialisation: if you set parameters to zero then initially model is just the identity and learning from there is feasible
\end{itemize}

Worth mentioning that said vanishing behaviour is also somtimes observed in forward passes.

% generative models (any other section name i've come up with has been due to overthinking)
\section{Generative Models}

How is a mention of modelling the joint not in here? E.g. GPT models sequentially sample from conditional distributions. Almost feels like classification\dots

\TODO{: Re-word paragraph and emphasise that it's all about modelling the joint distribution. Also get in how this is what most of the sciences most often do. Also, mention how these approaches are more generally deep generative models but my interests are in vision generative things.} Since the era of deep learning began in 2012, tons of generative image, video and text models have arisen. The general idea of these generative models is that if we have a model that fits the data generating process, e.g. the relevant distribution if taking a probabilistic approach, then samples from the model should be sufficiently convincing. That is, if I get a nice fit for the distribution of images of cats lounging in the sun then samples from my fit distirubtion should look convincingly like cats lounging in the sun.

The problem with fitting such a distribution is that it is not at all clear what the distribution function would be. These distributions are complex and generally very hard to get right. Most ways to fit the underlying distribution is to perfrom maximum likelihood in some way. Performing maximum likelihood directly without access to any sort of direct distribution function is pretty hard, so it is often done indirectly. For example, when training a variatioanl autoencoder (VAE), you look to maximise a lower bound of the likelihood. The idea is that if this lower bound is sufficiently tight then maximising it in some sense pushes up the true likelihood, maximising it.

% bayesian nets
\subsection{\TODO{ }Bayesian networks}
Probabilistic circuits should get an honourable mention

% variational autoencoders
\subsection{Variational Autoencoders (VAEs)}
\TODO{: re-word whole subsection.} Let's first motivate autoencoders which are essentially dimension reduction machines. From there, we can add some spice in the form of something that allows us to do generative things.

% autoencoders
\subsubsection{Autoencoders: no variation yet!}
\label{sec:autoencoders}
An autoencoder $(\Omega_{\X},d,\theta,\phi)$ consists of a sample space $\Omega_{\X}\subset\mathbb{R}^n$, a latent dimension $d\in\mathbb{Z}_{\geq1}$, an encoder $\theta:\mathbb{R}^n\to\mathbb{R}^d$ and a decoder $\phi:\mathbb{R}^d\to\mathbb{R}^n$. Using a subset of $\Omega_{\X}$, one typically learns the encoder $\theta$ and the decoder $\phi$ with the goal of approximating the identity on $\Omega_{\X}$ via $\phi\circ \theta$. That is, roughly put, one seeks to obtain an encoder/decoder pair such that $\phi(\theta(\x))\approx\x$ for all $\x\in\Omega_{\X}$.

\begin{figure}[t]
    \centering
    \includegraphics{./figures/neural_nets/AE_autoencoder.pdf}
    \caption{An autoencoder in which $\theta$ and $\phi$ are fit using multi-layer perceptrons (MLPs).}
    \label{fig:autoencoder}
\end{figure}

In intuitive terms, one can see the encoder $\theta$ as a compressor of samples in $\Omega_{\X}$ to their compressed (or latent) representation in $\mathbb{R}^d$ and so one might refer to the image of $\theta$ as the latent space $\Omega_{\Z}$. Similarly, the decoder $\phi$ can be seen as a decompressor of compressed representations $\z\in\Omega_{\Z}$ yielding the original sample $\x$. In line with this notion of an autoencoder as a compressor/decompressor, the latent dimension $d$ is typically taken to be far smaller than the dimension of the distribution of interest, i.e. $d\ll n$. Of course, when $d\ll n$, learning such mappings $\theta$ and $\phi$ typically involves some loss of information if $\Omega_{\X}$ is a manifold whose intrinsic dimension is greater than $d$. That is, autoencoders are typically lossy compressors.

\begin{tcolorbox}[title={\centering\textbf{Autoencoder example}}, colback=myLightBlue, colbacktitle=myDarkBlue, colframe=myDarkBlue, coltitle=white]
    Suppose $\Omega_{\X}=\{(a,a,b)\in\mathbb{R}^3:||(a,a,b)||\leq1\}$ and $d=2$. One immediately notices that $\Omega_{\X}$ is a two-dimensional surface embedded in $\mathbb{R}^3$ as it is the intersection of the unit ball and the plane $\{(a,a,b):a,b\in\mathbb{R}\}$. To produce representations of $\x=(a,a,b)\in\Omega_{\X}$ in $\mathbb{R}^2$ (i.e. to compress a sample) one might employ the encoder $\theta_2(x,y,z)=(x,z)$. To reconstruct samples from their latent representation (i.e. to decompress) one might employ the decoder $\phi_2(x,z)=(x,x,z)$. The autoencoder $(\Omega_{\X},2,\theta_2,\phi_2)$ offers lossless compression on the sample space of interest as $(\phi_2\circ\theta_2)(\x)=\x$ for all $\x\in\Omega_{\X}$.
    
    \hspace{15pt} If we instead desire latent representations of $\x=(a,a,b)\in\Omega_{\X}$ in $\mathbb{R}$, i.e. if $d=1$, one might employ the encoder $\theta_1(x,y,z)=x$ and the decoder $\phi_1(x)=(x,x,x)$. The autoencoder $(\Omega_{\X},1,\theta_1,\phi_1)$ offers lossy compression, i.e. some information pertaining to a sample is lost in compressing and then decompressing it as $(\phi_1\circ\theta_1)(a,a,b)=(a,a,a)$ for all $a,b\in\mathbb{R}$.
\end{tcolorbox}

One's tolerance for the loss incurred by a given encoder/decoder pair is task-dependent. Given a dataset $\mathcal{D}=\{\x_1,\dots,\x_M\}\subset\Omega_{\X}$ and a latent dimension $d$, one might compute the pair $(\theta^*,\phi^*)\in\Theta\times\Phi$ which minimises the empirical risk over $\mathcal{D}$ where $\Theta$ and $\Phi$ are chosen function classes. That is, computing
$$
(\theta^*,\phi^*)
=
\argmin_{(\theta,\phi)\in\Theta\times\Phi}
\sum_{i=1}^M||\x_i-\phi(\theta(\x_i))||^2.
$$
If the function classes permit for computing the gradients of $\theta\in\Theta$ and $\phi\in\Phi$ then this computation may be done using gradient descent. For example, one could take $\Theta$ and $\Phi$ to be the classes of MLPs of appropriate input and output dimensions, as illustrated in \autoref{fig:autoencoder}.

\subsubsection{Motivating VAEs}
Autoencoders are great for data compression/decompression, denoising, interpreting complex models, etc. but we haven't seen yet how we might make use of them for generative tasks, i.e. sampling from the nasty/complex underlying distribution of the data they are trained on. An intuitive first idea is to train a compression/decompression autoencoder, randomly generate points in its latent space and feed them through the decoder, as in \autoref{fig:autoencoder_generation_1}. The outputs of the decoder should be similar to the training data, right? No.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.60\columnwidth]{./figures/generative_models/AE_gen_1.png}
    \caption{The output of the decoder of an autoencoder with randomly generated latent points as input. Gibberish output. \TODO{: Make own figure.}}
    \label{fig:autoencoder_generation_1}
\end{figure}

\noindent Disaster strikes and we begin to see just how unstructured the latent space of an autoencoder is: randomly sampling from it (whatever this may mean) results in nonsense outputs from the decoder because most of the latent space itself is meaningless (nothing is encoded to most of it, so in some sense a lot of it is `un-utilised'). This is perhaps unsurprising as at no point during training an autoencoder do we impose any sort of restriction on the structure of its latent space.

New idea: how about we feed the decoder latent points that are pretty close to the embedding of a given training sample, as in \autoref{fig:autoencoder_generation_2}? This should result in decoder outputs that are similar in structure to the input sample but a bit different, right? No.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\columnwidth]{./figures/generative_models/AE_gen_2.png}
    \caption{The output of the decoder of an autoencoder points relatively close to the latent embedding of a given training sample. \TODO{: Make own figure.}}
    \label{fig:autoencoder_generation_2}
\end{figure}

\noindent Distaster strikes again. To get decoder outputs that are meaningful using this idea, one must take points in the latent space which are ridiculously close to this chosen sample's latent embedding. So close that you'd be practically reconstructing the original training sample each time — certainly not what we mean when we say that we'd like to generate samples. So how do we do autoencoder-like things in a way that yields well-structured latent spaces? Variational autoencoders (VAEs)!

\subsubsection{Formulating VAEs}
Leaving aside, for now, how to learn one from data, VAEs can informally be seen as an extension of autoencoders in which the encoder and decoder each output parameters of a distribution belonging to some pre-chosen distribution family. Extending the notation used to define autoencoders, a variational autoencoder $(\Omega_{\X},d,\mathcal{Q}_d,\mathcal{P}_n,\theta,\phi)$ consists of the sample space of the distribution of interest $\Omega_{\X}\subset\mathbb{R}^n$, a latent dimension $d\in\mathbb{Z}_{\geq1}$, the parameter space $\mathcal{Q}_d$ of a family of $d-$dimensional conditional distributions denoted $q_{\theta}(\z|\x)$, the parameter space $\mathcal{P}_n$ of a family of $n-$dimensional conditional distributions denoted $p_{\phi}(\x|\z)$, an encoder $\theta:\mathbb{R}^n\to\mathcal{Q}_d$ and a decoder $\phi:\mathbb{R}^d\to\mathcal{P}_n$.

To illustrate the intended meaning of the newly-introduced parameter spaces $\mathcal{Q}_d$ and $\mathcal{P}_n$, one's encoder might yield the expectation vector and covariance matrix of a $d-$dimensional Gaussian $\mathcal{N}(\mu,\Sigma)$, i.e. $\mathcal{Q}_d$ could be the parameter space of the family of $d-$dimensional Gaussian distributions yielding
$$
\mathcal{Q}_d
=
\{
(\mu,\Sigma):\mu\in\mathbb{R}^d, \Sigma\in\mathcal{S}_{++}^d
\}
=
\mathbb{R}^d\times\mathcal{S}_{++}^d
$$
where $\mathcal{S}_{++}^d$ denotes the set of all positive-definite matrices in $\mathbb{R}^{d\times d}$.

As the encoder of a VAE yields a $d-$dimensional distribution $q_{\theta}(\z|\x)$ given the sample $\x\in\Omega_{\X}$, to obtain a latent $d-$dimensional representation $\z'\in\mathbb{R}^d$ of $\x$, one computes the parameters $\theta(\x)\in\mathcal{Q}_d$ of $q_{\theta}(\z|\x)$, e.g. a $d-$dimensional Gaussian, via the encoder and samples $\z'\sim q_{\theta}(\z|\x)$. To reconstruct $\x$ from its latent representation $\z'$, one computes the parameters $\phi(\z')\in\mathcal{P}_n$ of the $n-$dimensional reconstruction distribution $p_{\phi}(\x|\z')$ via the decoder. Sampling $\x'\sim p_{\phi}(\x|\z')$ yields (ideally) a sufficiently-accurate reconstruction of the original sample $\x$. Achieving accurate reconstructions is done in a similar manner to autoencoders: by including a penalty term pertaining to reconstruction quality in the loss function used to learn the encoder and decoder.

Note that one's choice of $\mathcal{Q}_d$ and $\mathcal{P}_n$ should take into account the need to sample efficiently and so they should correspond to families of distributions which offer efficient means of sampling, e.g. Gaussians, as in \autoref{fig:VAE_architecture}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.46\linewidth]{figures/generative_models/VAE_encoder.pdf}
    \hspace{20pt}
    \includegraphics[width=0.46\linewidth]{figures/generative_models/VAE_decoder.pdf}
    \caption{Left: an encoder which outputs parameters of the latent distribution $q_{\theta}(\z|\x)=\mathcal{N}(\mu_{\theta},\Sigma_{\theta})$. Right: a decoder which outputs parameters of the reconstruction distribution $p_{\phi}(\x|\z')=\mathcal{N}(\mu_{\phi},\Sigma_{\phi})$ in which $\z'\sim q_{\theta}(\z|\x)$.}
    \label{fig:VAE_architecture}
\end{figure}

\begin{tcolorbox}[title={\centering\textbf{VAE example}}, colback=myLightBlue, colbacktitle=myDarkBlue, colframe=myDarkBlue, coltitle=white]
    Suppose $X_1\sim\mathcal{N}(1,2)$ and $X_3\sim\mathcal{N}(-1,1)$ are independent. Let $X_2=X_1+\epsilon$ where $\epsilon\sim\mathcal{N}(0,1)$ is independent of $X_1$ and $X_3$. If $X=(X_1,X_2,X_3)$ then $X\sim\mathcal{N}(\mu,\Sigma)$ with $\mu=(1,1,-1)'$ and
    $$
    \Sigma
    =
    \begin{bmatrix}
        2 & 2 & 0\\
        2 & 3 & 0\\
        0 & 0 & 1
    \end{bmatrix}.
    $$
    As such, $\Omega_{\X}=\mathbb{R}^3$. To obtain two-dimensional representations of samples $\x\in\Omega_{\X}$ (so $d=2$), one might choose $\mathcal{Q}_2=\mathbb{R}^2\times\mathcal{S}_{++}^2$ and $\mathcal{P}_3=\mathbb{R}^3\times\mathcal{S}_{++}^3$. That is, we could fit a two-dimensional Gaussian to the latent space and a three-dimensional Gaussian to the reconstruction space. Knowing $X_2=X_1+\epsilon$, where $\epsilon\sim\mathcal{N}(0,1)$, one might choose the encoder
    \begin{align*}
        \theta&:
        \Omega_{\X}\to\mathcal{Q}_2\\
        &(x_1,x_2,x_3)\mapsto((x_1,x_3),\sigma^2I_2)
    \end{align*}
    where $\sigma>0$ is small. That is, for a sample $(x_1,x_2,x_3)\in\Omega_{\X}$, the encoder's output would yield the latent distribution $q_{\theta}(\z|\x)=\mathcal{N}((x_1,x_3),\sigma^2I_2)$. As decoder, if all we desire is accurate reconstructions, we might choose
    \begin{align*}
        \phi&:
        \mathbb{R}^2\to\mathcal{P}_3\\
        &(z_1,z_2)\mapsto((z_1,z_1,z_2), \sigma^2I_3).
    \end{align*}
    That is, for the latent sample $\z'=(z_1',z_2')$, the decoder's output would yield the reconstruction distribution $p_{\phi}(\x|\z')=\mathcal{N}((z_1',z_1',z_2'),\sigma^2I_3)$. Ideally, sampling $(x_1',x_2',x_3')\sim p_{\phi}(\x|\z')$ would yield a sample sufficiently similar to the original sample $\x=(x_1,x_2,x_3)$.

    \hspace{15pt} Note that in practice, one does not know the true distribution $p(\x)$ and so hand-picking the encoder and decoder as in this example is infeasible. Typically, the encoder and decoder are learned from a dataset $\mathcal{D}\subset\Omega_{\X}$.
\end{tcolorbox}

At this point, a natural question arises: for which tasks is learning a VAE more appropriate than learning an autoencoder? The answer lies in the purpose of VAEs which is two-fold: 1) to perform sufficiently-accurate compression/decompression and 2) to produce a sufficiently regularised approximation of the latent space $\Omega_{\Z}$. The latter is ensured by how one learns a VAE from data, which we soon consider. In brief, when learning an autoencoder one never imposes restrictions on the latent space beyond encouraging the model to yield sufficiently-accurate reconstructions. As a result, the latent space of an autoencoder is not well-structured. For example, for latent samples $\z_1,\z_2\in\Omega_{\Z}$ which are `close' in the latent space, their reconstructions are not necessarily `close' in $\mathbb{R}^n$. VAEs seek to remedy this.

To learn a VAE from data, we look to maximise the evidence lower bound (ELBO) over some dataset $\mathcal{D}=\{\x_1,\dots,\x_M\}\subset\Omega_{\X}$. Over a single sample $\x\in\Omega_{\X}$, the ELBO is a tight lower bound of $\log(p(\x))$. As such, maximising the ELBO over $\mathcal{D}$ can be seen as performing approximate maximum-likelihood estimation over $\mathcal{D}$ (sometimes referred to as evidence maximisation). To derive the ELBO, first note that given a decoder $\phi$ (which parameterises the reconstruction distribution $p_{\phi}(\x|\z)$), we may express the marginal distribution $p(\x)$ using $p_{\phi}(\x|\z)$ as
\begin{equation}
\label{eq:marginal_likelihood}
p(\x)
=
\int p(\x,\z)d\z
=
\int p(\x|\z)p(\z)d\z
=
\int p_{\phi}(\x|\z)p(\z)d\z.
\end{equation}
Using \autoref{eq:marginal_likelihood}, along with the encoder $\theta$ (which parameterises the latent distribution $q_{\theta}(\z|\x)$) we derive the ELBO over a single sample $\x\in\Omega_{\X}$:
\begin{align*}
    \log(p(\x))
    &=
    \log\l(\int p_{\phi}(\x|\z)p(\z)d\z\r)\\
    &=
    \log\l(\int q_{\theta}(\z|\x)\frac{p_{\phi}(\x|\z)p(\z)}{q_{\theta}(\z|\x)}d\z\r)\\
    &=
    \log\l(\E_{\Z\sim q_{\theta}(\z|\x)}\l[\frac{p_{\phi}(\x|\Z)p(\Z)}{q_{\theta}(\Z|\x)}\r]\r)\\
    &\geq
    \E_{\Z\sim q_{\theta}(\z|\x)}\l[\log\l(\frac{p_{\phi}(\x|\Z)p(\Z)}{q_{\theta}(\Z|\x)}\r)\r]\\
    &=
    \E_{\Z\sim q_{\theta}(\z|\x)}\l[\log(p_{\phi}(\x|\Z))\r]-\E_{\Z\sim q_{\theta}(\z|\x)}\l[\log\l(\frac{q_{\theta}(\Z|\x)}{p(\Z)}\r)\r]\\
    &=
    \E_{\Z\sim q_{\theta}(\z|\x)}\l[\log(p_{\phi}(\x|\Z))\r]-D_{\text{KL}}(q_{\theta}(\Z|\x)||p(\Z))\\
    &=:
    \text{ELBO}
\end{align*}
in which the inequality arises due to Jensen's inequality, as in $\E[\log(f(\Z))]\leq\log(\E[f(\Z)])$, and $D_\text{KL}(Q||P)$ denotes the KL-divergence between distributions $Q$ and $P$, which is detailed in \autoref{app:entropy}. The effect of maximising
$$
\text{ELBO}
=
\E_{\Z\sim q_{\theta}(\z|\x)}\l[\log(p_{\phi}(\x|\Z))\r]-D_{\text{KL}}(q_{\theta}(\Z|\x)||p(\Z))
$$
over a dataset is often explained term-by-term. The first term is a principled measure of the VAE's ability to reconstruct latent representations, as with autoencoders. The second term is a principled measure of the similarity of the latent distribution $q_{\theta}(\z|\x)$ and the prior $p(\z)$. The prior is chosen before training and the most common choice is a $d-$dimensional standard Gaussian, i.e. $p(\z)=\mathcal{N}(0,I_d)$. As such, minimising the negative KL-divergence between the latent distribution and said prior is often interpreted as encouraging the learning of the encoder such that latent representations are distributed according to the prior. This is particularly useful in the case that one is learning a VAE to generate new samples from $\Omega_{\X}$: post-training, sample $\z'\sim p(\z)$, compute the parameters $\phi(\z')$ of the reconstruction distribution $p_{\phi}(\x|\z')$ via the decoder and sample from it. Note that using a VAE for generative purposes does not invoke the use of the encoder, only the decoder is required post-training.

Given a dataset $\mathcal{D}=\{\x_1,\dots,\x_M\}\subset\Omega_{\X}$, we learn a VAE by choosing function classes $\Theta$ and $\Phi$ (e.g. MLPs as in \autoref{fig:VAE_architecture}) and computing
$$
\argmax_{(\theta,\phi)\in\Theta\times\Phi}
\l[
\sum_{i=1}^M
\E_{\Z\sim q_{\theta}(\z|\x_i)}\l[\log(p_{\phi}(\x_i|\Z))\r]-D_{\text{KL}}(q_{\theta}(\Z|\x_i)||p(\Z))
\r].
$$
In practice, after choosing a latent dimension $d$, we often take $p(\z)=\mathcal{N}(0,I_d)$, $\mathcal{Q}_d=\mathbb{R}^d\times\mathcal{S}_{++}^d$ and $\mathcal{P}_n=\mathbb{R}^n\times\mathcal{S}_{++}^n$, i.e. Gaussians for the latent and reconstruction distribution families and the standard $d-$dimensional Gaussian for the prior. A benefit of these choices is that it yields a differentiable and easy-to-implement closed form for the KL-divergence term in the ELBO. Additionally, the expectation pertaining to the reconstruction term is typically approximated via a single sample, boiling down to a mean square error term\footnote{\url{https://n8python.github.io/mnistLatentSpace/}}.

\subsubsection{Backpropagation for VAEs}
\TODO{} The expectation term in the ELBO requires us to sample from $q_{\theta}(\z|\x)$. Back propping through sampling is not a thing, it ain't differentiable. We need the reparameterisation trick!

% general adversarial networks
\subsection{\TODO{: }Generative Adversarial Networks (GANs)}

% normalising flow models
\subsection{\TODO{: }Normalising Flows}

% diffusion models
\subsection{Diffusion Models}
The purpose of diffusion models is to facilitate the generation of samples from complex distributions from which sampling is typically intractable. While this overarching motive is not unique to diffusion models, how a diffusion model learns and how it generates new samples is quite distinct from other well-known generative models like VAEs and GANs (though some ideas are certainly comparable). A diffusion model can be described by its forward (noising) and backward (denoising) processes. Its forward process iteratively noises a sample $\x_0\in\mathcal{X}$, belonging to the distribution of interest, $T$-many times to obtain its noised equivalent $\x_T$. Formally, this is done via the Markov process
$$
q(\x_t|\x_{t-1})
=
\mathcal{N}\l(\x_t\Big|\sqrt{1-\beta_t}\x_{t-1},\beta_tI\r)
$$
where $\beta_1,\dots,\beta_T\in[0,1]$ are hyperparameters satisfying $\beta_i<\beta_{i+1}$, often referred to as the noise schedule of the model. With an appropriately chosen final time $T$, these noised equivalents $\x_T$ are akin to random noise sampled from $\mathcal{N}(0,I)$.

\begin{figure}[t]
    \centering
    \includegraphics[height=3cm]{./figures/generative_models/diffusion_process.pdf}
    \caption{Johann Bernoulli being denoised in line with the backward process of a diffusion model. \TODO{: Improve figure.}}
    \label{fig:diffusion_process}
\end{figure}

Letting $\alpha_t=1-\beta_t$ and $\Bar{\alpha}_t=\prod_{i=1}^t\alpha_i$, we can describe the backward process of a diffusion model again as a Markov process in which we sample some random noise $\x_T$ from $\mathcal{N}(0,I)$ and obtain the $(t-1){\text{st}}$ denoised sample from $p_{\theta}(\x_{t-1}|\x_t)$ via
$$
\x_{t-1}=\frac{1}{\sqrt{\alpha_t}}\l(\x_t-\frac{1-\alpha_t}{\sqrt
{1-\bar{\alpha}_t}}\epsilon_{\theta}(\x_t,t)\r)+\sigma_t\z
$$
where $\sigma_1,\dots,\sigma_T$ are hyperparameters, $\epsilon_{\theta}$ is the diffusion model's denoiser and $\z$ is sampled from $\mathcal{N}(0,I)$. % For clarity, $p_{\theta}(\x_{t-1}|\x_t)=\mathcal{N}(\x_{t-1}|\mu_{\theta}(\x_t,t),\sigma_tI)$ where $\mu_{\theta}(\x_t,t)$ is determined by the decoder $\epsilon_{\theta}$, the current noised image $\x_t$ and the time step $t$.
The purpose of the denoiser $\epsilon_{\theta}$, where $\theta$ is its tuple of parameters, is akin to its name: it is used to iteratively turn sampled noise $\x_T\in\mathcal{N}(0,I)$ into something resembling a sample $\x_0\in\mathcal{X}$ from the distribution of interest. If the architecture of its denoiser can be backpropagated through then training a diffusion model can be done in the usual manner of choosing an appropriate loss function and performing gradient descent in which gradients are computed via backpropagation through the entire model. We leave further details of training a diffusion model out for the sake of brevity but these can easily be found in literature.

So, what is an appropriate choice for the architecture of a diffusion model's denoiser? Before considering transformers for this task, we consider a more often-used choice, U-Net: a class of convolutional neural networks (CNNs).

\subsubsection{U-Net denoisers}
Despite not being the choice of denoiser made in the seminal paper introducing diffusion models, U-Net became the go to choice of denoiser architecture for contemporary diffusion models. To understand U-Net's popularity in this regard, it is worth understanding its architecture which consists of a contracting branch, terminating at its bottleneck, followed by an expansion branch, illustrated in \autoref{fig:u_net_arch}. This architecture can be thought of as forming a `U'-like shape, with the bottleneck lying at the bottom, hence its name.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{./figures/generative_models/unet.png}
    \caption{Example U-Net architecture. \TODO{: Make own figure.}}
    \label{fig:u_net_arch}
\end{figure}

In the context of denoising, the contraction branch takes a noised image as input and iteratively completes a series of convolution operations followed by max pooling until reaching the bottleneck layer. At the bottleneck, the model has heavily reduced the spatial dimension of the input image while extracting varying levels of feature abstractions. For example, after the first iteration of convolution and max pooling, the spatial dimension of the input image may be reduced from $1024\times1024$ to $512\times512$ but edges and textures within the original image may be encoded in the feature maps at this stage. Then, during expansion, the model looks to upscale from the bottleneck in a way that retains the underlying image while removing noise. This is done by iteratively completing a series of deconvolutions, which correspond to upscaling, and using skip connections from its corresponding component in the contraction branch, seen in \autoref{fig:u_net_arch}, in order to retain the underlying image. With this in mind, it is clear why U-Net has been the go to choice of denoiser when developing a diffusion model.

% model evaluation
\subsection{\TODO{: }Evaluating Generative Models}
Since samples are not labeled, how does one evaluate a generative model? What does under/overfitting mean for generative models?

% object detection
\section{Object Detection Models}

Object detection is the task of localisation and classification within an image or video. Before the era of deep learning, it was typically solved using Haar-like filters\dots As of August 2025, there are three flavours of object detection models: R-CNN, YOLO and DETR.

% R-CNN
\subsection{\TODO{: }(Fast/Faster) R-CNN}
R-CNN models has evolved significantly since its proposal in 2013 with three evolutions worthy of a new name: R-CNN (the original), Fast R-CNN and Faster R-CNN. R-CNN stands for \textit{Regions with CNN features}. Roughly put, R-CNN model consist of two steps:
\begin{enumerate}
    \item Proposing regions of interest within the image
    \item Classifying what is in each region (if there is something in the region)
\end{enumerate}

% YOLO
\subsection{\TODO{: }YOLO}

% DETR
\subsection{\TODO{: }DETR}

% % future sections
% \section{To-be-included sections, one day}
% I'll get around to including these sections one day.

% % unsupervised learning
% \subsection{Unsupervised Learning}
% ain't no labels available to do supervised learning

% \subsubsection{Principal Component Analysis}
% bunch of linear algebra

% \subsubsection{$k$-means Clustering}
% what i think of when i think of clustering, easy example of latent variable

% \subsubsection{Autoencoders}
% already written in the variational autoencoder subsection

% % self-supervised learning
% \subsection{Self-supervised Learning}
% some labelled some ain't

% % nlp
% \subsection{Natural Language Processing (NLP)}
% blelele distributional semantics
% \begin{quote}
%     ``You shall know a word by the company it keeps." - some linguist
% \end{quote}

% appendices
\newpage
\begin{appendices}
\addtocontents{toc}{\protect\setcounter{tocdepth}{0}}

% probability things
\section{Probability Theory and Statistics Things}
Here are some probability theory and statistics-related things, e.g. brief statements, long derivations, and so on, which belong in an appendix. I wish I knew more probability/measure theory and classical statistics. In another life.

\subsection{Sample Independence}
Suppose we'd like to get an idea of the mean population height of men and women. If we randomly sample 100 men and 100 women then we're good to go. This could be done by independently sampling, without replacement, from the categorical corresponding to the ID numbers of individuals. If we instead randomly sample 100 couples then our samples would not be independent since couples' heights correlate.

Sample independence is massively important, e.g. for maximum likelihood estimation. Independence is also assumed in a ton of other statistics-related things like t-tests, ANOVA, etc.

\subsection{Derivations Related to Common Distributions}

\subsubsection*{1) From Bernoulli to Binomial}
Let's try to extend the Bernoulli distribution to the binomial distribution. If $X\sim\text{Ber}(p)$ then $\Omega_X=\{0,1\}$ and $\mathbb{P}(X=x)=p^x(1-p)^{1-x}$, so $\mathbb{P}(X=1)=p$ and $\mathbb{P}(X=0)=1-p$.

Taking $n$ independent Bernoulli random variables and summing them yields the random variable $\X=\sum_{i=1}^nX_i\sim\text{Bin}(n,p)$ with $\Omega_{\X}=\{0,\dots,n\}$. Let $k$ denote a realisation of $\X$, i.e. $k=x_1+\dots+x_n$. The corresponding mass function is given by
\begin{align*}
    \mathbb{P}(\X=k)
    &=
    Z\cdot\prod_{i=1}^n\mathbb{P}(X_i=x_i)\\
    &=
    Z\cdot\prod_{i=1}^np^{x_i}(1-p)^{1-x_i}\\
    &=
    Z\cdot p^{x_1+\dots+x_n}(1-p)^{n-(x_1+\dots+x_n)}\\
    &=
    Z\cdot p^k(1-p)^{n-k}
\end{align*}
where $Z$ is a normalising constant such that $\sum_{k=0}^nZp^k(1-p)^{n-k}=1$. It's more intuitively seen as the number of ways of placing $k$-many $1$s among a series of $n\geq k$ bits, i.e. $Z=\binom{n}{k}$, so
$$
\mathbb{P}(\X=k)
=
\binom{n}{k}p^k(1-p)^{n-k}.
$$

\subsubsection*{2) From Categorical to Multinomial}
The categorical distribution is a generalisation of the Bernoulli distribution and the multinomial distribution is a generalisation of the binomial distribution. In line with this, we'd hope to be able to extend the categorical distribution to the multinomial distribution.

If $X\sim\text{Cat}(p_1,\dots,p_C)$ then realisations of $X$ can be represented by single integers in $\{1,\dots,C\}$ but I prefer to represent them in terms of their $C-$long one-hot encodings. So I denote the presence of the $c^{\text{th}}$ class in a realisation as the unit row vector $\mathbf{e}_c^\top $, e.g. $(0,1,0\dots,0)$ denotes a sample in which only the second class is present. The corresponding mass function is then given by
$$
\mathbb{P}(X=(x_1,\dots,x_C))
=
\prod_{j=1}^Cp_j^{x_j}.
$$
Note that, in this notation, all but one of these $x_j$ terms are zero, so it really boils down to just a single probability value, e.g. $\mathbb{P}(X=(0,1,\dots,0))=p_2$. The mass function can also be written using indicator functions but the form offered above is easiest to understand for me.

Applying the same idea used to extend Bernoulli to binomial, consider the random variable $\X=\sum_{i=1}^nX_i$ pertaining to $n$ independent categorical trials. Each realisation of $X_1,\dots,X_n$ can be represented by a $C-$long one-hot encoded vector and so $n$ realisations of $X\sim\text{Cat}(p)$ can be thought of as a matrix $\x\in\{0,1\}^{n\times C}$ whose rows are unit vectors in $\R^C$. As such, letting $k_1,\dots,k_C$ denote the number of $1$s in the columns of $\x$ (so $k_1+\dots+k_C=n$) we see immediately that $(k_1,\dots,k_C)$ is a realisation of $\X$. So what is the corresponding mass function? Let $x_{ij}$ denote the element in the $i^{\text{th}}$ row and $j^{\text{th}}$ column of $\x$ and let $k_j=x_{1j}+\dots+x_{nj}$ denote the sum of all elements in the $j^{\text{th}}$ column of $\x$. Note that $k_j$ is the number of realisations of $X_1,\dots,X_n$ in which the $j^{\text{th}}$ class is present. We have
\begin{align*}
    \mathbb{P}(\X=(k_1,\dots,k_C))
    &=
    Z\cdot\prod_{i=1}^n\mathbb{P}(X_i=(x_{i1},\dots,x_{iC}))\\
    &=
    Z\cdot\prod_{i=1}^n\prod_{j=1}^Cp_j^{x_{ij}}\\
    &=
    Z\cdot\prod_{j=1}^Cp_j^{x_{1j}+\dots+x_{nj}}\\
    &=
    Z\cdot\prod_{j=1}^Cp_j^{k_j}\\
    &=
    Z\cdot p_1^{k_1}\cdots p_C^{k_C}.
\end{align*}
So all that's left to do is derive the normalisation constant $Z$. Note that the realisation $\x$ has $n$ rows of which there are $n!$-many orderings. Additionally note that for each $j\in\{1,\dots,C\}$ we know that there are $k_j$-many $1$s in column $j$ so $k_1$ of these $n$ rows must pertain to the first class for which there are $\binom{n}{k_1}$-many orderings. From here, see that $k_2$ of the $n-k_1$ remaining rows must pertain to the second class for which there are $\binom{n-k_1}{k_2}$-many orderings. Continuing this line of reasoning, we obtain
\begin{align*}
    Z
    &=
    \prod_{i=1}^C\binom{n-\sum_{j=1}^{i-1}k_j}{k_i}\\
    &=
    \binom{n}{k_1}\binom{n-k_1}{k_2}\binom{n-(k_1+k_2)}{k_3}\cdots\binom{n-(k_1+\dots+k_{C-1})}{k_C}\\
    &=
    \frac{n!}{k_1!}\cdot\frac{(n-k_1)!}{k_2!(n-(k_1+k_2))!}\cdot\frac{(n-(k_1+k_2))!}{k_3!(n-(k_1+k_2+k_3))!}\cdots\frac{k_C!}{k_C!0!}\\
    &=
    \frac{n!}{k_1!\cdots k_C!}
\end{align*}
from which we obtain
$$
\mathbb{P}(\X=(k_1,\dots,k_C))
=
\frac{n!}{k_1!\cdots k_C!}p_1^{k_1}\cdots p_C^{k_C}.
$$

\subsubsection*{3) Negative Binomial and Geometric}
We keep flipping our (maybe biased) coin until we observe $r$ successes ($r$ is a parameter) where individual trials are Ber$(p)$ distribured. So if $X\sim\text{NB}(r,p)$ then
$$
\mathbb{P}(X=k)
=
Z\cdot p^r(1-p)^k.
$$
A trial must be of the form $(x_1,\dots,x_{k+r-1},1)$ which means that $r-1$ of the first $k+r-1$ elements must be a success of which there are $\binom{k+r-1}{r-1}$ possibilities, thus
$$
\mathbb{P}(X=k)
=
\binom{k+r-1}{r-1}p^r(1-p)^k.
$$
The case $r=1$ yields the shifted geometric distribution, whose pmf is given by
$$
\mathbb{P}(X=k)
=
p(1-p)^k.
$$

\subsubsection*{4) From Binomial to Poisson}
Let $X_n\sim\text{Bin}(n,p)$ and $\lambda=np$. How might $X_n$ look as $n\to\infty$? Why might we be interested in this at all? It can be thought of as \dots

To see what $X_n$ converges to in distribution, note that for all $x\in\{0,\dots,n\}$
\begin{align*}
    p(X_n=x)
    &=
    \binom{n}{x}\l(\frac{\lambda}{n}\r)^x\l(1-\frac{\lambda}{n}\r)^{n-x}\\
    &\approx
    \frac{n!}{x!(n-x)!}\frac{\lambda^x}{n^x}e^{-\lambda}e^{\lambda x/n}\\
    &=
    e^{\lambda x/n}\frac{n(n-1)\cdots(n-x+1)}{n\cdots n}e^{-\lambda}\frac{\lambda^x}{x!}\\
    & \xrightarrow{n\to\infty}
    e^{-\lambda}\frac{\lambda^x}{x!}.
\end{align*}
Showing this statement more rigorously might make use of Stirling's approximation of $n!$. Since $X_n$ is discrete for all $n\in\mathbb{N}$ we conclude that
$$
X_n
\xrightarrow[d]{n\to\infty}
\text{Poi}(\lambda).
$$

\subsubsection*{5) From Poisson to Exponential}
When working with a Poisson process, a natural question is the distribution of time between the occurence of events. Let $T$ denote the time at which the first event occurs, so $\Omega_T=[0,\infty)$. See that if $N(t)\sim\text{Poi}(\lambda t)$ is a Poisson process then $p(N(t)=x)$ is the probability of $x$ events occuring in $[0,t]$. We have
\begin{align*}
    p(T<t)
    &=
    \sum_{x=1}^{\infty}p(N(t)=x)\\
    &=
    1-p(N(t)=0)\\
    &=
    1-e^{-\lambda t}
\end{align*}
and so
$p(t)=\lambda e^{-\lambda t}$.

\subsection{The Law of Large Numbers}
If $\X_1,\dots,\X_n$ are i.i.d. and $\E[\X]<\infty$ then
$$
\frac{1}{n}\sum_{i=1}^n\X_i
\xrightarrow{n\to\infty}
\E[\X]
$$
with respect to some type of convergence. The law comes in a weak flavour and a stronger one. The weak flavour pertains to convergence in probability and the latter to asmymptotic convergence.

Frankly, the details aren't important for my purposes, so I simply think of it as meaning ``The sample mean of a buncha samples is the population mean.''. It's useful for justifying certain approaches, e.g. Monte Carlo integration.

\subsection{The (univariate) Central Limit Theorem (CLT)}
If $X_1,\dots,X_n$ are i.i.d. and $\text{Var}(X)<\infty$ then
$$
\sqrt{n}\l(\frac{1}{n}\sum_{i=1}^nX_i-\E[X]\r)
\xrightarrow[d]{n\to\infty}
\mathcal{N}(0,\text{Var}(X)).
$$
It's one of the most important results in probability theory and statistics: it helps explain why the normal distribution appears so often in real world data. I think of it as meaning ``The scaled sum of i.i.d. samples will look normal upon repeatedly sampling.".

Its clearest use-case to me is in justifying the modelling of residuals in linear regression as normally distributed.

\subsection{Jensen's Inequality}
If $f:\mathbb{R}\to\mathbb{R}$ is convex and $\E[\X]<\infty$ then
$$
f(\E[\X])\leq\E[f(\X)].
$$
If $f$ is concave then
$$
f(\E[\X])\geq\E[f(\X)].
$$
I've never come across a particularly inciteful proof of it, so I won't include one.

\subsection{Motivating Variance and the Bias of Sample Variance}
\TODO{}

\subsection{Entropy and its Friend KL-divergence}
\label{app:entropy}

The entropy of a distribution can be motivated by the notion of the surprise of (or information learned from) observing realisations of said random variable. Given a discrete random variable $\X$, an event $\x\in\Omega_{\X}$ and a surprise function $S:\Omega_{\X}\rightarrow[0,\infty)$, the surprise of observing $\x$ is $S(\x)$. Before we continue, we need to understand what we want out of our surprise function. Following the use of `surprising' in day-to-day communication, we want events with low probability to be highly surprising and events with high probability to be less surprising, with some extra conditions. So if $p(\x)=0.01$ then we want $S(\x)$ to be close to relatively high (strictly speaking it doesn't need to be bounded above) and if $p(\x)=0.99$ then we want $S(\x)$ to be close to 0.

An easy way to achieve this is to take $S(\x)=-\log(p(\x))$ where $\log$ denotes the natural logarithm unless stated otherwise. Quickly see that $\log(0.01)=4.61$ and $\log(0.99)=0.01$. From here, we define the entropy of a random variable as its expected surprise
$$
H(p)
=
\E[-\log(p(\X))]
=
-\sum_{\x\in \Omega_{\X}}p(\x)\log(p(\x)).
$$
More precisely, Claude Shannon wanted such a surprise function to satisfy three intuitive properties. Firstly, the surprise of an event with probability $1$ should be $0$. Secondly, the surprise of two independent events occurring should be the sum of the surprises of the events individually. Thirdly, the surprise of a given event should be higher than the surprise of any less probable event. So, for all $\x_1,\x_2\in\Omega_{\X}$, we desire
\begin{enumerate}
    \item $p(\x)=1\implies S(\x)=0$
    \item $p(\x_1,\x_2)=p(\x_1)\cdot p(\x_2)\implies S(\x_1,\x_2)=S(\x_1)+S(\x_2)$
    \item $p(\x_1)>p(\x_2)\implies S(\x_1)<S(\x_2)$
\end{enumerate}
It's straightforward to see that $S(\x)=-\log(p(\x))$ satisfies these three properties but it turns out that it is unique in satisfying these properties, up to its base.

In literature, entropies are measured in nats (natural unit of information) if the natural logarithm is used for their computation and bits if $\log_2$ is used.

\begin{tcolorbox}[title={\centering\textbf{Technical note regarding} $\text{dom}(S)$}, colback=myLightBlue, colbacktitle=myDarkBlue, colframe=myDarkBlue, coltitle=white]
    It's clear from the second condition that $S$ is actually a function whose domain is the powerset of $\Omega_{\X}$ but accommodating this detail isn't worth it — the idea being conveyed is clear without.
\end{tcolorbox}

\subsubsection{Kullback-Leibler Divergence (KL-divergence)}

Given probability density/mass functions $p$ and $q$ on the same space $\Omega_{\X}$, their Kullback-Leibler divergence (KL-divergence) is given by
$$
D_{\text{KL}}(p||q)
=
\E_{\X\sim p}\left[\log\left(\frac{p(\X)}{q(\X)}\right)\right].
$$
It is often used as a measure of similarity between two distributions: it follows from Gibbs' inequality that it is non-zero and it is often proposed as a loss function when assessing how well a model $q$ encodes an underlying distribution $p$. In many cases, a loss function is chosen whose minimisation is equivalent to minimising the relevant KL-divergence.

The KL-divergence of two distributions can be expressed in terms of a self-entropy and a cross-entropy as in
\begin{align*}
    D_{\text{KL}}(p||q)
    &=
    \E_{\X\sim p}\left[\log\left(\frac{p(\X)}{q(\X)}\right)\right]\\
    &=
    \E_{\X\sim p}\left[-\log(q(\X))\right]-\E_{\X\sim p}\left[-\log(p(\X))\right]\\
    &=
    H(p,q)-H(p)
\end{align*}
where $H(p,q)$ denotes the cross-entropy between $p$ and $q$ and $H(p)$ denotes the self-entropy of $p$. It follows that minimising the KL-divergence $D_{\text{KL}}(p,q)$ in $q$ corresponds to minimising the cross-entropy $H(p,q)$.

\subsubsection*{Example: fitting via cross-entropy/KL-divergence}

Suppose we have the geometric distribution with $p=1/2$ and would like to fit it using a Poisson distribution. That is, we would like to best approximate
\begin{align*}
    p:\mathbb{N}&\to[0,1]\\
    k&\mapsto2^{-(k+1)}
\end{align*}
by finding a nice $\lambda$ for
\begin{align*}
    q:\mathbb{N}&\to[0,1]\\
    k&\mapsto\frac{e^{-\lambda}\lambda^k}{k!}.
\end{align*}
To find a good $\lambda$, it makes sense to first come up with a metric for how good a given value is. For this, we can use the cross-entropy of the distributions $p$ and $q$ given by
$$
H(p,q)
=
-\E_{K\sim p}\l[\log(q(K))\r]
=
-\sum_{k=1}^{\infty}p(k)\log(q(k))
$$
which can be seen as the incurred cost of using the model $q$ in place of the true underlying model $p$. There are a bunch of different ways of describing/interpreting this quantity. The most interesting is perhaps the one related to encodings. Anyway, in our case, if we can find a closed form of this expression in terms of $\lambda$ then we can look to compute which value of $\lambda$ it's minimised by. In line with this, we compute
\begin{align*}
    H(p,q)
    &=
    -\E_{K\sim p}\l[\log(q(K))\r]\\
    &=
    -\sum_{k=1}^{\infty}p(k)\log(q(k))\\
    &=
    -\sum_{k=1}^{\infty}2^{-k}\l(-\lambda+k\log(\lambda)-\log(k!)\r)\\
    &=
    \lambda\sum_{k=1}^{\infty}\frac{1}{2^k}-\log(\lambda)\sum_{k=1}^{\infty}\frac{k}{2^k}+\sum_{k=1}^{\infty}\frac{\log(k!)}{2^k}\\
    &=
    \lambda-\log(\lambda)+C
\end{align*}
where $C=\sum_{k=1}^{\infty}\frac{\log(k!)}{2^k}$ is independent of $\lambda$. In computing the minmum of $H(p,q)$ in $\lambda$ we obtain
$$
\frac{\partial}{\partial \lambda}H(p,q)
=
1-\frac{1}{\lambda}
$$
which yields $\lambda=1$. So according cross-entropy, the best-fitting Poisson distribution to our geometric distribution is Poi$(1)$.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{./figures/appendix/poi_geo_plot.pdf}
    \caption{Our geometric distribution $p$ and Poisson fit $q$.}
    \label{fig:poi_geo_plot}
\end{figure}

For $\lambda=1$ we compute a cross-entropy of
$$
H(p,q;1)
=
1+\sum_{k=1}^{\infty}\frac{\log(k!)}{2^k}
\approx
2.01567.
$$
On its own, this quantity isn't too useful. To get a sense of goodness-of-fit we desire the KL-divergence, i.e. the cross-entropy minus the self-entropy of $p$. Let's compute said self-entropy:
\begin{align*}
    H(p)
    &=
    -\sum_{k=1}^{\infty}p(k)\log(p(k))\\
    &=
    -\sum_{k=1}^{\infty}2^{-k}\log(2^{-k})\\
    &=
    \log(2)\sum_{k=1}^{\infty}\frac{k}{2^k}\\
    &=
    2\log(2)\\
    &\approx
    1.386
\end{align*}
from which we know that the KL-divergence between the underlying geometric distribution $p$ and our Poisson fit $q$ is given by
$$
\text{KL}(p||q)
\approx
2.016-1.386
=
0.63.
$$

\subsection{Pearson Correlation and Mutual Information}
\label{app:pearson_correlation}
Let $\bar{X}=X-\E[X]$ and $\bar{Y}=Y-\E[Y]$ where $X$ and $Y$ are scalar. The Pearson correlation of $X$ and $Y$ is given by
$$
\rho_{X,Y}
=
\frac{\E\l[\bar{X}\bar{Y}\r]}{\sqrt{\E\l[\bar{X}^2\r]\E\l[\bar{Y}^2\r]}}
=
\frac{\text{Cov}(X,Y)}{\sigma_X\sigma_Y}
\in[-1,1]
$$
where the expectation in the numerator is over $p(x,y)$. The overall quantity yields the linear dependence between $X$ and $Y$ and the denominator is for normalisation.

Pearson correlation is a measure of linear dependence between random variables. What if we want to know about non-linear dependence between random variables, i.e. what if Pearson correlation doesn't suffice? Mutual information to the rescue. It's a property of a joint distribution but is often described as a statement of two random variables $X$ and $Y$ and is denoted as such as in
$$
I(X;Y)
=
D_{\text{KL}}(p_{X,Y}||p_X\otimes p_Y).
$$
I'm a big fan of writing it as a KL-divergence as its meaning is immediately clear: its a measure of divergence between the joint and the product of the marginals. Informally, I see it as asking ``How well does the product of distributions model the joint?'' which is precisely the question we seek to answer when utilising things like Pearson correlation and mutual information.

\subsection{Quality of fit $\approx$ Encoding quality?}
\TODO{: Perhaps worthy of its own section, unsure}

\subsection{The Expectation-Maximisation (EM) Algorithm}
Maximum likelihood estimation (MLE) is great but what do we do when the data generating process is not determined entirely by model variables $\X=(X_1,\dots,X_q)$, i.e. hidden variables $\Z=(Z_1,\dots,Z_{q'})$ are involved? That is, what if $p(\x)$ is simply the marginal of the true joint $p(\x,\z)$?

It turns out that there's a clever way of perfmorming MLE while accounting for hidden variables called the expectation-maximisation (EM) algorithm. It involves obtaining a lower bound for the log-likelihood of a given sample and instead maximising the lower bound in the model's parameters. If the conditions are met for said lower bound to in fact equal to the sample's log-likelihood then the application of EM is MLE itself. If the bound is not exact then we refer to the process as variational EM.

In obtaining a lower bound, let $\theta$ denote the parameters of the true joint $p_{\theta}(\x,\z)$ and see that if $q$ is a probability mass function with domain $\Omega_{\Z}$ then
\begin{align*}
    \log(p_{\theta}(\x))
    &=
    \log\l(\sum_{\z\in\Omega_{\Z}}p_{\theta}(\x,\z)\r)\\
    &=
    \log\l(\sum_{\z\in\Omega_{\Z}}q(\z)\frac{p_{\theta}(\x,\z)}{q(\z)}\r)\\
    &=
    \log\l(\E_{\Z\sim q}\l[\frac{p_{\theta}(\x,\Z)}{q(\Z)}\r]\r)\\
    &\geq
    \E_{\Z\sim q}\l[\log\l(\frac{p_{\theta}(\x,\Z)}{q(\Z)}\r)\r]\\
    &=
    \E_{\Z\sim q}\l[\log\l(p_{\theta}(\x,\Z)\r)\r]+H(q)\\
    &=:
    \text{ELBO}(q,\theta).
\end{align*}
Here, the latent variables are assumed to be discrete for convenient notation. Note that ELBO stands for evidence lower bound. You find similar terminally when learning about variational autoencoders (VAEs).

The EM algorithm performs coordinate ascent (component-wise equivalent of gradient ascent) on the ELBO: iteratively maximise $\text{ELBO}(q,\theta)$ in $q$ (with $\theta$ fixed) and then in $\theta$ (with $q$ fixed) until some convergence crtieria is met. With $q$ fixed, maximising the ELBO in $\theta$ amounts to computing
\begin{align*}
    \argmax_{\theta}\text{ELBO}(q,\theta)
    &=
    \argmax_{\theta}\E_{\Z\sim q}\l[\log\l(p_{\theta}(\x,\Z)\r)\r]\\
    &=
    \argmax_{\theta}\Big(\E_{\Z\sim q}\l[\log\l(p_{\theta}(\x|\Z)\r)\r]+\E_{\Z\sim q}\l[\log\l(p_{\theta}(\Z)\r)\r]\Big)\\
    &=
    \argmax_{\theta}Q(\theta;q)
\end{align*}
where $Q(\theta;q)=\E_{\Z\sim q}\l[\log\l(p_{\theta}(\x|\Z)\r)\r]+\E_{\Z\sim q}\l[\log\l(p_{\theta}(\Z)\r)\r]$. Note that equality is offered by the bound precisely when $q(\z)=p_{\theta}(\z|\x)$ and maximising the ELBO in $q$ with $\theta$ fixed amounts to computing
$$
\argmax_{q}\text{ELBO}(q,\theta)
=
p_{\theta}(\z|\x),
$$
i.e. computing the posterior.

With both statements in mind, the EM algorithm amounts to initialising the parameters $\theta^{(0)}$ and repeating the following two steps until some pre-determined stopping criteria is met:
\begin{enumerate}
    \item (E step: $\theta^{(t)}\to q_t$) Obtain a closed form for $q_t(\z)=p_{\theta^{(t)}}(\z|\x)$ and in turn a closed form for $Q(\theta;q_t)$
    \item (M step: $q_t\to\theta^{(t+1)}$) Compute $\theta^{(t+1)}=\argmax_{\theta}Q(\theta;q_t)$
\end{enumerate}

\begin{tcolorbox}[title={\centering\textbf{Variational EM}}, colback=myLightBlue, colbacktitle=myDarkBlue, colframe=myDarkBlue, coltitle=white]
    Computing the posterior exactly is often infeasible in practice. Varational EM seeks to alleviate this issue by instead restricting $q$ to a family of functions $\mathcal{F}$, e.g. some family of MLPs, and replacing the E-step with computing
    $$
    q_t
    =
    \argmax_{q\in\mathcal{F}}\text{ELBO}\l(q,\theta^{(t)}\r).
    $$
    If the posterior $p_{\theta^{(t)}}(\z|\x)$ belongs to $\mathcal{F}$ for all relevant $t\in\mathbb{N}$ then variational EM is simply EM.
\end{tcolorbox}
Regarding the correctness of the EM algorithm, we seek to show that
$$
\log(p_{\theta^{(t+1)}}(\x))
\geq
\log(p_{\theta^{(t)}}(\x))
$$
for all $t\in\mathbb{N}$, i.e. individual steps in the loss landscape are neglible at worst and otherwise increase the likelihood of the data. Showing this turns out to be elegant, as in
$$
\log(p_{\theta^{(t+1)}}(\x))
\geq
\text{ELBO}\l(q_t,\theta^{(t+1)}\r)
\geq
\text{ELBO}\l(q_t,\theta^{(t)}\r)
=
\log(p_{\theta^{(t)}}(\x)).
$$
The leftmost inequality is simply due to how the ELBO is defined. The second inequality is seen from
$$
\theta^{(t+1)}
=
\argmax_{\theta}\text{ELBO}\l(q_t,\theta\r).
$$
Finally, the equality follows from the `matching the true posterior with $q_t$' argument offered earlier. That is, $q_t$ is such that this equality is ensured, by definition.

% philosophy things
\section{Philosophy Things}
I don't think about the philosophic aspects of maching learning very often as I'm in the camp of people whose interests lie in solving present problems. I find some topics interesting though.

% thoughts on reasoning
\subsection{Thoughts on reasoning (Summer 2024)}

At the time of writing, machine learning, as a term, is used almost interchangeably with artificial intelligence (AI). AI isn't particularly well-defined in itself, even in academic settings, so its use in day-to-day things, e.g. in the news, often causes confusion and debate. I'm not sure how I would define AI in a way that'd satisfy most. Maybe the discipline of studying things that can `reason', as a human does, given some agreed upon notion of `reasoning'. A common issue found in discussions surrounding this topic is that a definition of `reasoning' is left out.

Right now, I'm unaware of any machine learning model that can `reason' by any agreed upon definition of the term. I suspect that while current models can simulate `reasoning' to the extent of being able to effectively solve tasks which require some sort of underlying understanding, they do not `reason' in the human sense, which is grounded in embodied experience. Let's say that an LLM (e.g. GPT-4o) concludes that a continuous real-valued function $f:\R\rightarrow\R$ has a root in some interval $(a, b)$ using the intermediate value theorem (IVT) by noting that $f(a)<0$ and $f(b)>0$. Did the LLM `reason' in its application of the IVT or do we instead brush this off as the LLM having been trained on many examples of the application of the IVT? Does it `understand' what it's concluding? As of right now I'd say that it doesn't. This of course isn't something unique to the IVT — we can construct many such examples. What I think makes the IVT interesting is that we can effectively express what it means to `understand' its correctness as a human. I'd say that the distinction between the LLM's `understanding' and a human's here is that a human knows that if they trace their finger on a piece of paper starting from the bottom half, crossing a horizontal line cutting through the middle of the paper, to the top half of the paper without removing their finger then their finger must have touched the horizontal line at some point (ignore the fact that such a traversal is not necessarily a function, you get the idea). You don't need any mathematical training to know and, more importantly, understand why your finger crosses through the horizontal line on the page. LLMs certainly do not `understand' the correctness of the IVT in this way, or really any way that isn't purely symbolic. Maybe multimodal LLMs will alleviate this.

A separate but related question is to what extent it matters that a model `reasons' in order to arrive its conclusions. Somewhat in the same way that it doesn't matter that a calculator has no inherent idea of why it outputs 6 when you input $3\cdot2$. From a purely problem solving point of view, these philosophical debates regarding what constitutes reasoning, understanding and knowing aren't too important in the short term. It'd be funny if 50 years from now we're well-taken care of by a fleet of machine learning-powered robots and we still look down on them for not being capable of true reasoning.

\subsection{Sufficiency of Internet-Scraped Data}
\TODO{}

\subsection{The Principled Measure-Explainability Tradeoff}
\TODO{}

\end{appendices}

\end{document}
