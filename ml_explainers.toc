\contentsline {section}{\numberline {1}Introduction}{2}{section.1}%
\contentsline {section}{\numberline {2}Supervised Learning}{4}{section.2}%
\contentsline {subsection}{\numberline {2.1}Linear Regression}{5}{subsection.2.1}%
\contentsline {subsubsection}{\numberline {2.1.1}Statistical Motivation}{8}{subsubsection.2.1.1}%
\contentsline {subsubsection}{\numberline {2.1.2}Example}{8}{subsubsection.2.1.2}%
\contentsline {subsubsection}{\numberline {2.1.3}Regularisation}{11}{subsubsection.2.1.3}%
\contentsline {subsubsection}{\numberline {2.1.4}$R^2$ value}{11}{subsubsection.2.1.4}%
\contentsline {subsubsection}{\numberline {2.1.5}Terminology}{12}{subsubsection.2.1.5}%
\contentsline {subsubsection}{\numberline {2.1.6}Opportunity to Demonstrate High Quality Data}{13}{subsubsection.2.1.6}%
\contentsline {subsection}{\numberline {2.2}Logistic Regression}{13}{subsection.2.2}%
\contentsline {subsubsection}{\numberline {2.2.1}Statistical Motivation}{15}{subsubsection.2.2.1}%
\contentsline {subsection}{\numberline {2.3}Support Vector Machines (SVMs)}{15}{subsection.2.3}%
\contentsline {subsubsection}{\numberline {2.3.1}Hard-margin SVMs}{16}{subsubsection.2.3.1}%
\contentsline {subsubsection}{\numberline {2.3.2}Soft-margin SVMs}{18}{subsubsection.2.3.2}%
\contentsline {subsubsection}{\numberline {2.3.3}Non-linear SVMs}{19}{subsubsection.2.3.3}%
\contentsline {subsection}{\numberline {2.4}Decision Trees and Random Forests}{19}{subsection.2.4}%
\contentsline {subsection}{\numberline {2.5}Handcrafted Features}{19}{subsection.2.5}%
\contentsline {subsection}{\numberline {2.6}The Bias-Variance Trade-Off}{19}{subsection.2.6}%
\contentsline {subsubsection}{\numberline {2.6.1}Double descent}{19}{subsubsection.2.6.1}%
\setcounter {tocdepth}{1}
\contentsline {subsection}{\numberline {2.7}Misc. questions}{19}{subsection.2.7}%
\setcounter {tocdepth}{2}
\contentsline {section}{\numberline {3}Gradient Descent and its Optimisation}{20}{section.3}%
\contentsline {subsection}{\numberline {3.1}Gradient Descent}{21}{subsection.3.1}%
\contentsline {subsection}{\numberline {3.2}Momentum}{21}{subsection.3.2}%
\contentsline {subsection}{\numberline {3.3}Hyperparameter Tuning}{21}{subsection.3.3}%
\setcounter {tocdepth}{1}
\contentsline {subsection}{\numberline {3.4}Misc. questions}{21}{subsection.3.4}%
\setcounter {tocdepth}{2}
\contentsline {subsubsection}{\numberline {3.4.1}Why does $f(\mathbf {x})$ ascend most in the direction of $\nabla f(\mathbf {x})$?}{21}{subsubsection.3.4.1}%
\contentsline {subsubsection}{\numberline {3.4.2}Choosing a loss function}{22}{subsubsection.3.4.2}%
\contentsline {subsubsection}{\numberline {3.4.3}How should we choose the learning rate $\eta $?}{23}{subsubsection.3.4.3}%
\contentsline {subsubsection}{\numberline {3.4.4}How should we initialise the parameters?}{23}{subsubsection.3.4.4}%
\contentsline {section}{\numberline {4}Neural Networks}{24}{section.4}%
\contentsline {subsection}{\numberline {4.1}Multi-Layer Perceptrons (MLPs)}{24}{subsection.4.1}%
\contentsline {subsubsection}{\numberline {4.1.1}Constructive example - The algebra}{26}{subsubsection.4.1.1}%
\contentsline {subsubsection}{\numberline {4.1.2}Constructive example - Subbing values in}{27}{subsubsection.4.1.2}%
\contentsline {subsubsection}{\numberline {4.1.3}Universal function approximation theorem (expressivity)}{28}{subsubsection.4.1.3}%
\contentsline {subsubsection}{\numberline {4.1.4}Depth $>$ Width}{28}{subsubsection.4.1.4}%
\contentsline {subsection}{\numberline {4.2}Backpropagation}{28}{subsection.4.2}%
\contentsline {subsection}{\numberline {4.3}Convolutional Neural Networks (CNNs)}{29}{subsection.4.3}%
\contentsline {subsubsection}{\numberline {4.3.1}Convolutions}{29}{subsubsection.4.3.1}%
\contentsline {subsubsection}{\numberline {4.3.2}Pooling}{30}{subsubsection.4.3.2}%
\contentsline {subsubsection}{\numberline {4.3.3}Example}{31}{subsubsection.4.3.3}%
\contentsline {subsection}{\numberline {4.4}Recurrent Neural Networks (RNNs)}{32}{subsection.4.4}%
\contentsline {subsubsection}{\numberline {4.4.1}Long Short-Term Memory (LSTMs)}{32}{subsubsection.4.4.1}%
\contentsline {subsubsection}{\numberline {4.4.2}Gated Recurrent Units (GRUs)}{32}{subsubsection.4.4.2}%
\contentsline {subsection}{\numberline {4.5}Autoencoders}{32}{subsection.4.5}%
\contentsline {subsubsection}{\numberline {4.5.1}Example autoencoder (visualisation bonus)}{33}{subsubsection.4.5.1}%
\setcounter {tocdepth}{1}
\contentsline {subsection}{\numberline {4.6}Skip Connections/Residual Networks}{33}{subsection.4.6}%
\setcounter {tocdepth}{2}
\setcounter {tocdepth}{1}
\contentsline {subsection}{\numberline {4.7}Misc. questions}{34}{subsection.4.7}%
\setcounter {tocdepth}{2}
\contentsline {section}{\numberline {5}Generative Models}{38}{section.5}%
\contentsline {subsection}{\numberline {5.1}Variational Autoencoders (VAEs)}{38}{subsection.5.1}%
\contentsline {subsubsection}{\numberline {5.1.1}think of different subsubsection name}{39}{subsubsection.5.1.1}%
\contentsline {subsubsection}{\numberline {5.1.2}The Reparameterization Trick}{42}{subsubsection.5.1.2}%
\contentsline {subsubsection}{\numberline {5.1.3}VAE architecture for MNIST}{42}{subsubsection.5.1.3}%
\contentsline {subsection}{\numberline {5.2}Generative Adversarial Networks (GANs)}{42}{subsection.5.2}%
\contentsline {subsection}{\numberline {5.3}Normalising Flow Models}{42}{subsection.5.3}%
\contentsline {subsection}{\numberline {5.4}Diffusion Models}{42}{subsection.5.4}%
\contentsline {subsection}{\numberline {5.5}Transformers}{42}{subsection.5.5}%
\contentsline {subsubsection}{\numberline {5.5.1}Token embedding}{43}{subsubsection.5.5.1}%
\contentsline {subsubsection}{\numberline {5.5.2}Positional encoding}{43}{subsubsection.5.5.2}%
\setcounter {tocdepth}{1}
\contentsline {subsection}{\numberline {5.6}Misc. questions}{43}{subsection.5.6}%
\setcounter {tocdepth}{2}
\contentsline {section}{Appendices}{44}{section*.24}%
\setcounter {tocdepth}{0}
\contentsline {section}{\numberline {A}Probability Theory Things}{44}{appendix.a.A}%
\contentsline {subsection}{\numberline {A.1}From Bernoulli to Binomial}{44}{subsection.a.A.1}%
\contentsline {subsection}{\numberline {A.2}From Categorical to Multinomial}{45}{subsection.a.A.2}%
\contentsline {subsection}{\numberline {A.3}Negative Binomial and Geometric}{46}{subsection.a.A.3}%
\contentsline {subsection}{\numberline {A.4}Entropy}{47}{subsection.a.A.4}%
\contentsline {subsubsection}{\numberline {A.4.1}Example of using entropy}{48}{subsubsection.a.A.4.1}%
\contentsline {subsection}{\numberline {A.5}The Central Limit Theorem (CLT)}{49}{subsection.a.A.5}%
\contentsline {section}{\numberline {B}Statistics Things}{49}{appendix.a.B}%
\contentsline {subsection}{\numberline {B.1}Independent samples}{49}{subsection.a.B.1}%
\contentsline {subsection}{\numberline {B.2}Assumptions of Normality}{49}{subsection.a.B.2}%
